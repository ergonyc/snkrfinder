{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.transfer\n",
    "# default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snkrfinder.data.munge'; 'snkrfinder.data' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-216ba8a44914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msnkrfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msnkrfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msnkrfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmunge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msnkrfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'snkrfinder.data.munge'; 'snkrfinder.data' is not a package"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from snkrfinder.imports import *\n",
    "from snkrfinder.core import *\n",
    "from snkrfinder.data.munge import *\n",
    "from snkrfinder.model.core import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first snkrfinder.model.transfer\n",
    "\n",
    "\n",
    "\n",
    "## OVERVIEW: model module- MobileNet_v2 feature extractor\n",
    "\n",
    "This is a project initiated while an Insight Data Science fellow.  It grew out of my interest in making data driven tools in the fashion/retail space I had most recently been working.   The original over-scoped idea was to make a shoe desighn tool which could quickly develop some initial sneakers based on choosing some examples, and some text descriptors.  Designs are constrained by the \"latent space\" defined (discovered?) by a database of shoe images.  However, given the 3 week sprint allowed for development, I pared the tool down to a simple \"aesthetic\" recommender for sneakers, using the same idea of utilizing an embedding space defined by the database fo shoe images.\n",
    "\n",
    "Feature extractor: \n",
    "\n",
    "    1. embed database into feature space.\n",
    "    2. evaluate/validate by simple logistic regression on classification.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### A: MobileNet_v2 as feature extractor\n",
    "\n",
    "\n",
    "### B: add ResNet and expand modeling\n",
    "embed database into feature space.\n",
    "evaluate by simple logistic regression on classification.\n",
    "\n",
    "NOTE:  symbolic link in the nbs directory to enable the module loads in these notebooks.  i.e. `ln -s ../snkrfinder/ snkrfinder`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "print(fastai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put is in the base snkr-finder directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "print(Path().cwd())\n",
    "if Path().cwd().parts[-1] == 'nbs': \n",
    "    os.chdir('..')\n",
    "    print('moved out of `nbs` directory')\n",
    "    \n",
    "print(Path().cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# this should go into a utils or cfg module\n",
    "HOME = get_home()\n",
    "HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "images_path = D_ROOT/'ut-zap50k-images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ZAPPOS_DF_SIMPLIFIED # \"zappos-50k-simplified\"\n",
    "df = pd.read_pickle(f\"data/{filename}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer learning \n",
    "\n",
    "\n",
    "### by hand\n",
    "Here's how we could would do transfer learning \"by hand\":   \n",
    "\n",
    "    1. load the pretrained network\n",
    "    2. create a new linear classifier e.g. `nn.Linear(num_ftrs, n_categories)`\n",
    "    3. _freeze_ the parameters by setting `param.requires_grad=False` (NOTE, that to actually work we need to NOT freeze batchnorm layers)\n",
    "    4. create a training loop (or send to a fastai learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def transfer_mobilenet_v2(n_cat=4,freeze=True):\n",
    "    model_conv = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "    # Parameters of newly constructed modules have requires_grad=True by default\n",
    "    # just read this off: model_conv.classifier\n",
    "    num_ftrs = model_conv.classifier._modules['1'].in_features\n",
    "    model_conv.classifier._modules['1'] = nn.Linear(num_ftrs, n_cat)\n",
    "    if freeze:\n",
    "        for param in model_conv.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    return model_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the fastai way\n",
    "With the fastai API we can simply use `cnn_learner` with the name of the architecture, and everything else is semi-automatic. e.g.\n",
    "    1. load the arcitecture and trained weights\n",
    "    3. creating a classifier \"head\"\n",
    "    3. setting up the parameters for freezing (avoiding batchnorms)\n",
    "    \n",
    "Note that the mobilenet V2 architecture is NOT part of the API so we'll need to get the weights and arch from torchvision, and hack in the the `splitter` and `cut` points.\n",
    "\n",
    "I'm also going to wrap the dataframe -> DataBlock -> dataloaders in some convenience functions to make the whole shebang just a few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_zappos_datablock(size=IMG_SIZE,rand_aug=True):    \n",
    "    batch_tfms = Normalize.from_stats(*imagenet_stats)\n",
    "    if rand_aug:\n",
    "        rand_tfms = aug_transforms(mult=1.0, \n",
    "                   do_flip=True, \n",
    "                   flip_vert=False, \n",
    "                   max_rotate=3.0, \n",
    "                   min_zoom=.95, \n",
    "                   max_zoom=1.0, \n",
    "                   max_lighting=0.1, \n",
    "                   max_warp=0.023, \n",
    "                   p_affine=0.66, \n",
    "                   p_lighting=0.2, \n",
    "                   xtra_tfms=None, \n",
    "                   size=None, \n",
    "                   mode='bilinear', \n",
    "                   pad_mode='border', \n",
    "                   align_corners=True, \n",
    "                   batch=False, \n",
    "                   min_scale=1.0)\n",
    "        batch_tfms = rand_tfms+[batch_tfms]\n",
    "                      \n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                   splitter=ColSplitter('is_valid'), \n",
    "                   get_x=zap_get_x, \n",
    "                   get_y=zap_get_y,\n",
    "                   item_tfms=Resize(size, method='pad', pad_mode='border'),\n",
    "                   batch_tfms=batch_tfms)  # border pads white...\n",
    "    return dblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prep_df_for_datablocks(df):\n",
    "    df = df[[\"Category\",\"path\",\"train\",\"test\",\"validate\",\"t_t_v\"]].copy()\n",
    "    # I could remove all the \"test\" rows... for now i'll choose an alternate strategy:\n",
    "    # Drop all the \"test\" rows for now, and create an \"is_valid\" column...\n",
    "    # should probably drop a ton of columns to jus tkeep the file paths...\n",
    "    # just keep what we'll need below\n",
    "    df.loc[:,'is_valid'] = df.test | df.validate\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_zappos_cat_dataloaders(data=None,batch_size=32, size=IMG_SIZE, device=None):\n",
    "    # put everythign in train, and don't do any augmentation since we are just going \n",
    "    # resize to 224\n",
    "    # set up the helper functions to pass data into the\n",
    "    if device is None:\n",
    "        device = get_cuda()\n",
    "    if data is None:\n",
    "        filename = ZAPPOS_DF_SIMPLIFIED # \"zappos-50k-simplified\"\n",
    "        data = pd.read_pickle(f\"data/{filename}.pkl\")\n",
    "    \n",
    "    data = prep_df_for_datablocks(data)\n",
    "\n",
    "    dblock = get_zappos_datablock(size=size)\n",
    "        \n",
    "    dls = dblock.dataloaders(data,bs=batch_size,drop_last=True,device=device)\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ZAPPOS_DF_SIMPLIFIED # \"zappos-50k-simplified\"\n",
    "df = pd.read_pickle(f\"data/{filename}.pkl\")\n",
    "df = prep_df_for_datablocks(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.sample(df.shape[0]-1)\n",
    "#dls = get_zappos_cat_dataloaders(df)\n",
    "dls = get_zappos_cat_dataloaders()\n",
    "rnet_learn = cnn_learner(dls, resnet18, metrics=error_rate)\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_min,lr_steep = rnet_learn.lr_find()\n",
    "mlr = .5*(lr_min+lr_steep)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr_min,lr_steep]).log().mean().exp().tolist()\n",
    "lr_min,lr_steep,mlr,gmlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnet_learn.fine_tune(2, base_lr=gmlr,freeze_epochs=1)\n",
    "rnet_learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'rnet18_transfer-feb20_1x2b'\n",
    "rnet_learn.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnet_learn.export(fname=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_epochs,epochs = 4,2\n",
    "lr_min,lr_steep = rnet_learn.lr_find()\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr_min,lr_steep]).log().mean().exp().tolist()\n",
    "\n",
    "rnet_learn.fine_tune(epochs, base_lr=gmlr,freeze_epochs=freeze_epochs)\n",
    "rnet_learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'rnet18_transfer-feb20_{freeze_epochs}x{epochs}b'\n",
    "rnet_learn.save(filename)\n",
    "\n",
    "rnet_learn.save('rnet18_transfer-fep20_1x2')\n",
    "rnet_learn.export(fname=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_learner(dls, arch, normalize=True, n_out=None, pretrained=True, config=None,\n",
    "                # learner args\n",
    "                loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=None, cbs=None, metrics=None, path=None,\n",
    "                model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95,0.85,0.95),\n",
    "                # other model args\n",
    "                **kwargs):\n",
    "    \"Build a convnet style learner from `dls` and `arch`\"\n",
    "\n",
    "    meta = model_meta.get(arch, _default_meta)\n",
    "\n",
    "    if n_out is None: n_out = get_c(dls)\n",
    "    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n",
    "    model = create_cnn_model(arch, n_out, pretrained=pretrained, **kwargs)\n",
    "\n",
    "    splitter=ifnone(splitter, meta['split'])\n",
    "    learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=opt_func, lr=lr, splitter=splitter, cbs=cbs,\n",
    "                   metrics=metrics, path=path, model_dir=model_dir, wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn,\n",
    "                   moms=moms)\n",
    "    if pretrained: learn.freeze()\n",
    "    # keep track of args for loggers\n",
    "    store_attr('arch,normalize,n_out,pretrained', self=learn, **kwargs)\n",
    "    return learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnet_learn = cnn_learner(dls,torchvision.models.mobilenet_v2, n_out=4,\n",
    "                    pretrained=True,metrics=error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_min,lr_steep = mnet_learn.lr_find()\n",
    "mlr = .5*(lr_min+lr_steep)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr_min,lr_steep]).log().mean().exp().tolist()\n",
    "lr_min,lr_steep,mlr,gmlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_epochs,epochs = 4,2\n",
    "mnet_learn.fine_tune(epochs, base_lr=gmlr,freeze_epochs=freeze_epochs)\n",
    "mnet_learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'mnet18_transfer-feb20_{freeze_epochs}x{epochs}b'\n",
    "rnet_learn.save(filename)\n",
    "rnet_learn.export(fname=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_epochs,epochs = 2,1\n",
    "mnet_learn = cnn_learner(dls,torchvision.models.mobilenet_v2, n_out=4,\n",
    "                    pretrained=True,metrics=error_rate)\n",
    "\n",
    "lr_min,lr_steep = mnet_learn.lr_find()\n",
    "gmlr = torch.tensor([lr_min,lr_steep]).log().mean().exp().tolist() #geometric mean\n",
    "\n",
    "mnet_learn.fine_tune(epochs, base_lr=gmlr,freeze_epochs=freeze_epochs)\n",
    "mnet_learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'mnet18_transfer-feb20_{freeze_epochs}x{epochs}b'\n",
    "rnet_learn.save(filename)\n",
    "rnet_learn.export(fname=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = Interpretation.from_learner(mnet_learn)\n",
    "interp.plot_top_losses(9, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnet_learn=load_learner('rnet_transfer-feb20_1x2a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = Interpretation.from_learner(rnet_learn)\n",
    "interp.plot_top_losses(9, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "# ### NOTE: these are from https://walkwithfastai.com/Style_Transfer\n",
    "\n",
    "# # these are the convlayer \"features\" to define style for styletransfer\n",
    "# _vgg_config = {\n",
    "#     'vgg16' : [1, 11, 18, 25, 20],\n",
    "#     'vgg19' : [1, 6, 11, 20, 29, 22]\n",
    "# }\n",
    "\n",
    "# from torchvision.models import vgg19, vgg16\n",
    "\n",
    "# feat_net = vgg19(pretrained=True).features.cuda().eval()\n",
    "\n",
    "# def get_feats(arch:str, pretrained=True):\n",
    "#     \"Get the features of an architecture\"\n",
    "#     feat_net, layers = _get_layers(arch, pretrained)\n",
    "#     hooks = hook_outputs(layers, detach=False)\n",
    "#     def _inner(x):\n",
    "#         feat_net(x)\n",
    "#         return hooks.stored\n",
    "#     return _inner\n",
    "\n",
    "\n",
    "\n",
    "# def _get_layers(arch:str, pretrained=True):\n",
    "#     \"Get the layers and arch for a VGG Model (16 and 19 are supported only)\"\n",
    "#     feat_net = vgg19(pretrained=pretrained).cuda() if arch.find('9') > 1 else vgg16(pretrained=pretrained).cuda()\n",
    "#     config = _vgg_config.get(arch)\n",
    "#     features = feat_net.features.cuda().eval()\n",
    "#     for p in features.parameters(): p.requires_grad=False\n",
    "#     return feat_net, [features[i] for i in config]\n",
    "\n",
    "\n",
    "\n",
    "# feats = get_feats('vgg19')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
