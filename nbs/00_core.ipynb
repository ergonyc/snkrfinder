{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp core\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# snkrfinder.core\n",
    "\n",
    "> API details:\n",
    "    - sets up the data strucures and tools for Snkr Finder\n",
    "    \n",
    "    \n",
    "\n",
    "version 0.2 Jan 2021 (0.1 Insight tf/keras Jan 2020)\\\n",
    "version 0.2.1 Feb 2021\n",
    "\n",
    "## OVERVIEW\n",
    "\n",
    "This is a project initiated while an Insight Data Science fellow.  It grew out of my interest in making data driven tools in the fashion/retail space I had most recently been working.   The original over-scoped idea was to make a shoe desighn tool which could quickly develop some initial sneakers based on choosing some examples, and some text descriptors.  Designs are constrained by the \"latent space\" defined (discovered?) by a database of shoe images.  However, given the 3 week sprint allowed for development, I pared the tool down to a simple \"aesthetic\" recommender for sneakers, using the same idea of utilizing an embedding space defined by the database fo shoe images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from snkrfinder.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## structure\n",
    "\n",
    "\n",
    "\n",
    "### snkrfinder.core\n",
    "- constants and data-structues\n",
    "- configurations\n",
    "- helper functions / utilities\n",
    "    - e.g. pickle convenience fns\n",
    "\n",
    "### snkrfinder.data\n",
    "- data handling\n",
    "    - download / locate\n",
    "    - munging and cleaning and merging\n",
    "        - zappos 50 k - matlab imports !?!\n",
    "        - scraped data\n",
    "            - goat\n",
    "            - sns\n",
    "    - load into dataframe\n",
    "        - train / validate / test\n",
    "        - categories\n",
    "       \n",
    "### snkrfinder.model\n",
    "- pretrained architectures\n",
    "    - mobilenet v2 (from torchvision)\n",
    "    - resnets (resnets for torchvision, xresnet, from fastai)\n",
    "    - TBD vgg, etc.?\n",
    "\n",
    "- feature extractor\n",
    "    - tools to separate \"body\" from classifying \"head\" \n",
    "        - where? aka \"cut\" or \"split\"\n",
    "        - pooling \n",
    "            - fastai max + average pooling across spatial dimension\n",
    "    - eval mode, turn off trainable params\n",
    "    \n",
    "- light visualization / plotting\n",
    "- UMAP\n",
    "\n",
    "### snkrfinder.model.transfer\n",
    "- transfer learning methods\n",
    "   \n",
    "   \n",
    "### snkrfinder.cvae \n",
    "- VAE datablocks \n",
    "- models\n",
    "- callbacks\n",
    "- learner hacks\n",
    "- TODO:\n",
    "    - AE version\n",
    "    - CVAE vs VAE\n",
    "\n",
    "### snkrfinder.widgets\n",
    "    - instantiate Sneaker Finder 2.0\n",
    "    - following the fastai wiget model\n",
    "    - datacleaner GUI (WIP)\n",
    "    - latent explorer (WIP)\n",
    "\n",
    "\n",
    "### snkrfinder.TBDextensions (TODO, etc)\n",
    "- visualization\n",
    "- embeddings\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "    \n",
    "    \n",
    "## constants, structures, functions to export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_home():\n",
    "    return Path().home()\n",
    "\n",
    "HOME = get_home()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Data methods\n",
    "def dump_pickle(filepath, item_to_save):\n",
    "    \"simple wrapper to load a pickelfile\"\n",
    "    with open(filepath, 'wb') as h:\n",
    "        pickle.dump(item_to_save, h)\n",
    "    \n",
    "\n",
    "def load_pickle(filepath):\n",
    "    \"simple wrapper to load a pickelfile\"\n",
    "    with open(filepath, \"rb\") as h:\n",
    "        item = pickle.load(h)\n",
    "    return item\n",
    "\n",
    "\n",
    "def add_time_stamp(filepath=\"\"):\n",
    "    \"add PST timestamp to path\"\n",
    "    os.environ[\"TZ\"] = \"US/Pacific\"\n",
    "    time.tzset()\n",
    "    return filepath + time.strftime(\"%m%d-%H%M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "# DEFAULT PARAMS\n",
    "L_ROOT = get_home()/'Projects/Project2.0/snkrfinder'\n",
    "D_ROOT = get_home()/'Projects/DATABASE'\n",
    "\n",
    "DBS_REL = {\"zappos\": \"zappos/ut-zap50k-images\",\n",
    "      \"sns\": \"scraped/sns\",\n",
    "      \"goat\": \"scraped/goat\"}\n",
    "\n",
    "DBS = {\"zappos\": \"ut-zap50k-images\",\n",
    "      \"sns\": \"SnkrScrpr/data/sns/img\",\n",
    "      \"goat\": \"SnkrScrpr/data/goat/img\"}\n",
    "\n",
    "ZAPPOS_META_DIR = D_ROOT/\"ut-zap50k-data\"\n",
    "SCRAPED_META_DIR = D_ROOT/\"SnkrScrpr/data/\"\n",
    "\n",
    "# Image sizes sm/med/large\n",
    "IMG_SIZE_LG = 224\n",
    "IMG_SIZE_MD = 160\n",
    "IMG_SIZE_SM = 128\n",
    "IMG_SIZE = IMG_SIZE_MD\n",
    "\n",
    "IMG_SIZES = {\"small\":  IMG_SIZE_SM,\n",
    "             \"medium\": IMG_SIZE_MD,\n",
    "             \"large\":  IMG_SIZE_LG}\n",
    "\n",
    "IMSZ2LAB = {IMG_SIZE_SM:\"small\",\n",
    "            IMG_SIZE_MD:\"medium\",\n",
    "            IMG_SIZE_LG:\"large\"}\n",
    "\n",
    "SIZE_ABBR = {\"small\": \"sm\", \"medium\": \"md\", \"large\":\"lg\"}\n",
    "\n",
    "\n",
    "# data files\n",
    "ZAPPOS_DF_SIMPLIFIED = \"zappos-50k-simplified_sort\"\n",
    "\n",
    "# additional df pickles of zappos data\n",
    "ZAPPOS_FEATS_ALL = \"zappos-50k-mobilenetv2-features_\"\n",
    "ZAPPOS_FEATS_ALL_SORT = \"zappos-50k-mobilenetv2-features_sort_3\"\n",
    "ZAPPOS_FEATS_SM = f\"mobilenetv2-features_small\"\n",
    "ZAPPOS_FEATS_MD = f\"mobilenetv2-features_medium\"\n",
    "ZAPPOS_FEATS_LG = f\"mobilenetv2-features_large\"\n",
    "\n",
    "SCRAPED_DF = \"full_data\"\n",
    "COMBINED_DF = \"full_db\"\n",
    "\n",
    "QUERY_IM = \"Shoes/Sneakers and Athletic Shoes/Nike/7716996.288224.jpg\"\n",
    "QUERY_IM2 = 'figs/491212_01.jpg.jpeg'\n",
    "\n",
    "# VAE PARAMETERS\n",
    "# differential weighting for the beta VAE MSE/(#latents) vs beta_weight*KLD/(#pixels)\n",
    "BETA = 2\n",
    "LATENT_DIM = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# copied the split and cut from denseness ... need to check if the splits and cut make sense\n",
    "def _mobilenet_v2_split(m:torch.nn.Module): \n",
    "    return L(m[0][0][:7],m[0][0][7:], m[1:]).map(params)\n",
    "\n",
    "mobilenet_v2_meta   = {'cut':-1, 'split':_mobilenet_v2_split, 'stats':imagenet_stats}\n",
    "\n",
    "model_meta[torchvision.models.mobilenet_v2] = {**mobilenet_v2_meta}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate extreme range for imagenet stats normilization..\n",
    "\n",
    "These are the extreme values for our images... we'll do our final scaling for the AE to the +/- the biggest\n",
    "> ```\n",
    "    TensorImage([[[[-2.1179,  2.2489]],\n",
    "          [[-2.0357,  2.4286]],\n",
    "          [[-1.8044,  2.6400]]]])\n",
    "          ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "IM_STATS = {'imagenet':imagenet_stats,'sneaker':([.5,.5,.5],[.5,.5,.5])}\n",
    "\n",
    "DEFAULT_STATS = 'sneaker'\n",
    "\n",
    "IMAGENET_OUT_RANGE = [-2.64,2.64]\n",
    "MY_OUT_RANGE = [-1,1]\n",
    "\n",
    "OUT_RANGE = MY_OUT_RANGE if DEFAULT_STATS == 'sneaker' else IMAGENET_OUT_RANGE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nbdev lib + docs export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01a_data.munge.ipynb.\n",
      "Converted 01b_data.load.ipynb.\n",
      "Converted 02a_model.core.ipynb.\n",
      "Converted 02b_model.transfer.ipynb.\n",
      "Converted 02c_model.cvae.ipynb.\n",
      "Converted 04a_widgets.feats.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "160*160*3/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
