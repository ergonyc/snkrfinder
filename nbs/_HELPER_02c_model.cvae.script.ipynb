{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.cvae\n",
    "#default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snkrfinder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0d0afe8bb0f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msnkrfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msnkrfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msnkrfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmunge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msnkrfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'snkrfinder'"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from snkrfinder.imports import *\n",
    "from snkrfinder.core import *\n",
    "from snkrfinder.data.munge import *\n",
    "from snkrfinder.model.core import *\n",
    "#from snkrfinder.model.transfer import *\n",
    "\n",
    "from fastai.test_utils import show_install, synth_learner, nvidia_smi, nvidia_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first snkrfinder.model.cvae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERVIEW: cvae module - convolutional variational auto encoder\n",
    "\n",
    "> TODO: clean up this preamble section\n",
    "\n",
    "preamble: This is a project initiated while an Insight Data Science fellow.  It grew out of my interest in making data driven tools in the fashion/retail space I had most recently been working.   The original over-scoped idea was to make a shoe desighn tool which could quickly develop some initial sneakers based on choosing some examples, and some text descriptors.  Designs are constrained by the \"latent space\" defined (discovered?) by a database of shoe images.  However, given the 3 week sprint allowed for development, I pared the tool down to a simple \"aesthetic\" recommender for sneakers, using the same idea of utilizing an embedding space defined by the database fo shoe images.\n",
    "\n",
    "\n",
    "Most of the code is derived from the work of @EtieeneT. (e.g.: TabularData https://github.com/EtienneT/TabularVAE and later https://github.com/EtienneT/vae )\n",
    "\n",
    "\n",
    "### sneaker image encoding via various flavors of auto-encoder.  \n",
    "- Auto Encoder: AE\n",
    "    - ~~linear encoder / decoder~~\n",
    "    - convolutional encoder / decoder\n",
    "        - encoders\n",
    "            - pretrained (e.g. resnet, mobilenet_v2)\n",
    "            - 'vanilla' convolutional\n",
    "            - 'vanilla' ResBlock\n",
    "\n",
    "        - decoders\n",
    "            - 'vanilla' convolutional\n",
    "            - 'vanilla' ResBlock\n",
    "    - Linear Latent representation (`LatentLayer`)\n",
    "\n",
    "- Variational- Auto Encoder:  VAE or 𝜷-VAE      \n",
    "    - Variational \"bottleneck\": \"reparameterazation trick\" for two variable linear latents\n",
    "        - 𝜷-VAE: by increasing the relative strenght of error signal of the KLD through a beta variational parameter we can get better formed latent spaces which are somewhat \"disentangled\". That is have some semantic or meaningful structure in the representation. \n",
    "    - MMD- variational autoencoder uses a Linear Latent and MMD as the retularizer\n",
    "        - The KLD has some problems and \"over-regularizes\" or simply isn't strong enough compared to the Maximum Mean Discrepancy (MMD) loss and tends to converge to a somewhat degenerate solution\n",
    "        - MD regularization of the latent space has great advantage but at a computational expense\n",
    "\n",
    "\n",
    "\n",
    "### FUTURE EXTENSIONS:\n",
    "    -  model based data cleaning (widget module?)\n",
    "    - GAN finetuning?\n",
    "    - crappify general pattern\n",
    "    - throw out based on inspection of high loss\n",
    "    - try mixed labels?  things that are >50% sneakers included???\n",
    "    \n",
    "    \n",
    "### TODO: fix loss / metrics reduction logic\n",
    "batchmean should *only* apply to KLD because the mean over latents in not meaningful.\n",
    "'none', 'mean', 'sum' for reduction over \"batches\"\n",
    "\n",
    "all the other _scaling_ should come through alpha....\n",
    "\n",
    "\n",
    "### TODO: find bugs in github actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "show_install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#nvidia_smi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "# fix our base directory\n",
    "print(Path().cwd())\n",
    "os.chdir(L_ROOT)\n",
    "print(Path().cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using  fastai V2: data pipelining\n",
    "- Datablock API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved merged database, and set the seeds.  And doublecheck our data is where we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "df = pd.read_pickle(f\"data/{COMBINED_DF}.pkl\")\n",
    "np.random.seed(3333)\n",
    "torch.manual_seed(3333)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#TODO:  fix the data layout... the links and paths are screwy\n",
    "image_path = L_ROOT/\"data\"\n",
    "#image_path = D_ROOT/DBS['zappos']\n",
    "batch_size = 64\n",
    "\n",
    "L([image_path/d for d in df.path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def prep_df_for_datablocks(df):\n",
    "    df = df[[\"path\",\"train\",\"test\",\"validate\",\"t_t_v\",\"Category\"]].copy()\n",
    "    # I could remove all the \"test\" rows... for now i'll choose an alternate strategy:\n",
    "    # Drop all the \"test\" rows for now, and create an \"is_valid\" column...\n",
    "    # should probably drop a ton of columns to jus tkeep the file paths...\n",
    "    # just keep what we'll need below\n",
    "    df.loc[:,'is_valid'] = df.test | df.validate\n",
    "    df.loc[:,'og_idx'] = df.index\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep_df_for_datablocks(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ae_btfms(stats = 'sneaker'):\n",
    "    # could use globals IM_STATS['sneaker'] and IM_STATS['imagenet']\n",
    "    im_stats = ([.5,.5,.5],[.5,.5,.5]) if stats == 'sneaker' else imagenet_stats\n",
    "    batch_tfms = Normalize.from_stats(*im_stats)\n",
    "    #batch_tfms = Normalize.from_stats([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    rand_tfms = aug_transforms(mult=1.0, \n",
    "               do_flip=True, \n",
    "               flip_vert=False, \n",
    "               max_rotate=5.0, \n",
    "               min_zoom=.95, \n",
    "               max_zoom=1.0, \n",
    "               max_lighting=0.1, \n",
    "               max_warp=0.1, \n",
    "               p_affine=0.66, \n",
    "               p_lighting=0.2, \n",
    "               xtra_tfms=None, \n",
    "               size=None, \n",
    "               mode='bilinear', \n",
    "               pad_mode='border', \n",
    "               align_corners=True, \n",
    "               batch=False, \n",
    "               min_scale=1.0)\n",
    "    return rand_tfms+[batch_tfms]\n",
    "\n",
    "def get_ae_no_aug(stats = 'sneaker'):\n",
    "    im_stats = ([.5,.5,.5],[.5,.5,.5]) if stats == 'sneaker' else imagenet_stats\n",
    "    batch_tfms = Normalize.from_stats(*im_stats)\n",
    "    return [batch_tfms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastAI \"block\" for autoencoder\n",
    "\n",
    "Next we need our own version of ReadTabBatch that will return our inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# NO CLUE WHY WE NEED TO HAVE THIS.... copied\n",
    "class TensorPoint(TensorBase):\n",
    "    \"Basic type for points in an image\"\n",
    "    _show_args = dict(s=10, marker='.', c='r')\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, t, img_size=None)->None:\n",
    "        \"Convert an array or a list of points `t` to a `Tensor`\"\n",
    "        return cls(tensor(t).view(-1, 2).float(), img_size=img_size)\n",
    "\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        if 'figsize' in kwargs: del kwargs['figsize']\n",
    "        x = self.view(-1,2)\n",
    "        ctx.scatter(x[:, 0], x[:, 1], **{**self._show_args, **kwargs})\n",
    "        return ctx\n",
    "\n",
    "\n",
    "class Tensor2Vect(TensorPoint): pass\n",
    "# TODO:  instantiate a show method\n",
    "\n",
    "\n",
    "class LatentsTensor(Tensor2Vect):\n",
    "    \"Basic type for latents as Tensor inheriting from TensorPoint (vectors)\"\n",
    "    @classmethod\n",
    "    def create(cls, ts, img_size=IMG_SIZE): \n",
    "        \"create IMG_SIZE attr to register plotting...\"\n",
    "\n",
    "        if isinstance(ts,tuple):\n",
    "            mu,logvar = ts\n",
    "        elif ts is None:\n",
    "            mu,logvar = None,None\n",
    "        else:\n",
    "            mu = None\n",
    "            logvar = None\n",
    "        if mu is None: mu = torch.empty(0)\n",
    "        elif not isinstance(mu, Tensor): Tensor(mu) \n",
    "        \n",
    "        if logvar is None: logvar = torch.empty(0)\n",
    "        elif not isinstance(logvar,Tensor): Tensor(logvar)\n",
    "        \n",
    "        t = torch.cat([mu,logvar],dim=-1) # in case its a batch?\n",
    "                \n",
    "        return cls(tensor(t).view(-1, 2).float(), img_size=img_size)\n",
    "\n",
    "    #     def show(self, ctx=None, **kwargs):\n",
    "    #         if 'figsize' in kwargs: del kwargs['figsize']\n",
    "    #         x = self.view(-1,2)\n",
    "    #         ctx.scatter(x[:, 0], x[:, 1], **{**self._show_args, **kwargs})\n",
    "    #         return ctx\n",
    "    #         mu,logvar = self\n",
    "    #         if not isinstance(mu, Tensor) or not isinstance(logvar,Tensor): return ctx\n",
    "\n",
    "    #         title_str = f\"mu-> {mu.mean():e}, {mu.std():e}  logvar->{logvar.mean():e}, {logvar.std():e}\"\n",
    "\n",
    "    #         if 'figsize' in kwargs: del kwargs['figsize']\n",
    "    #         if 'title' in kwargs: kwargs['title']=title_str\n",
    "    #         if ctx is None:\n",
    "    #             _,axs = plt.subplots(1,2, figsize=(12,6))\n",
    "    #             x=torch.linspace(0,1,mu[0].shape[0])\n",
    "    #             axs[0].scatter(x, mu[:], **{**self._show_args, **kwargs})\n",
    "    #             axs[1].scatter(x, logvar[:], **{**self._show_args, **kwargs})\n",
    "    #             ctx = axs[1]\n",
    "\n",
    "    #         ctx.scatter(mu[:], logvar[:], **{**self._show_args, **kwargs})\n",
    "    #         return ctx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def df_get_x(r): \n",
    "    \"datablock df helper for VAE Block using `LatentTuple`\"\n",
    "    return image_path/r['path']\n",
    "\n",
    "def df_get_y(r): \n",
    "    \"datablock df helper for VAE Block using `LatentTuple`\"\n",
    "    return (df_get_x(r),None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "\n",
    "def LatentsTensorBlock(): \n",
    "    \"Class wrapper for the AE `LatentTensor` Block\"\n",
    "    return TransformBlock(type_tfms=LatentsTensor.create, batch_tfms=noop)\n",
    "\n",
    "\n",
    "def df_ae_x(r,im_path=L_ROOT/\"data\"): \n",
    "    \"Autoencoder LatentsTensorBlock datablock df helper\"\n",
    "    return im_path/r['path']\n",
    "\n",
    "\n",
    "# need to make sure that we get the image whihc is \"Identical\" to the input.. how to test?\n",
    "\n",
    "def df_ae_y(r): \n",
    "    \"The target is the same as the input for AE\"# lambda o: o\n",
    "    return df_ae_x(r)\n",
    "\n",
    "\n",
    "\n",
    "#export \n",
    "# could we do a typedispatch to manage the transforms...?\n",
    "# def VAETargetTupleBlock(): \n",
    "#     return TransformBlock(type_tfms=VAETargetTuple.create, batch_tfms=IntToFloatTensor)\n",
    "\n",
    "def LatentTupleBlock(): \n",
    "    \"Class wrapper for the AE `LatentTuple` Block (depricated)\"\n",
    "    return TransformBlock(type_tfms=LatentTuple.create, batch_tfms=noop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to set  `n_inp=1`. Otherwise the default to make the  input to 1-len(blocks).  Also note that the `FeatsResize` is used to avoid the random jittering from resize during training.  Only the very narrow batch augmentations will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# \n",
    "\n",
    "def get_ae_DataBlock(aug=True,im_path=L_ROOT/\"data\",stats = 'sneaker',im_size=IMG_SIZE):\n",
    "    \"wrapper to get the standard AE datablock with `ImageBlock`,`LatentTensor` target\"\n",
    "    # use partials or a class wrapper to get around this yucky hack\n",
    "    global image_path\n",
    "    image_path = im_path\n",
    "    \n",
    "    mytfms = get_ae_btfms(stats=stats) if aug else get_ae_no_aug(stats=stats)\n",
    "    block = DataBlock(blocks=(ImageBlock(cls=PILImage), ImageBlock(cls=PILImage), LatentsTensorBlock ),\n",
    "              get_x=df_ae_x, \n",
    "              get_y=[df_ae_y, noop], #don't need to get the LatentsTensorBlock, just create\n",
    "              splitter=ColSplitter('is_valid'), \n",
    "              item_tfms= FeatsResize(im_size,method='pad', pad_mode='border'),\n",
    "              batch_tfms = mytfms,\n",
    "              n_inp = 1)\n",
    " \n",
    "    return block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "# block = get_ae_DataBlock(aug=True)\n",
    "# block.summary(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# dls = block.dataloaders(df, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the VAE\n",
    "\n",
    "Variational Auto-Encoder for fastai\n",
    "\n",
    "I'm going to use a generic convolutional net as the basis of the encoder, and its reverse as the decoder. This is a proof of concept for using the _fastai_ framework, and will experiment with pre-trained resnet and MobileNet_v2 later. I'd like to use the MobileNet_v2 as a direct link ot the \"SneakerFinder\" tool which motivated this experiment. [see SneakerFinder]\n",
    "\n",
    "A variational \"module\" will sit between the encoder and decoder as the \"Bottleneck\". The Bottleneck will map the resnet features into a latent space (e.g. ~100 dimensions) represented of standard normal variables.  The \"reparameterization trick\" will sample from this space and the \"decoder\" will generate images. \n",
    "\n",
    "Finally a simple \"decoder\" will sample from the variational latents space and be trained to reconstruct the images. \n",
    "\n",
    "The intention is the latent space can be used to generate novel sneaker images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE:  deterministic AutoEncoder ( _non- variational_)\n",
    "\n",
    "Although we give up the original utility we are going for --  creating new sneakers via the latent space -- having otherwise equivalent non-variational autoencoders for reference will be great. Furthermore, this latent space representation will be amenable to a MMD regularization later on.  This will be useful to avoid some of the limitiations of the KLD as a regularizer (overestimation of variance, and some degenerate convergences).\n",
    "Its sort of _hack_-ey but keeping the tooling equivalent to the betaVAE will ultimatel give some advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UpsampleBlock(Module):\n",
    "    def __init__(self, up_in_c:int, final_div:bool=True, blur:bool=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Upsampling using PixelShuffle_INCR and ConvLayer\n",
    "            - up_in_c :  \"Upsample input channel\"\n",
    "        \"\"\"\n",
    "        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)\n",
    "        ni = up_in_c//2\n",
    "        nf = ni if final_div else ni//2\n",
    "        self.conv1 = ConvLayer(ni, nf, **kwargs) # since we'll apply it by hand...\n",
    "        self.conv2 = ConvLayer(nf, nf, **kwargs)\n",
    "\n",
    "    def forward(self, up_in:Tensor) -> Tensor:\n",
    "        up_out = self.shuf(up_in)\n",
    "        return self.conv2(self.conv1(up_out))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class LatentLayer(Module):\n",
    "    \"\"\"\n",
    "    This layer encodes the latent \"bottleneck\" and is constructed to work with the specified VAE DataBlock be a replacement for \n",
    "    the variational (reparameter trick) layer for otherwise identical architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,in_features,latent_features):\n",
    "        \"\"\" \n",
    "        Compose a linear latent layer such that the mechanics are equivalent to the VAE\n",
    "              the \"dummy\" can be used for a shaddow logvar track a KLD estimate divergence \n",
    "              from latent gaussian prior\n",
    "              compute the variance across batchs for each latent feature as the dummy_var\n",
    "        \"\"\"       \n",
    "        self.latent = nn.Linear(in_features,latent_features)\n",
    "    \n",
    "    def forward(self,h):\n",
    "        z = self.latent(h)\n",
    "        #dummy_var = (z.var(dim=1).unsqueeze(-1).expand(z.size()) ) #variance across latent dim for each image\n",
    "        dummy_var = (z.var(dim=0).unsqueeze(0).expand(z.size()) ) #latent variance across batch\n",
    "        dummy_mu = z\n",
    "        return z, dummy_mu, dummy_var.log()\n",
    "        \n",
    "        #return z, torch.zeros_like(z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class AEEncoder(Module):\n",
    "    def __init__(self,arch_body,enc_dim, hidden_dim=None, im_size=IMG_SIZE):\n",
    "        \"\"\"\n",
    "        arch_body   list of layers (e.g. arch.children()[:cut])    \n",
    "        enc_dim,\n",
    "        hidden_dim. number of linear features to sandwich between the feature encoder and the latent layers\n",
    "        \"\"\"                \n",
    "        arch = arch_body + [Flatten()]\n",
    "\n",
    "        if hidden_dim:  # i.e. is not None\n",
    "            arch += [nn.Linear(enc_dim,hidden_dim)]\n",
    "                    # [LinBnDrop(enc_dim,hidden_dim,bn=True,p=0.0,act=nn.ReLU(),lin_first=True)]\n",
    "\n",
    "        self.encoder = nn.Sequential(*arch)\n",
    "        store_attr('enc_dim,hidden_dim')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "#### TODO: refactor this to take a \"BLOCK\" input so we can have either UpsampleBlocks or ResBlockUpsampleBlocks      \n",
    "class AEDecoder(Module):\n",
    "    def __init__(self, hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):\n",
    "        \"\"\"\n",
    "        Decoder Module made of `UpsampleBlock`s returning the latent representation back into an \"image\"\n",
    "            latent_dim - dimension of latent representation\n",
    "            hidden_dim - optional additional linear layer between the latent and decoder\n",
    "            im_size - passed to make sure we are scaling back to the right size\n",
    "            out_range - ensures the output is on teh same scale as the _normalized_ input image\n",
    "        \"\"\"\n",
    "        #decoder        \n",
    "        n_blocks = 5              \n",
    "        BASE = im_size//2**5   \n",
    "        \n",
    "        hidden = im_size*BASE*BASE if hidden_dim is None else hidden_dim\n",
    "        z_fc = [nn.Linear(latent_dim,hidden)] # [LinBnDrop(latent_dim,hidden,bn=True,p=0.0,act=nn.ReLU(),lin_first=True)]\n",
    "        if hidden_dim:  # i.e. is not None\n",
    "            z_fc += [nn.Linear(hidden,im_size*BASE*BASE)]  # should the hidden layer have activationa and/or batchnorm?\n",
    "            #z_fc += [LinBnDrop(hidden,im_size*n_blocks*n_blocks,bn=True,p=0.0,act=nn.ReLU(),lin_first=True)]\n",
    "        \n",
    "        \n",
    "        nfs = [3] + [2**i*BASE for i in range(n_blocks+1)] \n",
    "        nfs.reverse()\n",
    "        n = len(nfs)\n",
    "        \n",
    "        modules =  [UpsampleBlock(nfs[i]) for i in range(n - 2)]        \n",
    "        self.decoder = nn.Sequential(*z_fc,\n",
    "                                      ResizeBatch(im_size,BASE,BASE),\n",
    "                                      *modules,\n",
    "                                      ConvLayer(nfs[-2],nfs[-1],\n",
    "                                                ks=1,padding=0, norm_type=None, #act_cls=nn.Sigmoid) )\n",
    "                                                act_cls=partial(SigmoidRange, *out_range)))\n",
    "        \n",
    "        store_attr('latent_dim, hidden_dim,im_size,out_range')\n",
    "\n",
    "    def forward(self, z):    \n",
    "        return self.decoder(z)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to avoid the class wrappers to simplify the param splitting for freezing the pre-trained arcitecture.  \n",
    "We could enumerate the class layers and return sequential, but simply making some functions to put the layers togeter is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "    \n",
    "def build_AE_encoder(arch_body,enc_dim, hidden_dim=None, im_size=IMG_SIZE):\n",
    "    \"wrapper to sequential-ize AEEncoder class\"\n",
    "    encoder = AEEncoder(arch_body,enc_dim=enc_dim, hidden_dim=hidden_dim, im_size=im_size)\n",
    "    return nn.Sequential(*list(encoder.children()))\n",
    "\n",
    "    \n",
    "        \n",
    "def build_AE_decoder(hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):\n",
    "    \"wrapper to sequential-ize AEDecoder class\"\n",
    "    decoder = AEDecoder(hidden_dim=hidden_dim, latent_dim=latent_dim, im_size=im_size,out_range=out_range)\n",
    "    return nn.Sequential(*list(decoder.children()))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "        \n",
    "class AE(Module):\n",
    "    def __init__(self,enc_parts,hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):\n",
    "       \n",
    "        \"\"\"\n",
    "        inputs:  \n",
    "            arch, cut,pretrained\n",
    "            enc_dim\n",
    "            latent_dim\n",
    "            hidden_dim\n",
    "            \n",
    "        \"\"\"\n",
    "        enc_arch,enc_feats,name = enc_parts\n",
    "        \n",
    "        BASE = im_size//2**5\n",
    "        enc_dim = enc_feats * BASE**2  # 2**(3*3) * (im_size//32)**2 #(output of resneet) #12800\n",
    "\n",
    "        #encoder\n",
    "        self.encoder = build_AE_encoder(enc_arch,enc_dim=enc_dim, hidden_dim=hidden_dim, im_size=im_size)\n",
    "\n",
    "        in_dim = enc_dim if hidden_dim is None else hidden_dim\n",
    "        \n",
    "        # AE Bottleneck\n",
    "        self.bn = LatentLayer(in_dim,latent_dim)     \n",
    "        \n",
    "        #decoder\n",
    "        self.decoder = build_AE_decoder(hidden_dim=hidden_dim, latent_dim=latent_dim, im_size=im_size,out_range=out_range)\n",
    "\n",
    "        store_attr('name,enc_dim, in_dim,hidden_dim,latent_dim,im_size,out_range') # do i need all these?\n",
    "            \n",
    "    \n",
    "    def decode(self, z):    \n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.bn(h)\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #z, mu, logvar = self.encode(x)\n",
    "        #         h = self.encoder(x)\n",
    "        #         z, mu, logvar = self.bn(h) # reparam happens in the VAE layer\n",
    "        #         x_hat = self.decoder(z)\n",
    "        \n",
    "        z,mu,logvar = self.encode(x)  # z and mu are the same for \n",
    "        x_hat = self.decode(z)\n",
    "        latents = torch.stack([mu,logvar],dim=-1)\n",
    "        \n",
    "        return x_hat, latents # assume dims are [batch,latent_dim,concat_dim]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss\n",
    "\n",
    "These inherit from `Module`. (fastai's wrapper on `nn.Module`).  The base class `AELoss` initializes wiht a `batchmean`, `alpha`, and `useL1` parameters to set how the loss will be aggregated and regularized.  For the basice AutoEncoder we'll regularize the latent with a L1 to keep the magnitudes from exploding.   \n",
    "\n",
    "> NOTE: Here `batchmean` means we will divide the loss (either L1 or L2 depending on `useL1` flag) by which uses `reduction='sum'` by the batch size.   This technically makes it a _cost_ computed for each batch.   This same convention will be used later for KL-Divergence and MMD latent regularizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "        \n",
    "# class L1LatentReg(Module):\n",
    "#     \"\"\"\n",
    "#      add alpha?\n",
    "#     \"\"\"\n",
    "#     def __init__(self, batchmean=False):\n",
    "#         \"\"\"\n",
    "#         reduction 'sum', else 'batchmean'\n",
    "        \n",
    "#         \"\"\"\n",
    "#         l_one = self._L1mean if batchmean else self._L1\n",
    "#         store_attr('batchmean,l_one')\n",
    "    \n",
    "#     def _L1(self, a):\n",
    "#         return a.abs().sum()\n",
    "    \n",
    "#     def _L1mean(self, a):\n",
    "#         return a.abs().sum(dim=1).mean()\n",
    "\n",
    "#     def forward(self,z):\n",
    "#         return self.l_one(z)        \n",
    "            \n",
    "     \n",
    "class AELoss(Module):\n",
    "    \"\"\"\n",
    "    wrapper for loss_func which deals with potential annealed kl_weight\n",
    "    does MSE with 'mean' reduction\n",
    "    'batchmean'  averages as 'sum' MSE over batches\n",
    "    simple L1 regularizer on latent dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, batchmean=False, alpha=1.0,useL1=False):\n",
    "        \"\"\"\n",
    "        reduction 'sum'\n",
    "        \n",
    "        \"\"\"\n",
    "        pix_loss = MSELossFlat(reduction='sum') if not useL1 else L1LossFlat(reduction='sum')\n",
    "        store_attr('pix_loss,alpha,batchmean')\n",
    "        \n",
    "    def l_one_reg(self,pix_dim,z):\n",
    "        l_one = z.abs().sum()\n",
    "        l_one *= (3*pix_dim*pix_dim)/z.size()[1] \n",
    "        return l_one\n",
    "    \n",
    "    def forward(self, preds, *target):\n",
    "        \"\"\"\n",
    "        pred =(x_hat,KLD,kl_weight) #mu,log_var, kl_weight)\n",
    "        target is x (original) \n",
    "        \"\"\"        \n",
    "        # this handles the annealed kl_weight and passing the mu,logvar around we added...\n",
    "        if(len(preds) == 3):\n",
    "            x_hat, latents, _ = preds\n",
    "        else: #if len(preds) == 2:  # we should never get here... unless we delete teh callback\n",
    "            x_hat, latents = preds\n",
    "            \n",
    "\n",
    "        z, _ = latents.split(1,dim=2)\n",
    "        bs = latents.size()[0]\n",
    "\n",
    "        #note: both mse and l1_reg are summing errors over batches, and pixels or latents \n",
    "        pix_err = self.pix_loss(x_hat, target[0]) \n",
    "        pix_dim = x_hat.size()[-1]\n",
    "        l1_reg = self.l_one_reg(pix_dim,z)\n",
    "        \n",
    "        total =  pix_err + self.alpha*l1_reg\n",
    "        \n",
    "        total *= (1./bs) if self.batchmean else 1.0\n",
    "        \n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "\n",
    "These are called by the recorder callback and collect quantities to track training.  I made a simple meta class so I don't have to repreat the reset and mean value property for all the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MyMetric(Metric):\n",
    "    \"meta-class for simple average over epoch metric quantities\"\n",
    "    def reset(self):\n",
    "        \"Clear all targs and preds\"\n",
    "        self.vals = []\n",
    "    @property\n",
    "    def value(self):\n",
    "        return np.array(self.vals).mean()\n",
    "    \n",
    "class L1LatentReg(MyMetric):\n",
    "    \"Latent Regularizer with sum reduction and optinal batchmean scaling\"\n",
    "    def __init__(self,batchmean=False,alpha=1.0): \n",
    "        vals = []\n",
    "        store_attr('vals,batchmean,alpha')\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        #         pix_dim = to_detach(learn.y[0].size()[-1])\n",
    "        latents = to_detach(learn.pred[1])\n",
    "        bs = latents.size()[0]\n",
    "        z, _ = latents.split(1,dim=2)\n",
    "        #nll = torch.abs(recon_x - x).mean()\n",
    "        l_one = z.abs().sum()\n",
    "        #         l_one *= (3*pix_dim*pix_dim)/z.size()[1] \n",
    "        l_one *= (self.alpha/bs) if self.batchmean else self.alpha\n",
    "\n",
    "        self.vals.append(l_one)\n",
    "                        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some helpers for computing the KL Divergence as a function and a `Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def KLD(mu,logvar):\n",
    "    \"KLD helper which sum across latents, but not batches\"\n",
    "    return -0.5 * torch.sum(1 + logvar - mu*mu - logvar.exp(),1)\n",
    "\n",
    "        \n",
    "class KLDiv(Module):\n",
    "    \"\"\"\n",
    "    Module for computing the KL Divergence from a unit normal distribution.\n",
    "    'batchmean' option sums first and averages over batches \n",
    "    \"\"\"\n",
    "    def __init__(self, batchmean=False):\n",
    "        \"\"\"\n",
    "        reduction 'sum', else 'batchmean'\n",
    "        \n",
    "        \"\"\"\n",
    "        store_attr('batchmean')\n",
    "        \n",
    "    def __KLD(self,mu,logvar):\n",
    "        \"KLD helper which sum across latents, but not batches\"\n",
    "        return -0.5 * torch.sum(1 + logvar - mu*mu - logvar.exp(),1)\n",
    "\n",
    "\n",
    "    def forward(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        pred =(x_hat,KLD,kl_weight) #mu,log_var, kl_weight)\n",
    "        target is x (original) \n",
    "        \"\"\"        \n",
    "        kld = self.__KLD(mu,logvar)\n",
    "        \n",
    "        kld = kld.mean() if self.batchmean else kld.sum()\n",
    "        return kld\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "    \n",
    "class L2MeanMetric(MyMetric):\n",
    "    \"Mean square error\"\n",
    "    def __init__(self): self.vals = []\n",
    "    def accumulate(self, learn):\n",
    "        x = to_detach(learn.y[0])\n",
    "        recon_x = to_detach(learn.pred[0])\n",
    "        nll = (recon_x - x).pow(2).mean()\n",
    "        #nll = torch.mean((recon_x - x)**2)\n",
    "        self.vals.append(nll)\n",
    "\n",
    "class L1MeanMetric(MyMetric):\n",
    "    \"Mean absolute error\"\n",
    "    def __init__(self): self.vals = []\n",
    "    def accumulate(self, learn):\n",
    "        x = to_detach(learn.y[0])\n",
    "        recon_x = to_detach(learn.pred[0])\n",
    "        #nll = torch.abs(recon_x - x).mean()\n",
    "        nll = (recon_x - x).abs().mean()\n",
    "        self.vals.append(nll)\n",
    "        \n",
    "class L2Metric(MyMetric):\n",
    "    \"Sum square error\"\n",
    "    def __init__(self): self.vals = []\n",
    "    def accumulate(self, learn):\n",
    "        x = to_detach(learn.y[0])\n",
    "        recon_x = to_detach(learn.pred[0])\n",
    "        nll = (recon_x - x).pow(2).sum()\n",
    "        #nll = torch.mean((recon_x - x)**2)\n",
    "        self.vals.append(nll)\n",
    "\n",
    "class L1Metric(MyMetric):\n",
    "    \"Sum absolute error\"\n",
    "    def __init__(self): self.vals = []\n",
    "    def accumulate(self, learn):\n",
    "        x = to_detach(learn.y[0])\n",
    "        recon_x = to_detach(learn.pred[0])\n",
    "        #nll = torch.abs(recon_x - x).mean()\n",
    "        nll = (recon_x - x).abs().sum()\n",
    "        self.vals.append(nll)\n",
    "        \n",
    "        \n",
    "class L2BMeanMetric(MyMetric):\n",
    "    \"Summed square error average across batch \"\n",
    "    def __init__(self): self.vals = []\n",
    "    def accumulate(self, learn):\n",
    "        x = to_detach(learn.y[0])\n",
    "        recon_x = to_detach(learn.pred[0])\n",
    "        nll = (recon_x - x).pow(2).sum(dim=[1,2,3]).mean()\n",
    "        #nll = torch.mean((recon_x - x)**2)\n",
    "        self.vals.append(nll)\n",
    "\n",
    "class L1BMeanMetric(MyMetric):\n",
    "    \"Summed abs error average across batch \"\n",
    "    def __init__(self): self.vals = []\n",
    "    def accumulate(self, learn):\n",
    "        x = to_detach(learn.y[0])\n",
    "        recon_x = to_detach(learn.pred[0])\n",
    "        #nll = torch.abs(recon_x - x).mean()\n",
    "        nll = (recon_x - x).abs().sum(dim=[1,2,3]).mean()\n",
    "        self.vals.append(nll)\n",
    "        \n",
    "        \n",
    "class KLWeightMetric(MyMetric):\n",
    "    \"Injected KLD weighting\"\n",
    "    def __init__(self): self.vals = []\n",
    "    def accumulate(self, learn):\n",
    "        #kl = learn.model.kl_weight\n",
    "        kl = learn.opt.hypers[0]['kl_weight']\n",
    "        self.vals.append(to_detach(kl)) \n",
    "        \n",
    "        \n",
    "class RawKLDMetric(MyMetric):\n",
    "    \"KLD Metric, `batchmean` averages across batches\"\n",
    "    def __init__(self,batchmean=False): \n",
    "        vals = []\n",
    "        _KLD = KLDiv(batchmean=batchmean)\n",
    "        store_attr('vals,batchmean,_KLD')\n",
    "        \n",
    "    def accumulate(self, learn):\n",
    "        latents = learn.pred[1]\n",
    "        mu, logvar = latents.split(1,dim=2)\n",
    "        kld = self._KLD(mu,logvar)\n",
    "        self.vals.append(to_detach(kld)) \n",
    "        \n",
    "class WeightedKLDMetric(MyMetric):\n",
    "    \"\"\"weighted KLD Metric, `batchmean` averages across batches\n",
    "            the \"effective\" KLD regularization in e.g. a 𝜷-BAE\n",
    "    \"\"\"\n",
    "    def __init__(self,batchmean=False,alpha=1.0): \n",
    "        vals = []\n",
    "        _KLD = KLDiv(batchmean=batchmean)\n",
    "        store_attr('vals,batchmean,alpha,_KLD')\n",
    "        \n",
    "    def accumulate(self, learn):      \n",
    "        latents = learn.pred[1]\n",
    "        mu, logvar = latents.split(1,dim=2)\n",
    "        kld = self.alpha*self._KLD(mu,logvar)\n",
    "        self.vals.append(to_detach(kld)) \n",
    "        #         latents = to_detach(learn.pred[1])\n",
    "        #         mu, logvar = latents.split(1,dim=2)\n",
    "        #         kld = _KLD(mu,logvar).mean() if self.batchmean else _KLD(mu,logvar).sum()\n",
    "        #         self.vals.append(self.alpha*kld) \n",
    "        \n",
    "        \n",
    "class MuMetric(MyMetric):\n",
    "    \"average latent value (e.g. avg(`mu`)\"\n",
    "    def __init__(self): self.vals = []\n",
    "    def accumulate(self, learn):\n",
    "        latents = to_detach(learn.pred[1])\n",
    "        mu, logvar = latents.split(1,dim=2)\n",
    "        self.vals.append(mu.mean())\n",
    "\n",
    "    \n",
    "class MuSDMetric(MyMetric):\n",
    "    \"standard deviation of latent 𝝁 value (e.g. std(`mu`) )\"\n",
    "    def __init__(self): self.vals = []\n",
    "    def accumulate(self, learn):\n",
    "        latents = to_detach(learn.pred[1])\n",
    "        mu, logvar = latents.split(1,dim=2)\n",
    "        self.vals.append(mu.std())\n",
    "\n",
    "    \n",
    "class StdMetric(MyMetric):\n",
    "    \"average of latent 𝝈 value (e.g. std(exp(.5*`logvar`) )\"\n",
    "    def __init__(self): self.vals = []\n",
    "    def accumulate(self, learn):\n",
    "        latents = learn.pred[1]\n",
    "        mu, logvar = latents.split(1,dim=2)\n",
    "        std = torch.exp(0.5 * logvar).mean()\n",
    "        self.vals.append(to_detach(std)) \n",
    "        \n",
    "class StdSDMetric(MyMetric):\n",
    "    \"standard deviation of latent 𝝈 value (e.g. std(exp(.5*`logvar`) )\"\n",
    "    def __init__(self): self.vals = []\n",
    "    def accumulate(self, learn):\n",
    "        latents = learn.pred[1]\n",
    "        mu, logvar = latents.split(1,dim=2)\n",
    "        std = torch.exp(0.5 * logvar).std()\n",
    "        self.vals.append(to_detach(std)) \n",
    "        \n",
    "class LogvarMetric(MyMetric):\n",
    "    \"average of latent log(𝝈*𝝈) value (e.g. mean(`logvar`))\"\n",
    "    def __init__(self): self.vals = []\n",
    "    def accumulate(self, learn):\n",
    "        latents = to_detach(learn.pred[1])\n",
    "        mu, logvar = latents.split(1,dim=2)\n",
    "        self.vals.append(logvar.mean())\n",
    "                \n",
    "class LogvarSDMetric(MyMetric):\n",
    "    \"standard deviation of latent log(𝝈*𝝈)  value (e.g. std(`logvar`)\"\n",
    "    def __init__(self): self.vals = []\n",
    "    def accumulate(self, learn):\n",
    "        latents = to_detach(learn.pred[1])\n",
    "        mu, logvar = latents.split(1,dim=2)\n",
    "        self.vals.append(logvar.std())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metrics helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def default_AE_metrics(alpha,batchmean,useL1):\n",
    "    \"long-ish default list of metrics for the AE\"\n",
    "    \n",
    "    first = L2BMeanMetric() if batchmean else L2MeanMetric()\n",
    "    second = L1BMeanMetric() if batchmean else L2MeanMetric()\n",
    "    \n",
    "    if useL1: first,second = second,first\n",
    "        \n",
    "    metrics = [first,\n",
    "                L1LatentReg(batchmean=batchmean,alpha=alpha),\n",
    "                MuMetric(), \n",
    "                StdMetric(),\n",
    "                LogvarMetric(),\n",
    "                second,\n",
    "                WeightedKLDMetric(batchmean=batchmean,alpha=alpha),\n",
    "                MuSDMetric(), \n",
    "                LogvarSDMetric(),              \n",
    "               ]\n",
    "    return metrics\n",
    "\n",
    "def short_AE_metrics(alpha,batchmean,useL1):\n",
    "    \"short default list of metrics for the AE\"\n",
    "    \n",
    "    first = L2BMeanMetric() if batchmean else L2MeanMetric()\n",
    "    second = L1BMeanMetric() if batchmean else L2MeanMetric()\n",
    "    \n",
    "    if useL1: first,second = second,first\n",
    "        \n",
    "    metrics = [first,\n",
    "                L1LatentReg(batchmean=batchmean,alpha=alpha),\n",
    "                MuMetric(),              \n",
    "               ]\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback - AnnealedLoss\n",
    "\n",
    "The AnnealedLoss Callback basically injects a kl_weight parameter into the loss so we can start training without the full KLD regularization for the beta-VAE version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fastai `Learner` class does the training loop.  It took me a little digging into the code to figure out how Metrics are called since its not really stated anywhere in the documentation (_Note: create PR for fastai for extra documentation on `Metrics` logic_).  By default one of the key `Callbacks` is the `Recorder`.  It prints out the training summary at each epoch (via `ProgressCallBack`) and collects all the `Metrics`. Which by default only loss is a `train_met` and others are `valid_met`.\n",
    "The `Recorder` resets (maps `reset()` to all mets) the metrics `before_train` and `before_valid`. The `Recorder` maps `accumulate()` to the metrics on `after_batch`.  Finally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AnnealedLossCallback will inject the latent mu and logvar and a kl_weight variable into our loss.  The `mu` and `logvar` will be used to compute the KLD. The kl_weight is a scheduled weighting for the KLD. You can see the schedule graph of the parameter. At the beginning it will be 0, thus the KLD part of the loss will get ignored. So during 10% of training, we will fit a normal auto-encoder. Then gradually for 30% of trainning, increase kl_weight to 1 and then remain there for the remaining training time so that the auto encoder now becomes full variational. The way this callback is done, the loss will receive this parameter, but not the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "        \n",
    "class AnnealedLossCallback(Callback):\n",
    "    \"injects `kl_weight` for access during loss function calculation\"\n",
    "    def after_pred(self):\n",
    "        kl_weight = self.learn.pred[0].new(1)\n",
    "        kl_weight[0] = self.opt.hypers[0]['kl_weight'] if 'kl_weight' in self.opt.hypers[0].keys() else 1.0\n",
    "        self.learn.pred = self.learn.pred + (kl_weight,)\n",
    "    def after_batch(self):\n",
    "        pred, latents, _ = self.learn.pred\n",
    "        self.learn.pred = (pred,latents)\n",
    "        \n",
    "        \n",
    "def default_KL_anneal_in():\n",
    "    \"reasonable default for 'warming up' the KL Div\"\n",
    "    return combine_scheds([ .7, .3], [SchedCos(0,1), SchedNo(1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets start with ~10 epochs of no KL regularizaiton, and ramp through the next 5\n",
    "n_epochs = 10\n",
    "f_init = combine_scheds([.1, .7, .2], [SchedNo(0,0),SchedCos(0,1), SchedNo(1,1)])\n",
    "# f = combine_scheds([.8, .2], [SchedCos(0,0), SchedCos(0,.5)])\n",
    "p = torch.linspace(0.,1.,100)\n",
    "pp = torch.linspace(0.,1.*n_epochs,100)\n",
    "\n",
    "plt.plot(pp,[f_init(o) for o in p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> WARNING: Avoid using early stopping because the AnnealedLossCallback will make the loss grow once the KL divergence weight kicks in. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to note something here that was a little confusing to me: `params(model)` is a builtin fastai `PyTorch.core` function which returns all of the parameters of the modules.    i.e.\n",
    "    \n",
    "```python    \n",
    "def params(m):\n",
    "    \"Return all parameters of `m`\"\n",
    "    return [p for p in m.parameters()]\n",
    "```\n",
    "\n",
    "\n",
    "The toplevel `fastai core` functions with simple names that _almost_ match class attributes was one of my biggest stumbling blocks in getting acquainted with the fastai v2 API.  (The other is the documentation which is _autogenerated_ by the fastdev frameworks from their development noteboooks.  More on that struggle and my tips if that is troblesome for you later (here).\n",
    "\n",
    "> NOTE: that it is crucial that you don't freeze the batch norm layers.   The `bn_splitter` collects out all the batchnorm layers.  The simple splitting we do only freezes the `encoder` and leaves the latent layers (i.e. VAE or linear encoding bottlenedck) and the `decoder` in a parameter group with the batchnorm layers.\n",
    "\n",
    "\n",
    "\n",
    "#### parameter `Splitters`\n",
    "\n",
    "> WARNING:  there are two completely different `splitter`s in the FastAI API.  This `splitter` groups the model parameters into groups for `freezing` and for progressive learning rates. (The other one is splits data into train and validate.  I got imminiently confused when I first started with the API by this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO:  more sophisticated parameter splitting to enable progressive learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def bn_splitter(m):\n",
    "    \"splits all the batchnorm layers out\"\n",
    "    def _bn_splitter(l, g1, g2):\n",
    "        if isinstance(l, nn.BatchNorm2d): g2 += l.parameters()\n",
    "        elif hasattr(l, 'weight'): g1 += l.parameters()\n",
    "        for ll in l.children(): _bn_splitter(ll, g1, g2)\n",
    "        \n",
    "    g1,g2 = [],[]\n",
    "    _bn_splitter(m[0], g1, g2)\n",
    "    \n",
    "    g2 += m[1:].parameters()\n",
    "    return g1,g2\n",
    "\n",
    "def resnetVAE_split(m): \n",
    "    \"simple splitter to freeze the non batch norm pre-trained encoder\"\n",
    "    to_freeze, dont_freeze = bn_splitter(m.encoder)\n",
    "    #return L(to_freeze, dont_freeze + params(m.bn)+params(m.dec[:2]), params(m.dec[2:]))\n",
    "    return L(to_freeze, dont_freeze + params(m.bn)+params(m.decoder))\n",
    "    #return L(fz, nofz + params(m.bn)+params(m.dec[:6]), params(m.dec[6:]))\n",
    "    \n",
    "\n",
    "    \n",
    "def AE_split(m): \n",
    "    \"generic splitter for my AE classes- BVAE & AE & MMDVAE.\"\n",
    "    to_freeze, dont_freeze = bn_splitter(m.encoder)\n",
    "    return L(to_freeze, dont_freeze + params(m.bn)+params(m.decoder))\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other encoder and decoder types\n",
    "\n",
    "> 1. MobileNet_v2 as the encoder, as a continuation of the original Sneaker Finder\n",
    "\n",
    "> 2. simple bowtie convolutional encoder / decoder (Mimics the GOAT medium blog)\n",
    "    - Architecture Hyperparameters:\n",
    "        - Latent Size (research default 256, production default 32)\n",
    "        - Filter Factor Size (research default 16, production default 32)\n",
    "        - Latent Linear Hidden Layer Size (research default 2048, production default 1024)\n",
    "        - The encoder architecture is as follows with research defaults from above:\n",
    "            - Input 3x128x128 (conv2d block [conv2d, batchnorm2d, relu])\n",
    "            - 16x64x64 (conv2d block [conv2d, batchnorm2d, relu])\n",
    "            - 32x32x32 (conv2d block [conv2d, batchnorm2d, relu])\n",
    "            - 64x16x16 (conv2d block [conv2d, batchnorm2d, relu])\n",
    "            - 128x8x8 (conv2d block [conv2d, batchnorm2d, relu])\n",
    "            - Flatten to 8192\n",
    "            - 2048 (linear block [linear, batchnorm1d, relu])\n",
    "            - Split the 2048 dimension into mu and log variance for the parameters of the latent distribution \n",
    "            - Latent mu size 256 (linear layer only with bias)\n",
    "            - Latent logvar size 256 (linear layer only with bias)\n",
    "        - In the middle here you can break out the BCE and KLD loss for the final loss term and use the standard reparam trick to sample from the latent distribution.\n",
    "        - Decoder architecture an exact mirror \n",
    "            - Input 256\n",
    "            - 2048 (linear block [linear, relu])\n",
    "            - 8192 (linear block [linear, batchnorm1d, relu])\n",
    "            - reshape (128x8x8)\n",
    "            - 64x16x16 (conv2d transpose block [convtranspose2d, batchnorm2d, relu])\n",
    "            - 32x32x32 (conv2d transpose block [convtranspose2d, batchnorm2d, relu])\n",
    "            - 16x64x64 (conv2d transpose block [convtranspose2d, batchnorm2d, relu])\n",
    "            - 3x128x128 (conv2d transpose [convtranspose2d, sigmoid]\n",
    "        - For weight initialization I used a normal distribution centered at zero with 0.02 set for the stddev. Optimizer: Adam with default parameters, if I were to do it over again I'd spend more time here understanding the learning dynamics. The dataset was about ~10,000 with a 70/20/10 split, batch size 64, over 120 epochs, with a learning schedule to reduce when the loss started to plateau. No crazy image augmentation just resizing and standards flips. I used the ANN package Annoy to do the NN search for prod, normalizing the embeddings and using the cosine similarity, ANN factor was 128 for num_trees. \n",
    "\n",
    "\n",
    "> 3. MMD regularized VAE where the latents are drawn from a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO: Ranger optimizer might really help .. test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the transfer learning VAE tooling we previously built.  We just need to create the convolutional encoder and pass it in... Note that we don't have a pre-trained option, so DON'T FREEZE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just wrap that simple conv block architecture into a _builder_.   And a meta-wrapper to let us call the conv_encoder and pre-trained options with the same function.   (I'll also put the `get_pretrained_parts` function here now even though we won't use it till the next section, so that we can make the `get_encoder_parts` generic wrapper handle both properly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#### TODO: refactor this to take a \"BLOCK\" input so we can have either ConvLayer or ResBlock pieces     \n",
    "def get_conv_parts(im_size=IMG_SIZE):\n",
    "    \"\"\"\n",
    "    make a simple convolutional ladder encoder\n",
    "    \"\"\"\n",
    "    n_blocks = 5\n",
    "    BASE = im_size//2**5        \n",
    "    nfs = [3]+[(2**i)*BASE for i in range(n_blocks)] \n",
    "    n = len(nfs)\n",
    "\n",
    "    modules =  [ConvLayer(nfs[i],nfs[i+1],\n",
    "                            ks=5,stride=2,padding=2) for i in range(n - 1)]       \n",
    "\n",
    "    return modules,nfs[-1],'vanilla'\n",
    "\n",
    "\n",
    "\n",
    "def get_pretrained_parts(arch=resnet18):\n",
    "    \"this works for mobilnet_v2, resnet, and xresnet\"\n",
    "    cut = model_meta[arch]['cut']\n",
    "    name = arch.__name__\n",
    "    arch = arch(pretrained=True)\n",
    "    enc_arch = list(arch.children())[:cut]\n",
    "    enc_feats = 512\n",
    "    return enc_arch, enc_feats, name\n",
    "\n",
    "\n",
    "\n",
    "def get_encoder_parts(enc_type='vanilla',im_size=IMG_SIZE):\n",
    "    encoder_parts = get_conv_parts(im_size=im_size) if isinstance(enc_type,str) else get_pretrained_parts(arch=enc_type)\n",
    "    return encoder_parts # returns enc_arch,enc_dim,arch.__name__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Replace these training epochs with tests and maybe copy in some examples of the result?\n",
    "##   \n",
    "\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "# equalize KLDiv wrt errors per pixel\n",
    "alpha = 3*IMG_SIZE*IMG_SIZE/latent_dim\n",
    "alpha /= 20  # 5% retularizer\n",
    "\n",
    "batchmean = True \n",
    "useL1 = False\n",
    "hidden_dim = None\n",
    "# SaveModelCallback(fname=datetime.now().strftime('%Y-%m-%d %Hh%M.%S'), every_epoch=True), \n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(),  ParamScheduler({'kl_weight': SchedNo(1.,1.) })]\n",
    "\n",
    "metrics = default_AE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "block = get_ae_DataBlock(aug=True)\n",
    "batch_size = 64\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "arch='vanilla'\n",
    "\n",
    "vae = AE(get_encoder_parts(arch), hidden_dim=hidden_dim,latent_dim=latent_dim, im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "\n",
    "# let beta be calculated by : 3*im_size*im_size/latent_dim\n",
    "loss_func = AELoss(batchmean=batchmean,alpha=alpha,useL1=useL1)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split) #.to_fp16() #wd=config['wd'],opt_func=ranger,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: The `to_fp16()` callbacks work but increasing the batch size doesn't really speed things up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "# equalize KLDiv wrt errors per pixel\n",
    "alpha = 3*IMG_SIZE*IMG_SIZE/latent_dim\n",
    "alpha /= 20  # 5% retularizer\n",
    "\n",
    "batchmean = True \n",
    "useL1 = False\n",
    "hidden_dim = None\n",
    "# SaveModelCallback(fname=datetime.now().strftime('%Y-%m-%d %Hh%M.%S'), every_epoch=True), \n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(),  ParamScheduler({'kl_weight': SchedNo(1.,1.) })]\n",
    "\n",
    "metrics = default_AE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "block = get_ae_DataBlock(aug=True)\n",
    "batch_size = 64\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "arch='vanilla'\n",
    "\n",
    "vae = AE(get_encoder_parts(arch), hidden_dim=hidden_dim,latent_dim=latent_dim, im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "\n",
    "# let beta be calculated by : 3*im_size*im_size/latent_dim\n",
    "loss_func = AELoss(batchmean=batchmean,alpha=alpha,useL1=useL1)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split) #.to_fp16() #wd=config['wd'],opt_func=ranger,\n",
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constructing VAE with Module Class Layers\n",
    "\n",
    "For several of the decoder and \"sampler\" layers I might want to turn off the nonlinearity to give us more reasonable \"gaussian\" outputs to the Variational layer and the generated image which will is compared with the ImageNetStats batch-normalized image.\n",
    "\n",
    "\n",
    "\n",
    "> IMPORTANT VAE TIP!!!   Make sure NOT to use batch normalization and non-linearity in the linear layers of the VAE.  The normalization will affect the representation and the KLD constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class VAELinear(Module):\n",
    "    \"maps hidden (input) features to two latents (mu and logvar)\"\n",
    "    def __init__(self,in_features,latent_features):\n",
    "        self.mu_linear = nn.Linear(in_features,latent_features)\n",
    "        self.logvar_linear = nn.Linear(in_features,latent_features)\n",
    "\n",
    "    def forward(self,h):\n",
    "        #h = self.fc_in(h)\n",
    "        return self.mu_linear(h), self.logvar_linear(h)\n",
    "\n",
    "    \n",
    "    \n",
    "class VAELayer(Module):\n",
    "    \"\"\"\n",
    "    The VAE : in_features to latent_features through \n",
    "        the \"Variational\" magic: \"reparamaterization trick\"\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,latent_features):\n",
    "        self.mu_logvar = VAELinear(in_features,latent_features)\n",
    "\n",
    "    # \n",
    "    def reparam(self,mu,logvar):\n",
    "        # should we pass through a deterministic code when not training?\n",
    "        if False: return mu # self.training\n",
    "    \n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    \n",
    "    def forward(self,h):\n",
    "        mu,logvar = self.mu_logvar(h)\n",
    "        #logvar = F.softplus(logvar)   # force logvar>0\n",
    "        z = self.reparam(mu,logvar) # adds the noise by the reparam trick\n",
    "        \n",
    "        return z, mu, logvar\n",
    "    \n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple BVAE class from VAE layer class components\n",
    "\n",
    "This creates a pair of latents from which we can perform the \"resample\" trick. \n",
    "\n",
    "> NOTE:  this is a $\\beta$-VAE (hence `BVAE` because we have a weighting factor for the KL Divergence regularazation factor which acts as a Legrangian).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together gives us our VAE!   Note that we'll pass in the \"parts\" of the encoder for ease of using pretrained (or not) architectures.   The model name will correspond to the architecture of the encoder via `name`.\n",
    "\n",
    "Note that the `BVAE` can simply inherit from the `AE` class we defined above.  Really the only difference in the `__init__` function is that a `VAELayer` which performs the reparameterization trick replaces the `AElayer` as `self.bn` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "### TODO:  refactor the BVAE and AE to a single architecture... with a \"sample\" function ot \n",
    "\n",
    "class BVAE(AE):\n",
    "    \"\"\"\n",
    "    simple VAE made with an encoder passed in, and some builder function for the Latent (VAE reparam trick) and decoder\n",
    "    \"\"\"\n",
    "    def __init__(self,enc_parts,hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):\n",
    "       \n",
    "        \"\"\"\n",
    "        inputs:  \n",
    "            enc_arch (pre-cut / pretrained)\n",
    "            enc_dim\n",
    "            latent_dim\n",
    "            hidden_dim\n",
    "            im_size,out_range\n",
    "        \"\"\"\n",
    "        enc_arch,enc_feats,name = enc_parts\n",
    "\n",
    "        # encoder\n",
    "        #  arch,cut = xresnet18(pretrained=True),-4\n",
    "        #  enc_arch = list(arch.children())[:cut]\n",
    "        \n",
    "        BASE = im_size//2**5\n",
    "        enc_dim = enc_feats * BASE**2  # 2**(3*3) * (im_size//32)**2 #(output of resneet) #12800\n",
    "\n",
    "        self.encoder = build_AE_encoder(enc_arch,enc_dim=enc_dim, hidden_dim=hidden_dim, im_size=im_size)\n",
    "\n",
    "        in_dim = enc_dim if hidden_dim is None else hidden_dim\n",
    "\n",
    "        # VAE Bottleneck\n",
    "        self.bn = VAELayer(in_dim,latent_dim)     \n",
    "\n",
    "        #decoder\n",
    "        self.decoder = build_AE_decoder(hidden_dim=hidden_dim, latent_dim=latent_dim, im_size=im_size,out_range=out_range)\n",
    "\n",
    "        store_attr('name,enc_dim, in_dim,hidden_dim,latent_dim,im_size,out_range') # do i need all these?\n",
    "\n",
    "\n",
    "#     def decode(self, z):    \n",
    "#         return self.decoder(z)\n",
    "\n",
    "#     def encode(self, x):\n",
    "#         h = self.encoder(x)\n",
    "#         z, mu, logvar = self.bn(h) # reparam happens in the VAE layer\n",
    "#         return z, mu, logvar\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         #z, mu, logvar = self.encode(x)\n",
    "#         #         h = self.encoder(x)\n",
    "#         #         z, mu, logvar = self.bn(h) # reparam happens in the VAE layer\n",
    "#         #         x_hat = self.decoder(z)\n",
    "        \n",
    "#         z,mu,logvar = self.encode(x)\n",
    "#         x_hat = self.decode(z)\n",
    "#         latents = torch.stack([mu,logvar],dim=-1)\n",
    "        \n",
    "#         return x_hat, latents # assume dims are [batch,latent_dim,concat_dim]\n",
    "\n",
    "    \n",
    "# # AE \n",
    "#    def decode(self, z):    \n",
    "#         return self.decoder(z)\n",
    "    \n",
    "#     def encode(self, x):\n",
    "#         h = self.encoder(x)\n",
    "#         return self.bn(h)\n",
    "            \n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         pass the \"latents\" out to keep the learn mechanics consistent... \n",
    "#         \"\"\"\n",
    "#         h = self.encoder(x)\n",
    "#         z,logvar = self.bn(h)\n",
    "#         x_hat = self.decoder(z)                    \n",
    "#         latents = torch.stack([z,logvar] ,dim=-1)\n",
    "\n",
    "#         return x_hat , latents\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a nice wrapper for building the encoder parts above.\n",
    "\n",
    "```python\n",
    "def get_pretrained_parts(arch=resnet18):\n",
    "    \"this works for mobilnet_v2, resnet, and xresnet\"\n",
    "    cut = model_meta[arch]['cut']\n",
    "    name = arch.__name__\n",
    "    arch = arch(pretrained=True)\n",
    "    enc_arch = list(arch.children())[:cut]\n",
    "    enc_feats = 512\n",
    "    return enc_arch, enc_feats, name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, we've verified the arcitecture works, but we need to train it with a loss that constrains the variational layers with the KL Divergence.  Otherwise the simple MSE will diverge.  \n",
    "\n",
    "### VAE Loss functions classes\n",
    "\n",
    "This simply adds a KLD regularizer to the latent space defined by two rank-1 tensors defining gaussian-prior latents.  i.e. a mean ($\\mu$) and standard deviation ($\\sigma$).  \n",
    "\n",
    "> NOTE: for convenience and numeric stability the $\\sigma$ is representated as a $\\log(\\sigma^{s})$ so the tensors are called `mu` and `logvar`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# called `after_batch`\n",
    "\n",
    "class BVAELoss(Module):\n",
    "    \"\"\"\n",
    "    Measures how well we have created the original image, \n",
    "    plus the KL Divergence with the unit normal distribution\n",
    "    batchmean option sums first and averages over batches (for smaller total error magnitudes.. cosmentic)\n",
    "    \"\"\"\n",
    "    def __init__(self, batchmean=False, alpha=1.0,useL1=False):\n",
    "        \"\"\"\n",
    "        reduction 'sum', else 'batchmean'\n",
    "        \n",
    "        \"\"\"\n",
    "        pix_loss = MSELossFlat(reduction='sum') if not useL1 else L1LossFlat(reduction='sum')\n",
    "        _KLD = KLDiv(batchmean=False) # force to full sum\n",
    "        store_attr('pix_loss,alpha,batchmean,_KLD')\n",
    "        \n",
    "        \n",
    "    def forward(self, preds, *target):\n",
    "        \"\"\"\n",
    "        pred =(x_hat,KLD,kl_weight) #mu,log_var, kl_weight)\n",
    "        target is x (original) \n",
    "        \"\"\"\n",
    "        \n",
    "        # this handles the annealed kl_weight and passing the mu,logvar around we added...\n",
    "        if(len(preds) == 3):\n",
    "            x_hat, latents, kl_weight = preds\n",
    "        else: #if len(preds) == 2:  # we should never get here... unless we delete the callback\n",
    "            x_hat, latents = preds\n",
    "            \n",
    "            kl_weight = x_hat[0].new(1)\n",
    "            kl_weight[0] = 1.0\n",
    "        \n",
    "        mu, logvar = latents.split(1,dim=2)\n",
    "\n",
    "        #note: both mse and KLD are summing errors over batches, and pixels or latents \n",
    "        pix_err = self.pix_loss(x_hat, target[0]) \n",
    "        kld_err = self.alpha * self._KLD(mu,logvar).sum() #_KLD doesn't sum over batches by default\n",
    "        total =  (pix_err + kld_err*kl_weight) \n",
    "        if self.batchmean: total *= (1./mu.size()[0])\n",
    "        return total\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def default_VAE_metrics(alpha,batchmean,useL1):\n",
    "    \"long default list of metrics for the VAE\"\n",
    "    \n",
    "    first = L2BMeanMetric() if batchmean else L2Metric()\n",
    "    second = L1BMeanMetric() if batchmean else L1Metric()\n",
    "    \n",
    "    if useL1: first,second = second,first\n",
    "        \n",
    "    metrics = [first,\n",
    "                MuMetric(), \n",
    "                StdMetric(),\n",
    "                LogvarMetric(),\n",
    "                WeightedKLDMetric(batchmean=batchmean,alpha=alpha),\n",
    "                KLWeightMetric(), \n",
    "                second,\n",
    "                MuSDMetric(), \n",
    "                StdSDMetric(), \n",
    "                LogvarSDMetric(),              \n",
    "               ]\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "def short_VAE_metrics(alpha,batchmean,useL1):\n",
    "    \"short default list of metrics for the AE\"\n",
    "    \n",
    "    first = L2BMeanMetric() if batchmean else L2MeanMetric()\n",
    "    second = L1BMeanMetric() if batchmean else L2MeanMetric()\n",
    "    \n",
    "    if useL1: first,second = second,first\n",
    "        \n",
    "    metrics = [first,\n",
    "                MuMetric(), \n",
    "                StdMetric(),\n",
    "                LogvarMetric(),\n",
    "                WeightedKLDMetric(batchmean=batchmean,alpha=alpha)              \n",
    "               ]\n",
    "    return metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we put everything together.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "alpha = 5\n",
    "batchmean = True\n",
    "useL1 = False\n",
    "hidden_dim = None\n",
    "\n",
    "\n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(), ParamScheduler({'kl_weight':  SchedNo(1.,1.) })]\n",
    "\n",
    "metrics = default_VAE_metrics(alpha,batchmean,useL1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this callback if we want to save the model at every epoch.  Which could be super useful if we were able to actually overfit our model. \n",
    "> 'SaveModelCallback(fname=datetime.now().strftime('%Y-%m-%d %Hh%M.%S'), every_epoch=True)' \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "block = get_ae_DataBlock(aug=True)\n",
    "\n",
    "batch_size = 128\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "enc_parts = get_pretrained_parts(arch=resnet18)\n",
    "\n",
    "rnet_vae = BVAE(enc_parts, hidden_dim=hidden_dim,latent_dim=latent_dim, im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "    \n",
    "loss_func = BVAELoss(batchmean=batchmean,alpha=alpha,useL1=False)\n",
    "\n",
    "learn = Learner(dls, rnet_vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split)\n",
    "\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: lf_finder does NOT work correctly with our annealed kl_weight... \n",
    "\n",
    "lr1,lr2=learn.lr_find()\n",
    "mlr = .5*(lr1+lr2)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr1,lr2]).log().mean().exp().tolist()\n",
    "\n",
    "lr1,lr2,mlr,gmlr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: The CallBacks need to be updated for the KL loss annealing schedule as we tune.  We want to turn the `kl_weight` to 1.0 for instance when using the learning rate finder `learn.lr_find()`, and after an initial _burn in_ where the KL_loss term gradually ramps in, setting the `kl_weight` to stay at unity will be useful as we separately turn the learning rate (e.g. `fit_one_cycle` or `fit_flat_cosine`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#put in the annealied KL_weight...\n",
    "learn.remove_cb(learn.cbs[-1])\n",
    "# add new constant scheduler\n",
    "\n",
    "learn.add_cb(ParamScheduler({'kl_weight': default_KL_anneal_in()} ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 5\n",
    "\n",
    "#learn.fit_one_cycle(n_epochs,lr_max= lr1)\n",
    "learn.fit_one_cycle(n_epochs)\n",
    "learn.show_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vae with pretrained resnet encoder seems to train to a much better end-point if we keep the resnet frozen.  Hence the commented out `learn.unfreeze()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace kl_weight scheduler... with a constant unity... (NOTE: should be robust to failure if we don't replace it)\n",
    "learn.remove_cb(learn.cbs[-1])\n",
    "# add new constant scheduler\n",
    "learn.add_cb(ParamScheduler({'kl_weight':  SchedNo(1.,1.) }) )\n",
    "\n",
    "#learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 20\n",
    "\n",
    "#learn.fit_one_cycle(epochs, lr_max= 1e-3)\n",
    "learn.fit_flat_cos(epochs,pct_start=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"_Vanilla_\" convolutional beta-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "# equalize KLDiv wrt errors per pixel\n",
    "#  alpha = 3*IMG_SIZE*IMG_SIZE/latent_dim\n",
    "alpha = 5\n",
    "batchmean = True\n",
    "useL1 = False\n",
    "\n",
    "# SaveModelCallback(fname=datetime.now().strftime('%Y-%m-%d %Hh%M.%S'), every_epoch=True), \n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(), ParamScheduler({'kl_weight': default_KL_anneal_in() })]\n",
    "\n",
    "metrics = default_VAE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "\n",
    "block = get_ae_DataBlock(aug=True)\n",
    "batch_size = 128\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "arch='vanilla'\n",
    "vae = BVAE(get_encoder_parts(arch), hidden_dim=None,latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "                   \n",
    "# let beta be calculated by : 3*im_size*im_size/latent_dim\n",
    "loss_func = BVAELoss(batchmean=batchmean,alpha=alpha,useL1=False)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split)#.to_fp16() #wd=config['wd'],opt_func=ranger,\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MMD-VAE\n",
    "The MMD replace latent regularization term in loss_fn (KLD) with Maximal Meanm Discrepancy.\n",
    "\n",
    "We'll make an MMDVAE class to keep things declarative, but its really just an AE. i.e. a linear latent layer.\n",
    "\n",
    "Additional background on MME from [https://github.com/Saswatm123/MMD-VAE]:\n",
    "\n",
    "> Maximum Mean Discrepancy Variational Autoencoder, a member of the InfoVAE family that maximizes Mutual Information between the Isotropic Gaussian Prior (as the latent space) and the Data Distribution.\\\n",
    "Short explanation: The traditional VAE is known as the ELBO-VAE, named after the Evidence Lower Bound used in its objective. The ELBO suffers from two problems: overestimation of latent variance, and uninformative latent information.\\\n",
    "The latter is because one of the objective's terms is the KL-Divergence between the Gaussian parameterized by the encoder and the Standard Isotropic Gaussian. This dissuades usage of the latent code, so that the KL-Divergence term is allowed to fall even further. It is important to note that the KL-Divergence should never truly reach zero, as that means the encoder is not learning useful features and cannot find feature locality, and the decoder is just randomly sampling from Standard Gaussian noise.\\\n",
    "The overestimation of variance results from the KL-Divergence term not being strong enough to balance against the Reconstruction Error, and thus the Encoder prefers to learn a multimodal latent distribution with spread apart means, leading to low training error as it overfits, but low quality samples as well, as the sampling distribution is assumed to be a Standard Isotropic Gaussian. One effort to mitigate this effect is the Disentangled Variational Autoencoder, which simply raises the weight on the KL-Divergence term. However, this increases the problem stated in the paragraph above since it further penalizes using the latent code.\\\n",
    "For more detailed explanations, I used these resources to learn, in order of usefulness to me:\\\n",
    "    - https://arxiv.org/pdf/1706.02262.pdf \\\n",
    "    - http://ruishu.io/2018/03/14/vae/ \\\n",
    "    - http://approximateinference.org/accepted/HoffmanJohnson2016.pdf \\\n",
    "    - https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/ \\\n",
    "    - http://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/ \\\n",
    "    - https://ermongroup.github.io/cs228-notes/inference/variational/ \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def gaussian_kernel(a, b):\n",
    "    \"helper for computing MMD\"\n",
    "    dim1_1, dim1_2 = a.shape[0], b.shape[0]\n",
    "    depth = a.shape[1]\n",
    "    a = a.view(dim1_1, 1, depth)\n",
    "    b = b.view(1, dim1_2, depth)\n",
    "    a_core = a.expand(dim1_1, dim1_2, depth)\n",
    "    b_core = b.expand(dim1_1, dim1_2, depth)\n",
    "    numerator = (a_core - b_core).pow(2).mean(2)/depth\n",
    "    return torch.exp(-numerator)\n",
    "\n",
    "def MMD(a, b):\n",
    "    \"Max Mean Discrepancy\"\n",
    "    return gaussian_kernel(a, a).mean() + gaussian_kernel(b, b).mean() - 2*gaussian_kernel(a, b).mean()\n",
    "        \n",
    "def rawMMD(a, b):\n",
    "    \"_raw_ values from gauss kernals, assuming that and b have the same shape\"\n",
    "    return gaussian_kernel(a, a) + gaussian_kernel(b, b) - 2*gaussian_kernel(a, b)\n",
    "\n",
    "\n",
    "\n",
    "# the MMDVAE is built on the basic AE archiecure\n",
    "class MMDVAE(AE): pass\n",
    "    \n",
    "\n",
    "class MaxMeanDiscrepancy(Module):\n",
    "    \"\"\"\n",
    "     MMD\n",
    "     add alpha?\n",
    "    \"\"\"\n",
    "    def __init__(self, batchmean=False):\n",
    "        \"\"\"\n",
    "        reduction 'mean', else 'batchmean' means only over batch\n",
    "        \n",
    "        \"\"\"\n",
    "        MMD = self._MMDsum if batchmean else self._MMDmean\n",
    "        store_attr('batchmean,MMD')\n",
    "    \n",
    "    def _gaus_ker(self,a, b):\n",
    "        \"gaussian kernal\"\n",
    "        dim1_1, dim1_2 = a.shape[0], b.shape[0]\n",
    "        depth = a.shape[1]\n",
    "        numerator = 1.0/depth\n",
    "        a = a.view(dim1_1, 1, depth)\n",
    "        b = b.view(1, dim1_2, depth)\n",
    "        a_core = a.expand(dim1_1, dim1_2, depth)\n",
    "        b_core = b.expand(dim1_1, dim1_2, depth)\n",
    "        a_m_b = a_core - b_core\n",
    "        numerator *= (a_m_b*a_m_b).mean(2)  \n",
    "        #numerator = (a_core - b_core).pow(2).mean(2)   /depth\n",
    "        return torch.exp(-numerator)\n",
    "    \n",
    "    def _rawMMD(self, a, b):\n",
    "        return self._gaus_ker(a, a) +  self._gaus_ker(b, b) - 2*self._gaus_ker(a, b)\n",
    "    \n",
    "    def _MMDmean(self, a, b):\n",
    "        return self._rawMMD( a, b).mean()\n",
    "\n",
    "    def _MMDsum(self, a, b):\n",
    "        return self._rawMMD( a, b).sum()\n",
    "    \n",
    "\n",
    "    def forward(self,true_samples, latent):\n",
    "#         bs = latents.size()[0]\n",
    "#         latent_dim = z.size()[1]\n",
    "#         true_samples = torch.randn((bs,latent_dim), requires_grad=False).cuda()\n",
    "        mmd = self.MMD(true_samples, latent)        \n",
    "        return mmd\n",
    "        \n",
    "        \n",
    "class MMDLoss(Module):\n",
    "    \"\"\"\n",
    "    Measures mean square error of prediction and original image, \n",
    "    regularized by MMD.\n",
    "    \n",
    "    Note: using reuction = 'mean' because it keeps the regularization relatively potent (i.e. pixels>>latents)\n",
    "    \"\"\"\n",
    "    def __init__(self, batchmean=False, alpha=1.0,useL1=False):\n",
    "        \"\"\"\n",
    "        reduction 'sum', else 'batchmean'\n",
    "        \n",
    "        \"\"\"\n",
    "        if batchmean:\n",
    "            pix_loss = MSELossFlat(reduction='sum') if not useL1 else L1LossFlat(reduction='sum') \n",
    "            #mmd = _MMDsum\n",
    "        else:\n",
    "            pix_loss = MSELossFlat(reduction='mean') if not useL1 else L1LossFlat(reduction='mean') \n",
    "            #mmd = _MMD\n",
    "\n",
    "        mmd = MaxMeanDiscrepancy(batchmean=batchmean)\n",
    "        store_attr('pix_loss,alpha,batchmean,mmd')\n",
    "        \n",
    "        \n",
    "    def forward(self, preds, *target):\n",
    "        \"\"\"\n",
    "        pred =(x_hat,KLD,kl_weight) #mu,log_var, kl_weight)\n",
    "        target is x (original) \n",
    "        \"\"\"\n",
    "        \n",
    "        # this handles the annealed kl_weight and passing the mu,logvar around we added...\n",
    "        if(len(preds) == 3):\n",
    "            x_hat, latents, kl_weight = preds\n",
    "        else: #if len(preds) == 2:  # we should never get here... unless we delete teh callback\n",
    "            x_hat, latents = preds\n",
    "            \n",
    "            kl_weight = x_hat[0].new(1)\n",
    "            kl_weight[0] = 1.0\n",
    "        \n",
    "        z, _ = latents.split(1,dim=2)\n",
    "\n",
    "        #note: both mse and KLD are summing errors over batches, and pixels or latents \n",
    "        pix_err = self.pix_loss(x_hat, target[0]) \n",
    "        \n",
    "\n",
    "        bs = latents.size()[0]\n",
    "        latent_dim = z.size()[1]\n",
    "        true_samples = torch.randn((bs,latent_dim), requires_grad=False).cuda()\n",
    "        mmd_loss = self.mmd(true_samples, z) * self.alpha    \n",
    "    \n",
    "        total =  (pix_err + mmd_loss*kl_weight) \n",
    "        total *= (1./bs) if self.batchmean else 1.0\n",
    "        return total\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "class MMDMetric(MyMetric):\n",
    "    def __init__(self,batchmean=False,alpha=1.0): \n",
    "        vals = []\n",
    "        #mmd = _MMDsum if batchmean else _MMD\n",
    "        mmd = MaxMeanDiscrepancy(batchmean=batchmean)\n",
    "\n",
    "        store_attr('vals,batchmean,alpha,mmd')\n",
    "        \n",
    "    def accumulate(self, learn):        \n",
    "        latents = learn.pred[1]\n",
    "        z, _ = latents.split(1,dim=2)\n",
    "\n",
    "        bs = latents.size()[0]\n",
    "        latent_dim = z.size()[1]\n",
    "        true_samples = torch.randn((bs,latent_dim), requires_grad=False).cuda()\n",
    "        mmd_loss = self.mmd(true_samples, z)   \n",
    "        mmd_loss *= (self.alpha/bs) if self.batchmean else self.alpha\n",
    "\n",
    "        self.vals.append(to_detach(mmd_loss))\n",
    "\n",
    "# export\n",
    "\n",
    "def short_MMEVAE_metrics(alpha,batchmean,useL1):\n",
    "    \"short list of metrics for the VAE\"\n",
    "    \n",
    "    first = L2BMeanMetric() if batchmean else L2MeanMetric()\n",
    "    second = L1BMeanMetric() if batchmean else L1MeanMetric()\n",
    "    \n",
    "    if useL1: first,second = second,first\n",
    "        \n",
    "    metrics = [first,\n",
    "                MMDMetric(batchmean=batchmean,alpha=alpha), \n",
    "                MuMetric(), \n",
    "                MuSDMetric(), \n",
    "                ]\n",
    "    return metrics\n",
    "\n",
    "def default_MMEVAE_metrics(alpha,batchmean,useL1):\n",
    "    \"long default list of metrics for the VAE\"\n",
    "    \n",
    "    first = L2BMeanMetric() if batchmean else L2MeanMetric()\n",
    "    second = L1BMeanMetric() if batchmean else L1MeanMetric()\n",
    "    \n",
    "    if useL1: first,second = second,first\n",
    "        \n",
    "    metrics = [first,\n",
    "                MMDMetric(batchmean=batchmean,alpha=alpha), \n",
    "                MuMetric(),\n",
    "                StdMetric(),\n",
    "                second,\n",
    "                MuSDMetric(), \n",
    "                LogvarMetric(),\n",
    "                L1LatentReg(batchmean=batchmean,alpha=alpha),\n",
    "                WeightedKLDMetric(batchmean=batchmean,alpha=alpha),\n",
    "                LogvarSDMetric()]               \n",
    "    return metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla MMD VAE\n",
    "\n",
    "Simply call our `get_encoder_parts` with `arch='vanilla'` in the `MMDVAE` builder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(),         \n",
    "               ParamScheduler({'kl_weight': SchedNo(1.,1.) })]\n",
    "alpha = 20\n",
    "\n",
    "batchmean = True\n",
    "useL1 = False\n",
    "hidden_dim = None\n",
    "metrics = default_MMEVAE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "arch='vanilla'\n",
    "vae = MMDVAE(get_encoder_parts(arch), hidden_dim=hidden_dim,latent_dim=latent_dim, im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "\n",
    "loss_func = MMDLoss(batchmean=batchmean,alpha=alpha,useL1=useL1)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split) #.to_fp16() #wd=config['wd'],opt_func=ranger,\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Encoder MMDVAE\n",
    "\n",
    "We build these by passing `resnet18` (_not_ `'resnet18'`) to `get_encoder_parts` helper for the parts to init the `MMDVAE`.\n",
    "\n",
    "First a traditional \"fine_tune\" type of training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_dim = 128\n",
    "\n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(),         \n",
    "               ParamScheduler({'kl_weight': SchedNo(1.,1.) })]\n",
    "\n",
    "alpha = 10\n",
    "\n",
    "batchmean = True\n",
    "useL1 = False\n",
    "hidden_dim = None\n",
    "metrics = default_MMEVAE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "\n",
    "arch=resnet18\n",
    "vae = MMDVAE(get_encoder_parts(arch), hidden_dim=hidden_dim,latent_dim=latent_dim, im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "\n",
    "loss_func = MMDLoss(batchmean=batchmean,alpha=alpha,useL1=useL1)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split) #.to_fp16() #wd=config['wd'],opt_func=ranger,\n",
    "    \n",
    "learn.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm things up...  low-ish learning rate + anealing kldiv retularizaiton into\n",
    "learn.freeze()\n",
    "n_epoch = 20\n",
    "#learn.fit_flat_cos(n_epoch) #, lr=1e-3, div_final=1e6, pct_start=0.2)\n",
    "learn.fit_flat_cos(n_epoch)#, lr=lr1, div_final=1e5, pct_start=0.5)\n",
    "learn.fit_one_cycle(n_epoch)#,lr_max=gmlr) #, lr_max= base_lr)\n",
    "\n",
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm things up...  low-ish learning rate + anealing kldiv retularizaiton into\n",
    "n_epoch = 20\n",
    "learn.unfreeze()\n",
    "#learn.fit_flat_cos(n_epoch, lr=lr1, div_final=1e6, pct_start=0.7)\n",
    "#learn.fit_flat_cos(n_epoch, lr=1e-3, div_final=1e5, pct_start=0.5)\n",
    "learn.fit_one_cycle(n_epoch) #, lr_max= base_lr)\n",
    "\n",
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### vanilla-resnet MMD VAE\n",
    "##### no freeze\n",
    "\n",
    "We build these by passing `resnet18` (_not_ `'resnet18'`) to `get_encoder_parts` helper for the parts to init the `MMDVAE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: our architecture trains best when simply starting with the pretrained weights.  Trying to \"fine_tune\" by training the backend on a _frozen_ resnet and then unfreezing doesn't work with the parameter groupings from the `AE_split`.  The `learn.unfreeze()` doesn't actually do anythign (_unfrozen_ is default) but makes is clear we are _un_ -frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_dim = 128\n",
    "\n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(),         \n",
    "               ParamScheduler({'kl_weight': SchedNo(1.,1.) })]\n",
    "\n",
    "\n",
    "alpha = 10\n",
    "\n",
    "batchmean = True\n",
    "useL1 = False\n",
    "hidden_dim = None\n",
    "metrics = default_MMEVAE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "\n",
    "arch=resnet18\n",
    "vae = MMDVAE(get_encoder_parts(arch), hidden_dim=hidden_dim,latent_dim=latent_dim, im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "\n",
    "loss_func = MMDLoss(batchmean=batchmean,alpha=alpha,useL1=useL1)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split) #.to_fp16() #wd=config['wd'],opt_func=ranger,\n",
    "    \n",
    "    \n",
    "    \n",
    "learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vanilla-resnet MMD-VAE\n",
    "##### ResBlocks replacing Conv2d in encoder AND decoder.\n",
    "'Vanilla' encoder architecture made with `ResBlock` layers instead of `ConvLayer` layers and `Mish` activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "## This block is for experimenting with making a builder which can take ResBlock or ConvLayers (ConvBlocks?  ConvLayer(ni,nf),ConvLayer(nf,nf)??\n",
    "\n",
    "def get_conv_parts2(enc_type='vanilla',im_size=IMG_SIZE):\n",
    "    \"\"\"\n",
    "    make a simple convolutional ladder encoder\n",
    "    TODO:  make a switch on enc_type\n",
    "        - vanilla, resnet or vanilla-res\n",
    "        -TODO: change to 'convblock' and 'resblock'\n",
    "    \"\"\"\n",
    "    n_blocks = 5\n",
    "    BASE = im_size//2**5        \n",
    "    nfs = [3]+[(2**i)*BASE for i in range(n_blocks)] \n",
    "    n = len(nfs)\n",
    "\n",
    "    \n",
    "    \n",
    "    modules =  [ConvLayer(nfs[i],nfs[i+1],\n",
    "                            ks=5,stride=2,padding=2) for i in range(n - 1)]       \n",
    "    \n",
    "#     modules =  [ResBlock(1, nfs[i],nfs[i+1], \n",
    "#                          stride=2, act_cls=Mish)  for i in range(n - 1)]       \n",
    "\n",
    "    return modules,nfs[-1],'vanilla'\n",
    "\n",
    "\n",
    "def get_encoder_parts2(enc_type='vanilla',im_size=IMG_SIZE):\n",
    "    encoder_parts = get_conv_parts2(enc_type=enc_type,im_size=im_size) if isinstance(enc_type,str) else get_pretrained_parts(arch=enc_type)\n",
    "    return encoder_parts # returns enc_arch,enc_dim,arch.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UpsampleResBlock(Module):\n",
    "    def __init__(self, up_in_c:int, final_div:bool=True, blur:bool=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Upsampling using PixelShuffle_INCR and ResBlocks\n",
    "        - up_in_c :  \"Upsample input channel\"\n",
    "        \"\"\"\n",
    "        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)\n",
    "        ni = up_in_c//2\n",
    "        nf = ni if final_div else ni//2\n",
    "        self.conv1 = ResBlock(1,ni, nf, **kwargs) # since we'll apply it by hand...\n",
    "        self.conv2 = ResBlock(1,nf, nf, **kwargs)\n",
    "\n",
    "    def forward(self, up_in:Tensor) -> Tensor:\n",
    "        up_out = self.shuf(up_in)\n",
    "        return self.conv2(self.conv1(up_out))\n",
    "    \n",
    "\n",
    "\n",
    "def get_resblockencoder_parts(enc_type='vanilla',im_size=IMG_SIZE):\n",
    "    \"\"\"\n",
    "    make a simple (hence 'vanilla') convolutional ladder encoder with ResBlock parts\n",
    "    \"\"\"\n",
    "    n_blocks = 5\n",
    "    BASE = im_size//2**5        \n",
    "    nfs = [3]+[(2**i)*BASE for i in range(n_blocks)] \n",
    "    n = len(nfs)\n",
    "\n",
    "    \n",
    "    modules =  [ResBlock(1, nfs[i],nfs[i+1], \n",
    "                          stride=2, act_cls=Mish)  for i in range(n - 1)]       \n",
    "\n",
    "    return modules,nfs[-1],'resblock'\n",
    "\n",
    "\n",
    "\n",
    "# def build_ResBlockAE_decoder(hidden_dim=2048, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):\n",
    "#         BASE = im_size//2**5\n",
    "#         #store_attr('enc_dim,latent_dim, hidden_dim,im_size')\n",
    "\n",
    "        \n",
    "\n",
    "#         #decoder\n",
    "#         n_blocks = 5        \n",
    "#         nfs = [3] + [2**i*n_blocks for i in range(n_blocks+1)] \n",
    "#         nfs.reverse()\n",
    "#         n = len(nfs)\n",
    "        \n",
    "#         modules =  [UpsampleResBlock(nfs[i]) for i in range(n - 2)]        \n",
    "#         decoder = nn.Sequential( LinBnDrop(latent_dim,hidden_dim,\n",
    "#                                                 bn=True,# batch normalizaiton shouldn't be a problem here\n",
    "#                                                 p=0.0,act=nn.ReLU(),lin_first=True),\n",
    "#                                      LinBnDrop(hidden_dim,im_size*n_blocks*n_blocks,\n",
    "#                                                 bn=True,# batch normalizaiton shouldn't be a problem here\n",
    "#                                                 p=0.0,act=nn.ReLU(),lin_first=True),\n",
    "#                                       ResizeBatch(im_size,n_blocks,n_blocks),\n",
    "#                                       *modules,\n",
    "#                                       ResBlock(1,nfs[-2],nfs[-1],\n",
    "#                                                 ks=1,padding=0, norm_type=None, #act_cls=nn.Sigmoid) )\n",
    "#                                                 act_cls=partial(SigmoidRange, *out_range)))\n",
    "        \n",
    "#         return decoder\n",
    "\n",
    "\n",
    "class ResBlockAEDecoder(Module):\n",
    "    def __init__(self, hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):\n",
    "        \"\"\"\n",
    "        Decoder Module made of ResBlocks returning the latent representation back into an \"image\"\n",
    "            latent_dim - dimension of latent representation\n",
    "            hidden_dim - optional additional linear layer between the latent and decoder\n",
    "            im_size - passed to make sure we are scaling back to the right size\n",
    "            out_range - ensures the output is on teh same scale as the _normalized_ input image\n",
    "        \"\"\"\n",
    "        #decoder\n",
    "        n_blocks = 5 \n",
    "        BASE = im_size//2**5               \n",
    "        \n",
    "        hidden = im_size*BASE*BASE if hidden_dim is None else hidden_dim\n",
    "        z_fc = [nn.Linear(latent_dim,hidden)]\n",
    "        if hidden_dim:  # i.e. is not None\n",
    "            z_fc += [nn.Linear(hidden,im_size*BASE*BASE)]\n",
    "        \n",
    "        nfs = [3] + [2**i*BASE for i in range(n_blocks+1)] \n",
    "        nfs.reverse()\n",
    "        n = len(nfs)\n",
    "        \n",
    "        modules =  [UpsampleResBlock(nfs[i]) for i in range(n - 2)]        \n",
    "        self.decoder = nn.Sequential(*z_fc,\n",
    "                                      ResizeBatch(im_size,BASE,BASE),\n",
    "                                      *modules,\n",
    "                                      ResBlock(1,nfs[-2],nfs[-1],\n",
    "                                                ks=1,padding=0, norm_type=None, #act_cls=nn.Sigmoid) )\n",
    "                                                act_cls=partial(SigmoidRange, *out_range)))\n",
    "        \n",
    "        store_attr('latent_dim, hidden_dim,im_size,out_range')\n",
    "\n",
    "    def forward(self, z):    \n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "            \n",
    "    \n",
    "def build_ResBlockAE_decoder(hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):\n",
    "    \"wrapper to sequential-ize ResBlockAEDecoder class\"\n",
    "    decoder = ResBlockAEDecoder(hidden_dim=hidden_dim, latent_dim=latent_dim, im_size=im_size,out_range=out_range)\n",
    "    return nn.Sequential(*list(decoder.children()))\n",
    "    \n",
    "\n",
    "    \n",
    "class ResBlockAE(AE):\n",
    "    def __init__(self,enc_parts,hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE,isVAE=False):\n",
    "       \n",
    "        \"\"\"\n",
    "        inputs:  \n",
    "            enc_parts - encoder architecture \n",
    "            latent_dim - dimension of latent representation\n",
    "            hidden_dim - optional additional linear layer between the latent and decoder\n",
    "            im_size - passed to make sure we are scaling back to the right size\n",
    "            out_range - ensures the output is on teh same scale as the _normalized_ input image\n",
    "            isVae - switch for the type of latent representation\n",
    "            \n",
    "        \"\"\"\n",
    "        enc_arch,enc_feats,name = enc_parts\n",
    "        \n",
    "        BASE = im_size//2**5\n",
    "        enc_dim = enc_feats * BASE**2  # 2**(3*3) * (im_size//32)**2 #(output of resneet) #12800\n",
    "\n",
    "        #encoder\n",
    "        self.encoder = build_AE_encoder(enc_arch,enc_dim=enc_dim, hidden_dim=hidden_dim, im_size=im_size)\n",
    "\n",
    "        in_dim = enc_dim if hidden_dim is None else hidden_dim\n",
    "        \n",
    "        # AE Bottleneck\n",
    "        latent = VAELayer if isVAE else LatentLayer\n",
    "        \n",
    "        self.bn = latent(in_dim,latent_dim)     \n",
    "        \n",
    "        #decoder\n",
    "        self.decoder = build_ResBlockAE_decoder(hidden_dim=hidden_dim, latent_dim=latent_dim, im_size=im_size,out_range=out_range)\n",
    "\n",
    "        store_attr('name,enc_dim, in_dim,hidden_dim,latent_dim,im_size,out_range') # do i need all these?\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "#     def decode(self, z):    \n",
    "#         return self.decoder(z)\n",
    "    \n",
    "#     def encode(self, x):\n",
    "#         h = self.encoder(x)\n",
    "#         return self.bn(h)\n",
    "            \n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         pass the \"latents\" out to keep the learn mechanics consistent... \n",
    "#         \"\"\"\n",
    "#         h = self.encoder(x)\n",
    "#         z,logvar = self.bn(h)\n",
    "#         x_reconst = self.decoder(z)                    \n",
    "#         latents = torch.stack([z,logvar] ,dim=-1)\n",
    "\n",
    "#         return x_reconst , latents\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can use the `ResBlockBVAE` which instantiates a `ResBlock` `decoder` to optimize the architecture.  This is following the fastAI API lessong from the \"bag of tricks\" ResNet paper (CITATION), which is a general true-ism which could be glibly states as: \"replacing a `Conv` with a `ResBlock` always gets you better results\".  The Class constructor `ResBlockAE` takes `isVAE` to switch between `AELayer` and `LatentLayer` latents.\n",
    "\n",
    "> TODO:  update AE class to make a VAE or AE based on the `isVAE` switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "alpha = 5 # doubled because latent is half?\n",
    "batchmean = True\n",
    "useL1 = False\n",
    "\n",
    "# SaveModelCallback(fname=datetime.now().strftime('%Y-%m-%d %Hh%M.%S'), every_epoch=True), \n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(), ParamScheduler({'kl_weight':  SchedNo(1.,1.) })]\n",
    "\n",
    "\n",
    "metrics = default_VAE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "\n",
    "block = get_ae_DataBlock(aug=True)\n",
    "batch_size = 128\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "arch='resnblock'\n",
    "vae = ResBlockAE(get_resblockencoder_parts(arch), hidden_dim=2048,latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE,isVAE= True)\n",
    "                   \n",
    "# let beta be calculated by : 3*im_size*im_size/latent_dim\n",
    "loss_func = BVAELoss(batchmean=batchmean,alpha=alpha,useL1=False)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split)#.to_fp16() #wd=config['wd'],opt_func=ranger,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next steps.\n",
    "\n",
    "- visualize activations in encoder decoder\n",
    "- improve training with progressive learning rates\n",
    "- G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export / provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 128 latents, alpha=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "# cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(),         \n",
    "#                SaveModelCallback(fname=datetime.now().strftime('%Y-%m-%d %Hh%M.%S'), every_epoch=True),\n",
    "#                ParamScheduler({'kl_weight': SchedNo(1.,1.) })]\n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(),         \n",
    "               ParamScheduler({'kl_weight': SchedNo(1.,1.) })]\n",
    "\n",
    "alpha = 10\n",
    "# note that alpha needs to be adjusted to scale MMD regularizer compared to error for batchmean=true\n",
    "#.  e.g.  *= 3*IMG_SIZE**2/latent_dim\n",
    "batchmean = True\n",
    "useL1 = False\n",
    "hidden_dim = None\n",
    "\n",
    "metrics = default_MMEVAE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "batch_size = 128\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "arch='resblock'\n",
    "vae = ResBlockAE(get_resblockencoder_parts(arch), hidden_dim=hidden_dim,latent_dim=latent_dim,  im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "  \n",
    "# let beta be calculated by : 3*im_size*im_size/latent_dim\n",
    "loss_func = MMDLoss(batchmean=batchmean,alpha=alpha,useL1=False)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split)#.to_fp16() #wd=config['wd'],opt_func=ranger,\n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1,lr2=learn.lr_find()\n",
    "mlr = .5*(lr1+lr2)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr1,lr2]).log().mean().exp().tolist()\n",
    "\n",
    "lr1,lr2,mlr,gmlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm things up...  low-ish learning rate + anealing kldiv retularizaiton into\n",
    "#learn.freeze()\n",
    "n_epoch = 200\n",
    "#learn.fit_flat_cos(n_epoch) #, lr=1e-3, div_final=1e6, pct_start=0.2)\n",
    "learn.fit_flat_cos(n_epoch)#, lr=lr1, div_final=1e5, pct_start=0.5)\n",
    "#learn.fit_one_cycle(n_epoch,lr_max=gmlr) #, lr_max= base_lr)\n",
    "\n",
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"MMDVae-{'TMP'}-latent{latent_dim}\"\n",
    "filename = f\"frozen-{prefix}-{learn.model.name}-alpha{alpha:d}_{datetime.now().strftime('%Y-%m-%d_%H.%M.%S')}\"\n",
    "\n",
    "learn.save(filename)\n",
    "#learn.export(f'{filename}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 128 latents, alpha=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "# cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(),         \n",
    "#                SaveModelCallback(fname=datetime.now().strftime('%Y-%m-%d %Hh%M.%S'), every_epoch=True),\n",
    "#                ParamScheduler({'kl_weight': SchedNo(1.,1.) })]\n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(),         \n",
    "               ParamScheduler({'kl_weight': SchedNo(1.,1.) })]\n",
    "\n",
    "alpha = 20\n",
    "# note that alpha needs to be adjusted to scale MMD regularizer compared to error for batchmean=true\n",
    "#.  e.g.  *= 3*IMG_SIZE**2/latent_dim\n",
    "batchmean = True\n",
    "useL1 = False\n",
    "hidden_dim = None\n",
    "\n",
    "metrics = default_MMEVAE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "batch_size = 128\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "arch='resblock'\n",
    "vae = ResBlockAE(get_resblockencoder_parts(arch), hidden_dim=hidden_dim,latent_dim=latent_dim,  im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "  \n",
    "# let beta be calculated by : 3*im_size*im_size/latent_dim\n",
    "loss_func = MMDLoss(batchmean=batchmean,alpha=alpha,useL1=False)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split)#.to_fp16() #wd=config['wd'],opt_func=ranger,\n",
    "     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1,lr2=learn.lr_find()\n",
    "mlr = .5*(lr1+lr2)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr1,lr2]).log().mean().exp().tolist()\n",
    "\n",
    "lr1,lr2,mlr,gmlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm things up...  low-ish learning rate + anealing kldiv retularizaiton into\n",
    "#learn.freeze()\n",
    "n_epoch = 200\n",
    "#learn.fit_flat_cos(n_epoch) #, lr=1e-3, div_final=1e6, pct_start=0.2)\n",
    "learn.fit_flat_cos(n_epoch)#, lr=lr1, div_final=1e5, pct_start=0.5)\n",
    "#learn.fit_one_cycle(n_epoch,lr_max=gmlr) #, lr_max= base_lr)\n",
    "\n",
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"MMDVae-{'TMP'}-latent{latent_dim}\"\n",
    "filename = f\"frozen-{prefix}-{learn.model.name}-alpha{alpha:d}_{datetime.now().strftime('%Y-%m-%d_%H.%M.%S')}\"\n",
    "\n",
    "learn.save(filename)\n",
    "#learn.export(f'{filename}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64 latents, alpha=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "\n",
    "# cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(),         \n",
    "#                SaveModelCallback(fname=datetime.now().strftime('%Y-%m-%d %Hh%M.%S'), every_epoch=True),\n",
    "#                ParamScheduler({'kl_weight': SchedNo(1.,1.) })]\n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(),         \n",
    "               ParamScheduler({'kl_weight': SchedNo(1.,1.) })]\n",
    "\n",
    "alpha = 10\n",
    "# note that alpha needs to be adjusted to scale MMD regularizer compared to error for batchmean=true\n",
    "#.  e.g.  *= 3*IMG_SIZE**2/latent_dim\n",
    "batchmean = True\n",
    "useL1 = False\n",
    "hidden_dim = None\n",
    "\n",
    "metrics = default_MMEVAE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "batch_size = 128\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "arch='resblock'\n",
    "vae = ResBlockAE(get_resblockencoder_parts(arch), hidden_dim=hidden_dim,latent_dim=latent_dim,  im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "  \n",
    "# let beta be calculated by : 3*im_size*im_size/latent_dim\n",
    "loss_func = MMDLoss(batchmean=batchmean,alpha=alpha,useL1=False)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split)#.to_fp16() #wd=config['wd'],opt_func=ranger,\n",
    "     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1,lr2=learn.lr_find()\n",
    "mlr = .5*(lr1+lr2)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr1,lr2]).log().mean().exp().tolist()\n",
    "\n",
    "lr1,lr2,mlr,gmlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm things up...  low-ish learning rate + anealing kldiv retularizaiton into\n",
    "#learn.freeze()\n",
    "n_epoch = 200\n",
    "#learn.fit_flat_cos(n_epoch) #, lr=1e-3, div_final=1e6, pct_start=0.2)\n",
    "learn.fit_flat_cos(n_epoch)#, lr=lr1, div_final=1e5, pct_start=0.5)\n",
    "#learn.fit_one_cycle(n_epoch,lr_max=gmlr) #, lr_max= base_lr)\n",
    "\n",
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"MMDVae-{'TMP'}-latent{latent_dim}\"\n",
    "filename = f\"frozen-{prefix}-{learn.model.name}-alpha{alpha:d}_{datetime.now().strftime('%Y-%m-%d_%H.%M.%S')}\"\n",
    "\n",
    "learn.save(filename)\n",
    "#learn.export(f'{filename}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64 latents, alpha = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "\n",
    "# cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(),         \n",
    "#                SaveModelCallback(fname=datetime.now().strftime('%Y-%m-%d %Hh%M.%S'), every_epoch=True),\n",
    "#                ParamScheduler({'kl_weight': SchedNo(1.,1.) })]\n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(),         \n",
    "               ParamScheduler({'kl_weight': SchedNo(1.,1.) })]\n",
    "\n",
    "alpha = 20\n",
    "# note that alpha needs to be adjusted to scale MMD regularizer compared to error for batchmean=true\n",
    "#.  e.g.  *= 3*IMG_SIZE**2/latent_dim\n",
    "batchmean = True\n",
    "useL1 = False\n",
    "hidden_dim = None\n",
    "\n",
    "metrics = default_MMEVAE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "batch_size = 128\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "arch='resblock'\n",
    "vae = ResBlockAE(get_resblockencoder_parts(arch), hidden_dim=hidden_dim,latent_dim=latent_dim,  im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "  \n",
    "# let beta be calculated by : 3*im_size*im_size/latent_dim\n",
    "loss_func = MMDLoss(batchmean=batchmean,alpha=alpha,useL1=False)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split)#.to_fp16() #wd=config['wd'],opt_func=ranger,\n",
    "     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1,lr2=learn.lr_find()\n",
    "mlr = .5*(lr1+lr2)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr1,lr2]).log().mean().exp().tolist()\n",
    "\n",
    "lr1,lr2,mlr,gmlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm things up...  low-ish learning rate + anealing kldiv retularizaiton into\n",
    "#learn.freeze()\n",
    "n_epoch = 200\n",
    "#learn.fit_flat_cos(n_epoch) #, lr=1e-3, div_final=1e6, pct_start=0.2)\n",
    "learn.fit_flat_cos(n_epoch)#, lr=lr1, div_final=1e5, pct_start=0.5)\n",
    "#learn.fit_one_cycle(n_epoch,lr_max=gmlr) #, lr_max= base_lr)\n",
    "\n",
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"MMDVae-{'TMP'}-latent{latent_dim}\"\n",
    "filename = f\"frozen-{prefix}-{learn.model.name}-alpha{alpha:d}_{datetime.now().strftime('%Y-%m-%d_%H.%M.%S')}\"\n",
    "\n",
    "learn.save(filename)\n",
    "#learn.export(f'{filename}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vanilla-resnet beta-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockBVAE(BVAE):\n",
    "    \"\"\"\n",
    "    simple VAE with a _probably_ pretrained encoder \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,enc_parts,hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):\n",
    "       \n",
    "        \"\"\"\n",
    "        inputs:  \n",
    "            enc_arch (pre-cut / pretrained)\n",
    "            enc_dim\n",
    "            latent_dim\n",
    "            hidden_dim\n",
    "            im_size,out_range\n",
    "        \"\"\"\n",
    "        enc_arch,enc_feats,name = enc_parts\n",
    "\n",
    "        # encoder\n",
    "        #  arch,cut = xresnet18(pretrained=True),-4\n",
    "        #  enc_arch = list(arch.children())[:cut]\n",
    "        \n",
    "        BASE = im_size//2**5\n",
    "        enc_dim = enc_feats * BASE**2  # 2**(3*3) * (im_size//32)**2 #(output of resneet) #12800\n",
    "\n",
    "        self.encoder = build_AE_encoder(enc_arch,enc_dim=enc_dim, hidden_dim=hidden_dim, im_size=im_size)\n",
    "\n",
    "        in_dim = enc_dim if hidden_dim is None else hidden_dim\n",
    "\n",
    "        # VAE Bottleneck\n",
    "        self.bn = VAELayer(in_dim,latent_dim)     \n",
    "\n",
    "        #decoder\n",
    "        self.decoder = build_ResBlockAE_decoder(hidden_dim=hidden_dim, latent_dim=latent_dim, im_size=im_size,out_range=out_range)\n",
    "\n",
    "        store_attr('name,enc_dim, in_dim,hidden_dim,latent_dim,im_size,out_range') # do i need all these?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  THESE ARE INHERITED..\n",
    "#     def decode(self, z):    \n",
    "#         z = self.decoder(z)\n",
    "#         return z\n",
    "    \n",
    "#     def reparam(self, h):\n",
    "#         return self.bn(h)\n",
    "\n",
    "#     def encode(self, x):\n",
    "#         h = self.encoder(x)\n",
    "#         z, mu, logvar = self.reparam(h)\n",
    "#         return z, mu, logvar\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         z, mu, logvar = self.encode(x)\n",
    "#         x_hat = self.decode(z)\n",
    "#         latents = torch.stack([mu,logvar],dim=-1)\n",
    "#         return x_hat, latents # assume dims are [batch,latent_dim,concat_dim]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 128 latents, alpha=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "alpha = 5 # doubled because latent is half?\n",
    "batchmean = True\n",
    "useL1 = False\n",
    "\n",
    "# SaveModelCallback(fname=datetime.now().strftime('%Y-%m-%d %Hh%M.%S'), every_epoch=True), \n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(), ParamScheduler({'kl_weight':  SchedNo(1.,1.) })]\n",
    "\n",
    "\n",
    "metrics = default_VAE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "\n",
    "block = get_ae_DataBlock(aug=True)\n",
    "batch_size = 128\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "arch='resnblock'\n",
    "vae = ResBlockBVAE(get_resblockencoder_parts(arch), hidden_dim=2048,latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "                   \n",
    "# let beta be calculated by : 3*im_size*im_size/latent_dim\n",
    "loss_func = BVAELoss(batchmean=batchmean,alpha=alpha,useL1=False)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split)#.to_fp16() #wd=config['wd'],opt_func=ranger,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: lf_finder does NOT work correctly with our annealed kl_weight... \n",
    "\n",
    "lr1,lr2=learn.lr_find()\n",
    "mlr = .5*(lr1+lr2)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr1,lr2]).log().mean().exp().tolist()\n",
    "\n",
    "lr1,lr2,mlr,gmlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put in the annealied KL_weight...\n",
    "learn.remove_cb(learn.cbs[-1])\n",
    "# add new constant scheduler\n",
    "learn.add_cb(ParamScheduler({'kl_weight': default_KL_anneal_in()} ) )\n",
    "\n",
    "#fit the backend of the VAE (n)\n",
    "# the defaults are pretty good for now\n",
    "n_epochs = 10\n",
    "\n",
    "#learn.fit_one_cycle(freeze_epochs1,lr_max= lr1)#, lr_max= base_lr)\n",
    "#learn.fit_flat_cos(n_epochs, lr=lr1, pct_start=0.5)\n",
    "#learn.fit_flat_cos(n_epochs) #, lr=1e-4,pct_start=0.5)\n",
    "learn.fit_one_cycle(n_epochs)#, lr_max= base_lr)\n",
    "\n",
    "learn.show_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initial \"burning in\" of the KLD regularization is very unstable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace kl_weight scheduler... with a constant unity... (NOTE: should be robust to failure if we don't replace it)\n",
    "learn.remove_cb(learn.cbs[-1])\n",
    "# add new constant scheduler\n",
    "learn.add_cb(ParamScheduler({'kl_weight':  SchedNo(1.,1.) }) )\n",
    "\n",
    "#learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr1,lr2=learn.lr_find()\n",
    "\n",
    "mlr = .5*(lr1+lr2)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr1,lr2]).log().mean().exp().tolist()\n",
    "\n",
    "lr1,lr2,mlr,gmlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs = 5\n",
    "base_lr = 1e-5# gmlr #/= 2\n",
    "epochs = 100\n",
    "\n",
    "#learn.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div)\n",
    "#learn.fit_one_cycle(epochs, lr_max= 1e-3)\n",
    "#learn.fit_flat_cos(epochs,lr=lr1,pct_start=.05)\n",
    "learn.fit_flat_cos(epochs,div_final=1000)#,lr=1e-4)\n",
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"BVae-{'2step10_100'}-latent{latent_dim}\"\n",
    "filename = f\"{prefix}-{learn.model.name}-alpha{alpha:d}_{datetime.now().strftime('%Y-%m-%d_%H.%M.%S')}\"\n",
    "\n",
    "learn.save(filename)\n",
    "learn.export(f'{filename}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 128 latents, alpha=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "alpha = 10 # doubled because latent is half?\n",
    "batchmean = True\n",
    "useL1 = False\n",
    "\n",
    "\n",
    "\n",
    "# SaveModelCallback(fname=datetime.now().strftime('%Y-%m-%d %Hh%M.%S'), every_epoch=True), \n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(), ParamScheduler({'kl_weight':  SchedNo(1.,1.) })]\n",
    "\n",
    "\n",
    "metrics = default_VAE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "\n",
    "block = get_ae_DataBlock(aug=True)\n",
    "batch_size = 128\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "arch='resnblock'\n",
    "vae = ResBlockBVAE(get_resblockencoder_parts(arch), hidden_dim=2048,latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "                   \n",
    "# let beta be calculated by : 3*im_size*im_size/latent_dim\n",
    "loss_func = BVAELoss(batchmean=batchmean,alpha=alpha,useL1=False)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split)#.to_fp16() #wd=config['wd'],opt_func=ranger,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: lf_finder does NOT work correctly with our annealed kl_weight... \n",
    "\n",
    "lr1,lr2=learn.lr_find()\n",
    "mlr = .5*(lr1+lr2)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr1,lr2]).log().mean().exp().tolist()\n",
    "\n",
    "lr1,lr2,mlr,gmlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put in the annealied KL_weight...\n",
    "learn.remove_cb(learn.cbs[-1])\n",
    "# add new constant scheduler\n",
    "learn.add_cb(ParamScheduler({'kl_weight': default_KL_anneal_in()} ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fit the backend of the VAE (n)\n",
    "# # the defaults are pretty good for now\n",
    "n_epochs = 10\n",
    "\n",
    "learn.fit_one_cycle(n_epochs)#, lr_max= base_lr)\n",
    "# #learn.fit_flat_cos(n_epochs, lr=lr1, pct_start=0.5)\n",
    "# learn.fit_flat_cos(n_epochs, lr=1e-4,pct_start=0.5)\n",
    "\n",
    "# learn.show_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fit the backend of the VAE (n)\n",
    "# # the defaults are pretty good for now\n",
    "# n_epochs = 10\n",
    "\n",
    "# learn.fit_one_cycle(10)#, lr_max= base_lr)\n",
    "# #learn.fit_flat_cos(n_epochs, lr=lr1, pct_start=0.5)\n",
    "# #learn.fit_flat_cos(n_epochs, lr=1e-4,pct_start=0.5)\n",
    "\n",
    "# learn.show_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initial \"burning in\" of the KLD regularization is very unstable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace kl_weight scheduler... with a constant unity... (NOTE: should be robust to failure if we don't replace it)\n",
    "learn.remove_cb(learn.cbs[-1])\n",
    "# add new constant scheduler\n",
    "learn.add_cb(ParamScheduler({'kl_weight':  SchedNo(1.,1.) }) )\n",
    "\n",
    "#learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr1,lr2=learn.lr_find()\n",
    "\n",
    "mlr = .5*(lr1+lr2)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr1,lr2]).log().mean().exp().tolist()\n",
    "\n",
    "lr1,lr2,mlr,gmlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs = 5\n",
    "epochs = 100\n",
    "\n",
    "#learn.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div)\n",
    "#learn.fit_one_cycle(epochs, lr_max= 1e-3)\n",
    "#learn.fit_flat_cos(epochs,lr=lr1,pct_start=.05)\n",
    "learn.fit_flat_cos(epochs,div_final = 1000)#,lr=1e-4)\n",
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix = f\"BVae-{'POST-1CYCLE10'}-latent{latent_dim}\"\n",
    "# filename = f\"{prefix}-{learn.model.name}-alpha{alpha:d}_{datetime.now().strftime('%Y-%m-%d_%H.%M.%S')}\"\n",
    "\n",
    "# learn.save(filename)\n",
    "# learn.export(f'{filename}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #epochs = 5\n",
    "# base_lr = 1e-5# gmlr #/= 2\n",
    "# epochs = 50\n",
    "\n",
    "# #learn.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div)\n",
    "# #learn.fit_one_cycle(epochs, lr_max= 1e-3)\n",
    "# #learn.fit_flat_cos(epochs,lr=lr1,pct_start=.05)\n",
    "# learn.fit_flat_cos(epochs)\n",
    "# learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix = f\"BVae-{'POST-1CYCLE10-COS50'}-latent{latent_dim}\"\n",
    "# filename = f\"{prefix}-{learn.model.name}-alpha{alpha:d}_{datetime.now().strftime('%Y-%m-%d_%H.%M.%S')}\"\n",
    "\n",
    "# learn.save(filename)\n",
    "# learn.export(f'{filename}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix = f\"BVae-{'POST-1CYCLE10'}-latent{latent_dim}\"\n",
    "# filename = f\"{prefix}-{learn.model.name}-alpha{alpha:d}_{datetime.now().strftime('%Y-%m-%d_%H.%M.%S')}\"\n",
    "# filename = \"BVae-POST-1CYCLE10-latent128-resblock-alpha10_2021-03-24_21.33.02\"\n",
    "# learn.load(filename)\n",
    "# #epochs = 5\n",
    "# epochs = 10\n",
    "# learn.fit_one_cycle(epochs, lr_max=.001)\n",
    "# #learn.fit_flat_cos(epochs,lr=.0015,pct_start=.5,div_final=1000.0)\n",
    "# #learn.fit_one_cycle(epochs,lr_max=5e-3,pct_start=0.5,div_final=100000) # gets down to ~4500 loss in 10\n",
    "# learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"BVae-{'2step10_100'}-latent{latent_dim}\"\n",
    "filename = f\"{prefix}-{learn.model.name}-alpha{alpha:d}_{datetime.now().strftime('%Y-%m-%d_%H.%M.%S')}\"\n",
    "\n",
    "learn.save(filename)\n",
    "learn.export(f'{filename}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64 latents, alpha=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "alpha = 5 # doubled because latent is half?\n",
    "batchmean = True\n",
    "useL1 = False\n",
    "\n",
    "\n",
    "# SaveModelCallback(fname=datetime.now().strftime('%Y-%m-%d %Hh%M.%S'), every_epoch=True), \n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(), ParamScheduler({'kl_weight':  SchedNo(1.,1.) })]\n",
    "\n",
    "\n",
    "metrics = default_VAE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "\n",
    "block = get_ae_DataBlock(aug=True)\n",
    "batch_size = 128\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "arch='resnblock'\n",
    "vae = ResBlockBVAE(get_resblockencoder_parts(arch), hidden_dim=2048,latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "                   \n",
    "# let beta be calculated by : 3*im_size*im_size/latent_dim\n",
    "loss_func = BVAELoss(batchmean=batchmean,alpha=alpha,useL1=False)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split)#.to_fp16() #wd=config['wd'],opt_func=ranger,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: lf_finder does NOT work correctly with our annealed kl_weight... \n",
    "\n",
    "lr1,lr2=learn.lr_find()\n",
    "mlr = .5*(lr1+lr2)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr1,lr2]).log().mean().exp().tolist()\n",
    "\n",
    "lr1,lr2,mlr,gmlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put in the annealied KL_weight...\n",
    "learn.remove_cb(learn.cbs[-1])\n",
    "# add new constant scheduler\n",
    "learn.add_cb(ParamScheduler({'kl_weight': default_KL_anneal_in()} ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the defaults are pretty good for now\n",
    "n_epochs = 10\n",
    "\n",
    "learn.fit_one_cycle(n_epochs)#,lr_max= lr1)#, lr_max= base_lr)\n",
    "#learn.fit_flat_cos(n_epochs, lr=lr1, pct_start=0.5)\n",
    "#learn.fit_flat_cos(n_epochs, lr=1e-4,pct_start=0.5)\n",
    "\n",
    "learn.show_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initial \"burning in\" of the KLD regularization is very unstable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace kl_weight scheduler... with a constant unity... (NOTE: should be robust to failure if we don't replace it)\n",
    "learn.remove_cb(learn.cbs[-1])\n",
    "# add new constant scheduler\n",
    "learn.add_cb(ParamScheduler({'kl_weight':  SchedNo(1.,1.) }) )\n",
    "\n",
    "#learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr1,lr2=learn.lr_find()\n",
    "\n",
    "mlr = .5*(lr1+lr2)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr1,lr2]).log().mean().exp().tolist()\n",
    "\n",
    "lr1,lr2,mlr,gmlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs = 5\n",
    "base_lr = 1e-5# gmlr #/= 2\n",
    "epochs = 100\n",
    "\n",
    "#learn.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div)\n",
    "#learn.fit_one_cycle(epochs, lr_max= 1e-3)\n",
    "#learn.fit_flat_cos(epochs,lr=lr1,pct_start=.05)\n",
    "#learn.fit_flat_cos(epochs,lr=1e-4)\n",
    "learn.fit_flat_cos(epochs, div_final=1000.0)#,lr=1e-3)\n",
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"BVae-{'2step10_100'}-latent{latent_dim}\"\n",
    "filename = f\"{prefix}-{learn.model.name}-alpha{alpha:d}_{datetime.now().strftime('%Y-%m-%d_%H.%M.%S')}\"\n",
    "\n",
    "learn.save(filename)\n",
    "learn.export(f'{filename}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64 latents, alpha=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "alpha = 10 # doubled because latent is half?\n",
    "batchmean = True\n",
    "useL1 = False\n",
    "\n",
    "\n",
    "# SaveModelCallback(fname=datetime.now().strftime('%Y-%m-%d %Hh%M.%S'), every_epoch=True), \n",
    "cbs = [AnnealedLossCallback(),TerminateOnNaNCallback(), ParamScheduler({'kl_weight':  SchedNo(1.,1.) })]\n",
    "\n",
    "\n",
    "metrics = default_VAE_metrics(alpha,batchmean,useL1)\n",
    "\n",
    "\n",
    "block = get_ae_DataBlock(aug=True)\n",
    "batch_size = 128\n",
    "dls = block.dataloaders(df, batch_size=batch_size)\n",
    "\n",
    "arch='resnblock'\n",
    "vae = ResBlockBVAE(get_resblockencoder_parts(arch), hidden_dim=2048,latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE)\n",
    "                   \n",
    "# let beta be calculated by : 3*im_size*im_size/latent_dim\n",
    "loss_func = BVAELoss(batchmean=batchmean,alpha=alpha,useL1=False)\n",
    "\n",
    "learn = Learner(dls, vae, cbs=cbs,loss_func=loss_func, metrics=metrics,splitter=AE_split)#.to_fp16() #wd=config['wd'],opt_func=ranger,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: lf_finder does NOT work correctly with our annealed kl_weight... \n",
    "\n",
    "lr1,lr2=learn.lr_find()\n",
    "mlr = .5*(lr1+lr2)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr1,lr2]).log().mean().exp().tolist()\n",
    "\n",
    "lr1,lr2,mlr,gmlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put in the annealied KL_weight...\n",
    "learn.remove_cb(learn.cbs[-1])\n",
    "# add new constant scheduler\n",
    "learn.add_cb(ParamScheduler({'kl_weight': default_KL_anneal_in()} ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the backend of the VAE (n)\n",
    "# the defaults are pretty good for now\n",
    "n_epochs = 10\n",
    "\n",
    "learn.fit_one_cycle(n_epochs)#,lr_max= lr1)#, lr_max= base_lr)\n",
    "#learn.fit_flat_cos(n_epochs, lr=lr1, pct_start=0.5)\n",
    "#learn.fit_flat_cos(n_epochs, lr=1e-4,pct_start=0.5)\n",
    "\n",
    "learn.show_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initial \"burning in\" of the KLD regularization is very unstable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace kl_weight scheduler... with a constant unity... (NOTE: should be robust to failure if we don't replace it)\n",
    "learn.remove_cb(learn.cbs[-1])\n",
    "# add new constant scheduler\n",
    "learn.add_cb(ParamScheduler({'kl_weight':  SchedNo(1.,1.) }) )\n",
    "\n",
    "#learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr1,lr2=learn.lr_find()\n",
    "\n",
    "mlr = .5*(lr1+lr2)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr1,lr2]).log().mean().exp().tolist()\n",
    "\n",
    "lr1,lr2,mlr,gmlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs = 5\n",
    "base_lr = 1e-5# gmlr #/= 2\n",
    "epochs = 100\n",
    "\n",
    "#learn.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div)\n",
    "#learn.fit_one_cycle(epochs, lr_max= 1e-3)\n",
    "#learn.fit_flat_cos(epochs,lr=lr1,pct_start=.05)\n",
    "learn.fit_flat_cos(epochs,div_final=1000.)\n",
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"BVae-{'2step10_100'}-latent{latent_dim}\"\n",
    "filename = f\"{prefix}-{learn.model.name}-alpha{alpha:d}_{datetime.now().strftime('%Y-%m-%d_%H.%M.%S')}\"\n",
    "\n",
    "learn.save(filename)\n",
    "learn.export(f'{filename}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE graveyard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not using...\n",
    "class LatentTuple(fastuple):\n",
    "    \"Basic type for tuple of tensor (vectors)\"\n",
    "    _show_args = dict(s=10, marker='.', c='r')\n",
    "    @classmethod\n",
    "    def create(cls, ts): \n",
    "        if isinstance(ts,tuple):\n",
    "            mu,logvar = ts\n",
    "        elif ts is None:\n",
    "            mu,logvar = None,None\n",
    "        else:\n",
    "            mu = None\n",
    "            logvar = None\n",
    "            \n",
    "        if mu is None: mu = torch.empty(0)\n",
    "        elif not isinstance(mu, Tensor): Tensor(mu) \n",
    "        \n",
    "        if logvar is None: logvar = torch.empty(0)\n",
    "        elif not isinstance(logvar,Tensor): Tensor(logvar)\n",
    "            \n",
    "        return cls( (mu,logvar) ) \n",
    "        \n",
    "    def show(self, ctx=None, **kwargs): \n",
    "        mu,logvar = self\n",
    "        if not isinstance(mu, Tensor) or not isinstance(logvar,Tensor): return ctx\n",
    "\n",
    "        title_str = f\"mu-> {mu.mean():e}, {mu.std():e}  logvar->{logvar.mean():e}, {logvar.std():e}\"\n",
    "    \n",
    "        if 'figsize' in kwargs: del kwargs['figsize']\n",
    "        if 'title' in kwargs: kwargs['title']=title_str\n",
    "        if ctx is None:\n",
    "            _,axs = plt.subplots(1,2, figsize=(12,6))\n",
    "            x=torch.linspace(0,1,mu[0].shape[0])\n",
    "            axs[0].scatter(x, mu[:], **{**self._show_args, **kwargs})\n",
    "            axs[1].scatter(x, logvar[:], **{**self._show_args, **kwargs})\n",
    "            ctx = axs[1]\n",
    "        \n",
    "        ctx.scatter(mu[:], logvar[:], **{**self._show_args, **kwargs})\n",
    "        return ctx\n",
    "    \n",
    "# could we do a typedispatch to manage the transforms...?\n",
    "def VAETargetTupleBlock(): \n",
    "    return TransformBlock(type_tfms=VAETargetTuple.create, batch_tfms=IntToFloatTensor)\n",
    "\n",
    "def LatentTupleBlock(): \n",
    "    return TransformBlock(type_tfms=LatentTuple.create, batch_tfms=noop)\n",
    "    \n",
    "\n",
    "# class TensorPoint(TensorBase):\n",
    "#     \"Basic type for points in an image\"\n",
    "#     _show_args = dict(s=10, marker='.', c='r')\n",
    "\n",
    "#     @classmethod\n",
    "#     def create(cls, t, img_size=None)->None:\n",
    "#         \"Convert an array or a list of points `t` to a `Tensor`\"\n",
    "#         return cls(tensor(t).view(-1, 2).float(), img_size=img_size)\n",
    "\n",
    "#     def show(self, ctx=None, **kwargs):\n",
    "#         if 'figsize' in kwargs: del kwargs['figsize']\n",
    "#         x = self.view(-1,2)\n",
    "#         ctx.scatter(x[:, 0], x[:, 1], **{**self._show_args, **kwargs})\n",
    "#         return ctx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "dropout = .2\n",
    "im_size = IMG_SIZE\n",
    "n_blocks = 5        \n",
    "nfs = [3] + [2**i*n_blocks for i in range(n_blocks+1)] \n",
    "nfs.reverse()\n",
    "# decoder = nn.Sequential(\n",
    "#             nn.Linear(latent_size, 16),\n",
    "#             UnFlatten(4),\n",
    "#             ResBlock(1, 3, 4, act_cls=Mish),\n",
    "#             nn.Dropout2d(dropout),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             ResBlock(1, 4, 8, act_cls=Mish),\n",
    "#             nn.Dropout2d(dropout),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             ResBlock(1, 8, 16, act_cls=Mish),\n",
    "#             nn.Dropout2d(dropout),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             nn.Conv2d(16, 1, 3),\n",
    "#             nn.Dropout2d(dropout),\n",
    "#             #nn.AdaptiveAvgPool2d((3,im_size, im_size)) \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_blocks = 5\n",
    "hidden_dim = 2048\n",
    "out_range = [-1,1]\n",
    "tst = nn.Sequential(\n",
    "            nn.Linear(latent_dim,hidden_dim), #nn.Linear(latent_dim, 16)\n",
    "            nn.Linear(hidden_dim,im_size*n_blocks*n_blocks), #nn.Linear(latent_dim, 16)\n",
    "            ResizeBatch(im_size,n_blocks,n_blocks),#UnFlatten(n_blocks), #4\n",
    "            ResBlock(1, nfs[0], nfs[1], act_cls=Mish), #ResBlock(1, 1, 4, act_cls=Mish),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ResBlock(1, nfs[1], nfs[2], act_cls=Mish), #RResBlock(1, 4, 8, act_cls=Mish),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ResBlock(1, nfs[2], nfs[3], act_cls=Mish), #ResBlock(1, 8, 16, act_cls=Mish),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ResBlock(1, nfs[3], nfs[4], act_cls=Mish), #nn.Conv2d(16, 1, 3),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ResBlock(1, nfs[4], nfs[5], act_cls=Mish), #nn.Conv2d(16, 1, 3),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ResBlock(1, nfs[5], nfs[6], act_cls=Mish), #nn.Conv2d(16, 1, 3),\n",
    "            #nn.Dropout2d(dropout),\n",
    "            #nn.Upsample(scale_factor=2),            #\n",
    "            #nn.AdaptiveAvgPool2d((3,im_size, im_size)),\n",
    "            SigmoidRange(*out_range), #nn.Sigmoid()\n",
    ")\n",
    "tst,nfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=    torch.randn((32,latent_dim))\n",
    "#last_size = model_sizes(tst ) #[-1][1]\n",
    "#num_features_model(tst)\n",
    "#last_size\n",
    "#nfs\n",
    "tst(inp).shape,nfs\n",
    "#last_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "enc = nn.Sequential(\n",
    "    ResBlock(1, 1, 16, act_cls=nn.ReLU, norm_type=None),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    ResBlock(1, 16, 4, act_cls=nn.ReLU, norm_type=None),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    Flatten()\n",
    ")\n",
    "# torch.Size([32, 1, 28, 28])\n",
    "# torch.Size([32, 16, 28, 28])\n",
    "# torch.Size([32, 16, 14, 14])\n",
    "# torch.Size([32, 4, 14, 14])\n",
    "# torch.Size([32, 4, 7, 7])\n",
    "# torch.Size([32, 196])\n",
    "\n",
    "latent_size = 100\n",
    "\n",
    "enc = nn.Sequential(\n",
    "            ResBlock(1, 3, 5, stride=2, act_cls=Mish),#  1->3\n",
    "            ResBlock(1, 5, 5, stride=2, act_cls=Mish),\n",
    "            ResBlock(1, 5, 1, stride=2, act_cls=Mish),\n",
    "            Flatten(),\n",
    "            nn.Linear(400, latent_size) # 16->400\n",
    "        )\n",
    "#  torch.Size([32, 1, 28, 28])\n",
    "# torch.Size([32, 5, 14, 14])\n",
    "# torch.Size([32, 5, 7, 7])\n",
    "# torch.Size([32, 1, 4, 4])\n",
    "# torch.Size([32, 16])\n",
    "# torch.Size([32, 4])       \n",
    "inp=    torch.randn((32,3,160,160))\n",
    "for ii in range(0,8):\n",
    "    print(enc[:ii](inp).shape)\n",
    "\n",
    "z = enc(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=0\n",
    "dec = nn.Sequential(\n",
    "            nn.Linear(latent_size, 16),\n",
    "            UnFlatten(4),\n",
    "            ResBlock(1, 1, 4, act_cls=Mish), #4->5\n",
    "            #nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ResBlock(1, 4, 8, act_cls=Mish),\n",
    "            #nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ResBlock(1, 8, 16, act_cls=Mish),\n",
    "            #nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(16, 3, 3), #1->3\n",
    "            #nn.Dropout2d(dropout),\n",
    "            nn.AdaptiveAvgPool2d((28, 28)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "# torch.Size([32, 4])\n",
    "# torch.Size([32, 16])\n",
    "# torch.Size([32, 1, 4, 4])\n",
    "# torch.Size([32, 4, 4, 4])\n",
    "# torch.Size([32, 4, 8, 8])\n",
    "# torch.Size([32, 8, 8, 8])\n",
    "# torch.Size([32, 8, 16, 16])\n",
    "# torch.Size([32, 16, 16, 16])\n",
    "# torch.Size([32, 16, 32, 32])\n",
    "# torch.Size([32, 1, 30, 30])\n",
    "# torch.Size([32, 1, 28, 28])\n",
    "\n",
    "\n",
    "for ii in range(0,12):\n",
    "    print(dec[:ii](z).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_blocks = 5\n",
    "BASE = im_size//2**5        \n",
    "nfs = [3]+[(2**i)*BASE for i in range(n_blocks)] \n",
    "n = len(nfs)\n",
    "hidden_dim = 2048\n",
    "\n",
    "BASE = im_size//2**5\n",
    "        # encoder\n",
    "in_dim = nfs[-1] * BASE**2 \n",
    "        \n",
    "modules =  [ResBlock(1, nfs[i],nfs[i+1], \n",
    "                          stride=2, act_cls=Mish)  for i in range(n - 1)]    \n",
    "# enc =  nn.Sequential(\n",
    "#                 ConvLayer(nfs[0],nfs[1],ks=5,stride=2,padding=2),\n",
    "#                 ConvLayer(nfs[1],nfs[2],ks=5,stride=2,padding=2),\n",
    "#                 ConvLayer(nfs[2],nfs[3],ks=5,stride=2,padding=2),\n",
    "#                 ConvLayer(nfs[3],nfs[4],ks=5,stride=2,padding=2),\n",
    "#                 ConvLayer(nfs[4],nfs[5],ks=5,stride=2,padding=2),\n",
    "#                 Flatten(),\n",
    "#                 LinBnDrop(in_dim,hidden_dim,bn=True,p=0.0,act=nn.ReLU(),lin_first=True)\n",
    "#             )\n",
    "\n",
    "enc =  nn.Sequential(*modules,\n",
    "                Flatten(),\n",
    "                LinBnDrop(in_dim,hidden_dim,bn=True,p=0.0,act=nn.ReLU(),lin_first=True)\n",
    "            )\n",
    "\n",
    "nfs.reverse()\n",
    "\n",
    "print(nfs)\n",
    "#last_size = model_sizes(enc, size=(28,28))[-1][1]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "encoder = nn.Sequential(enc, nn.Linear(hidden_dim, z_dim))\n",
    "        \n",
    "decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, im_size*n_blocks*n_blocks),\n",
    "            ResizeBatch(im_size,n_blocks,n_blocks),#UnFlatten(n_blocks), #4\n",
    "            ResBlock(1, nfs[0], nfs[1], ks=1, act_cls=nn.ReLU, norm_type=None),\n",
    "            #nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ResBlock(1, nfs[1], nfs[2], act_cls=nn.ReLU, norm_type=None),\n",
    "            #nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(nfs[2], 3, 3, padding=1),\n",
    "            #nn.Dropout2d(dropout),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "#last_size\n",
    "\n",
    "#             nn.Linear(latent_dim,hidden_dim), #nn.Linear(latent_dim, 16)\n",
    "#             nn.Linear(hidden_dim,im_size*n_blocks*n_blocks), #nn.Linear(latent_dim, 16)\n",
    "#             ResizeBatch(im_size,n_blocks,n_blocks),#UnFlatten(n_blocks), #4\n",
    "#             ResBlock(1, nfs[0], nfs[1], act_cls=Mish), #ResBlock(1, 1, 4, act_cls=Mish),\n",
    "#             nn.Dropout2d(dropout),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             ResBlock(1, nfs[1], nfs[2], act_cls=Mish), #RResBlock(1, 4, 8, act_cls=Mish),\n",
    "#             nn.Dropout2d(dropout),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             ResBlock(1, nfs[2], nfs[3], act_cls=Mish), #ResBlock(1, 8, 16, act_cls=Mish),\n",
    "#             nn.Dropout2d(dropout),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             ResBlock(1, nfs[3], nfs[4], act_cls=Mish), #nn.Conv2d(16, 1, 3),\n",
    "#             nn.Dropout2d(dropout),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             ResBlock(1, nfs[4], nfs[5], act_cls=Mish), #nn.Conv2d(16, 1, 3),\n",
    "#             nn.Dropout2d(dropout),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             ResBlock(1, nfs[5], nfs[6], act_cls=Mish), #nn.Conv2d(16, 1, 3),\n",
    "\n",
    "            \n",
    "\n",
    "inp=    torch.randn((32,3,160,160))\n",
    "\n",
    "#encoder[:1](inp).shape\n",
    "for ii in range(0,10):\n",
    "    print(enc[:ii](inp).shape)\n",
    "\n",
    "z = encoder(inp)\n",
    "for ii in range(0,14):\n",
    "    print(decoder[:ii](z).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(Module):\n",
    "    def __init__(self, size=7):\n",
    "        self.size = size\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1, self.size, self.size)\n",
    "\n",
    "class MMD_VAE(Module):\n",
    "    def __init__(self, latent_size):        \n",
    "        self.encoder = nn.Sequential(\n",
    "            ResBlock(1, 1, 5, stride=2, act_cls=Mish),\n",
    "            ResBlock(1, 5, 5, stride=2, act_cls=Mish),\n",
    "            ResBlock(1, 5, 1, stride=2, act_cls=Mish),\n",
    "            Flatten(),\n",
    "            nn.Linear(16, latent_size)\n",
    "        )\n",
    "        \n",
    "        dropout=0\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 16),\n",
    "            UnFlatten(4),\n",
    "            ResBlock(1, 1, 4, act_cls=Mish),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ResBlock(1, 4, 8, act_cls=Mish),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ResBlock(1, 8, 16, act_cls=Mish),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(16, 1, 3),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.AdaptiveAvgPool2d((28, 28)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        latent = self.encoder(X)\n",
    "        return self.decoder(latent), latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#decoder\n",
    "n_blocks = 5        \n",
    "nfs = [3] + [2**i*n_blocks for i in range(n_blocks+1)] \n",
    "nfs.reverse()\n",
    "n = len(nfs)\n",
    "\n",
    "\n",
    "tst = nn.Sequential(\n",
    "            nn.Linear(latent_dim,hidden_dim, #nn.Linear(latent_dim, 16)\n",
    "            nn.Linear(hidden_dim,im_size*n_blocks*n_blocks) #nn.Linear(latent_dim, 16)\n",
    "            UnFlatten(n_blocks), #4\n",
    "            ResBlock(1, nfs[0], nfs[1], act_cls=Mish), #ResBlock(1, 1, 4, act_cls=Mish),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ResBlock(1, nfs[1], nfs[2], act_cls=Mish), #RResBlock(1, 4, 8, act_cls=Mish),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ResBlock(1, nfs[2], nfs[3], act_cls=Mish), #ResBlock(1, 8, 16, act_cls=Mish),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            ResBlock(1, nfs[3], nfs[4], act_cls=Mish), #nn.Conv2d(16, 1, 3),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.AdaptiveAvgPool2d((3,im_size, im_size)),\n",
    "            SigmoidRange(*out_range)#nn.Sigmoid()\n",
    "\n",
    "    \n",
    "                                         *modules,\n",
    "                                      ConvLayer(nfs[-2],nfs[-1],\n",
    "                                                ks=1,padding=0, norm_type=None, #act_cls=nn.Sigmoid) )\n",
    "                                                act_cls=partial(SigmoidRange, *out_range)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "\n",
    "\n",
    "class AE_vanillaResNet(Module):\n",
    "    def __init__(self,latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE,pretrained=True):\n",
    "        #  drop_p=0.0 default turns off dropout\n",
    "        \n",
    "        \n",
    "        self.im_size=im_size\n",
    "        # encoder\n",
    "\n",
    "          \n",
    "        n_blocks = 5\n",
    "        BASE = IMG_SIZE//2**5\n",
    "        \n",
    "        \n",
    "        nfs = [3]+[(2**i)*BASE for i in range(n_blocks)] \n",
    "        n = len(nfs)\n",
    "\n",
    "        modules =  [ResBlock(1,nfs[i],nfs[i+1],\n",
    "                                ks=5,stride=2,padding=2) for i in range(n - 1)]       \n",
    "        #self.in_dim = nfs[-1]*BASE*BASE# Sampling vector\n",
    "        self.in_dim = nfs[-1]*(BASE)**2# Sampling vector\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = 2048\n",
    "        self.encoder = nn.Sequential(*modules,\n",
    "                                 Flatten(),\n",
    "                                 LinBnDrop(self.in_dim,self.hidden_dim,bn=True,p=0.0,act=nn.ReLU(),lin_first=True)\n",
    "                                )\n",
    "        \n",
    "\n",
    "        self.bn = nn.Linear(self.hidden_dim,self.latent_dim)\n",
    "        #self.bn = AEBottleneck(self.hidden_dim,self.latent_dim)     \n",
    "\n",
    "        \n",
    "        nfs = [3] + [2**i*n_blocks for i in range(n_blocks+1)] \n",
    "        nfs.reverse()\n",
    "        n = len(nfs)\n",
    "        \n",
    "        modules =  [UpsampleResBlock(nfs[i]) for i in range(n - 2)]        \n",
    "        self.decoder = nn.Sequential( LinBnDrop(latent_dim,im_size*n_blocks*n_blocks,\n",
    "                                                bn=True,# batch normalizaiton shouldn't be a problem here\n",
    "                                                p=0.0,act=nn.ReLU(),lin_first=True),\n",
    "                                      ResizeBatch(im_size,n_blocks,n_blocks),\n",
    "                                      *modules,\n",
    "                                      ConvLayer(nfs[-2],nfs[-1],\n",
    "                                                ks=1,padding=0, norm_type=None, #act_cls=nn.Sigmoid) )\n",
    "                                                act_cls=partial(SigmoidRange, *out_range)))\n",
    "        \n",
    "\n",
    "\n",
    "    def decode(self, z):    \n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.bn(h)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        pass the \"latents\" out to keep the learn mechanics consistent... \n",
    "        \"\"\"\n",
    "        h = self.encoder(x)\n",
    "        z = self.bn(h)\n",
    "        x_reconst = self.decoder(z)                    \n",
    "        dummy_std = torch.ones_like(h) * h.std(dim=1)\n",
    "        latents = torch.stack([h,torch.exp(0.5 * dummy_std)] ,dim=-1)\n",
    "\n",
    "\n",
    "        return x_reconst , latents\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1,lr2=learn.lr_find()\n",
    "mlr = .5*(lr1+lr2)\n",
    "#geometric mean\n",
    "gmlr = torch.tensor([lr1,lr2]).log().mean().exp().tolist()\n",
    "\n",
    "lr1,lr2,mlr,gmlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm things up...  low-ish learning rate + anealing kldiv retularizaiton into\n",
    "n_epoch = 10\n",
    "#learn.fit_flat_cos(n_epoch) #, lr=1e-3, div_final=1e6, pct_start=0.2)\n",
    "learn.fit_flat_cos(n_epoch, lr=lr1, div_final=1e5, pct_start=0.5)\n",
    "#learn.fit_one_cycle(n_epoch) #, lr_max= base_lr)\n",
    "\n",
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm things up...  low-ish learning rate + anealing kldiv retularizaiton into\n",
    "n_epoch = 40\n",
    "#learn.unfreeze()\n",
    "#learn.fit_flat_cos(n_epoch, lr=1e-3, div_final=1e6, pct_start=0.2)\n",
    "learn.fit_flat_cos(n_epoch, lr=lr1, div_final=1e6, pct_start=0.05)\n",
    "\n",
    "#learn.fit_flat_cos(n_epoch, lr=1e-3, div_final=1e5, pct_start=0.5)\n",
    "#learn.fit_one_cycle(n_epoch) #, lr_max= base_lr)\n",
    "\n",
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"MMDVae-{'bmean' if batchmean else 'mean'}{'l1' if useL1 else 'l2'}\"\n",
    "filename = f\"frozen{prefix}-{learn.model.name}-alpha{alpha:d}_{datetime.now().strftime('%Y-%m-%d_%H.%M.%S')}\"\n",
    "\n",
    "learn.save(filename)\n",
    "learn.export(f'{filename}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##. resblock VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "    \n",
    "# check enc_dim... \n",
    "class CVAE_vanilla(Module):\n",
    "    def __init__(self,latent_dim=128, hidden_dim=2048, im_size=IMG_SIZE,out_range=OUT_RANGE,pretrained=True):\n",
    "        #  drop_p=0.0 default turns off dropout, removed enc_dim=512\n",
    "        \n",
    "        self.im_size=im_size\n",
    "        # encoder\n",
    "\n",
    "          \n",
    "        n_blocks = 5\n",
    "        BASE = im_size//2**5\n",
    "        \n",
    "        \n",
    "        nfs = [3]+[(2**i)*BASE for i in range(n_blocks)] \n",
    "        n = len(nfs)\n",
    "\n",
    "        modules =  [ConvLayer(nfs[i],nfs[i+1],\n",
    "                                ks=5,stride=2,padding=2) for i in range(n - 1)]       \n",
    "        #self.in_dim = nfs[-1]*BASE*BASE# Sampling vector\n",
    "        self.in_dim = nfs[-1]*(BASE)**2# Sampling vector\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder = nn.Sequential(*modules,\n",
    "                                 Flatten(),\n",
    "                                 LinBnDrop(self.in_dim,self.hidden_dim,bn=True,p=0.0,act=nn.ReLU(),lin_first=True)\n",
    "                                )\n",
    "        \n",
    "\n",
    "        self.bn = VAELayer(self.hidden_dim,self.latent_dim)     \n",
    "\n",
    "        \n",
    "        nfs = [3] + [2**i*n_blocks for i in range(n_blocks+1)] \n",
    "        nfs.reverse()\n",
    "        n = len(nfs)\n",
    "        \n",
    "        modules =  [UpsampleBlock(nfs[i]) for i in range(n - 2)]        \n",
    "        self.decoder = nn.Sequential( LinBnDrop(latent_dim,self.hidden_dim,\n",
    "                                                bn=True,# batch normalizaiton shouldn't be a problem here\n",
    "                                                p=0.0,act=nn.ReLU(),lin_first=True),\n",
    "                                     LinBnDrop(hidden_dim,im_size*n_blocks*n_blocks,\n",
    "                                                bn=True,# batch normalizaiton shouldn't be a problem here\n",
    "                                                p=0.0,act=nn.ReLU(),lin_first=True),\n",
    "                                      ResizeBatch(im_size,n_blocks,n_blocks),\n",
    "                                      *modules,\n",
    "                                      ConvLayer(nfs[-2],nfs[-1],\n",
    "                                                ks=1,padding=0, norm_type=None, #act_cls=nn.Sigmoid) )\n",
    "                                                act_cls=partial(SigmoidRange, *out_range)))\n",
    "        \n",
    "\n",
    "\n",
    "    def decode(self, z):    \n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "    \n",
    "    def reparam(self, h):\n",
    "        return self.bn(h)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.reparam(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        latents = torch.stack([mu,logvar],dim=-1)\n",
    "        return x_hat, latents # assume dims are [batch,latent_dim,concat_dim]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "# this is the pre-  refactored working version\n",
    "class VAE_rnet(Module):\n",
    "    def __init__(self,enc_dim=512, hidden_dim=2048,latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):\n",
    "        #  drop_p=0.0 default turns off dropout\n",
    "        \n",
    "        self.im_size=im_size\n",
    "        BASE = im_size//2**5\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # encoder\n",
    "        arch,cut = resnet18(pretrained=True),-4\n",
    "        self.in_dim = enc_dim * BASE**2  # 2**(3*3) * (im_size//32)**2 #(output of resneet) #12800\n",
    "        \n",
    "        self.encoder = nn.Sequential(*list(arch.children())[:cut],\n",
    "                                 Flatten(),\n",
    "                                 LinBnDrop(self.in_dim,self.hidden_dim,bn=True,p=0.0,act=nn.ReLU(),lin_first=True)\n",
    "                                )\n",
    "\n",
    "\n",
    "\n",
    "        # VAE Bottleneck\n",
    "        self.bn = VAELayer(self.hidden_dim,self.latent_dim)     \n",
    "\n",
    "        #decoder\n",
    "        n_blocks = 5        \n",
    "        nfs = [3] + [2**i*n_blocks for i in range(n_blocks+1)] \n",
    "        nfs.reverse()\n",
    "        n = len(nfs)\n",
    "        \n",
    "        modules =  [UpsampleBlock(nfs[i]) for i in range(n - 2)]        \n",
    "        self.decoder = nn.Sequential( LinBnDrop(latent_dim,self.hidden_dim,\n",
    "                                                bn=True,# batch normalizaiton shouldn't be a problem here\n",
    "                                                p=0.0,act=nn.ReLU(),lin_first=True),\n",
    "                                     LinBnDrop(hidden_dim,im_size*n_blocks*n_blocks,\n",
    "                                                bn=True,# batch normalizaiton shouldn't be a problem here\n",
    "                                                p=0.0,act=nn.ReLU(),lin_first=True),\n",
    "                                      ResizeBatch(im_size,n_blocks,n_blocks),\n",
    "                                      *modules,\n",
    "                                      ConvLayer(nfs[-2],nfs[-1],\n",
    "                                                ks=1,padding=0, norm_type=None, #act_cls=nn.Sigmoid) )\n",
    "                                                act_cls=partial(SigmoidRange, *out_range)))\n",
    "        \n",
    "\n",
    "\n",
    "    def decode(self, z):    \n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "    \n",
    "    def reparam(self, h):\n",
    "        return self.bn(h)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.reparam(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        latents = torch.stack([mu,logvar],dim=-1)\n",
    "        return x_hat, latents # assume dims are [batch,latent_dim,concat_dim]\n",
    "    \n",
    "resnet_vae = VAE_rnet(enc_dim=512)\n",
    "x_hat,latents = resnet_vae.cuda()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ARCHITECUTE\n",
    "def create_encoder(nfs,ks,conv=nn.Conv2d,bn=nn.BatchNorm2d,act_fn = nn.ReLU):\n",
    "    \"\"\"\n",
    "    constructor for generic convolutional encoder \n",
    "    \"\"\"\n",
    "    n = len(nfs)\n",
    "    conv_layers = [nn.Sequential(ConvBnRelu(nfs[i],nfs[i+1],kernel_size=ks[i],\n",
    "                                            conv = conv,bn=bn,act_fn=act_fn, padding = ks[i] //2 ),\n",
    "                                 Downsample(channels=nfs[i+1],filt_size=3,stride=2))\n",
    "                                   for i in range(n-1)]        \n",
    "    convs = nn.Sequential(*conv_layers)\n",
    "    return convs\n",
    "\n",
    "def create_encoder_denseblock(n_dense,c_start):\n",
    "    \"\"\"\n",
    "    constructor for resnet with dense blocks  (?) \n",
    "\n",
    "    n_dense\": 3,\n",
    "    \"c_start\": 4\n",
    "    \"\"\"\n",
    "    first_layer = nn.Sequential(ConvBnRelu(3,c_start,kernel_size=3,padding = 1),\n",
    "                                ResBlock(c_start),\n",
    "                                Downsample(channels=4,filt_size=3,stride=2))\n",
    "    \n",
    "    layers = [first_layer] + [\n",
    "        nn.Sequential(\n",
    "            DenseBlock(c_start * (2**c)),\n",
    "            Downsample(channels=c_start * (2**(c+1)),filt_size=3,stride=2)) for c in range(n_dense)\n",
    "    ]\n",
    "    \n",
    "    model = nn.Sequential(*layers)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_decoder(nfs, ks, size, conv=nn.Conv2d, bn=nn.BatchNorm2d, act_fn=nn.ReLU):\n",
    "    \"\"\"\n",
    "    CURR VALUES:\n",
    "    \"nfs\":[66,3*32,3*16,3*8,3*4,3*2,3,1,3],\n",
    "    \"ks\": [ 3, 1, 3,1,3,1,3,1],   \n",
    "    \"size\": IMG_SIZE \n",
    "    \"\"\"\n",
    "    n = len(nfs)\n",
    "    \n",
    "    # We add two channels to the first layer to include x and y channels\n",
    "    first_layer = ConvBnRelu(nfs[0], #input size \n",
    "                             nfs[1], # output size\n",
    "                             conv = PointwiseConv,\n",
    "                             bn=bn,\n",
    "                             act_fn=act_fn)\n",
    "\n",
    "    conv_layers = [first_layer] + [ConvBnRelu(nfs[i],nfs[i+1],kernel_size=ks[i-1],\n",
    "                                              padding = ks[i-1] // 2,conv = conv,bn=bn,act_fn=act_fn)\n",
    "                                   for i in range(1,n - 1)]        \n",
    "    dec_convs = nn.Sequential(*conv_layers)\n",
    "    \n",
    "    dec = nn.Sequential(SpatialDecoder2D(size),dec_convs)\n",
    "    #SigmoidRange(*y_range)\n",
    "    return dec\n",
    "\n",
    "def decoder_simple(y_range=OUT_RANGE, n_out=3):\n",
    "    return nn.Sequential(#UpsampleBlock(64),\n",
    "                         UpsampleBlock(32),\n",
    "                         nn.Conv2d(16, n_out, 1),\n",
    "                         SigmoidRange(*y_range)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
