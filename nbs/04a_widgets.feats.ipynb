{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp widgets.feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from snkrfinder.imports import *\n",
    "from snkrfinder.core import *\n",
    "from snkrfinder.data.munge import *\n",
    "from snkrfinder.data.load import *\n",
    "from snkrfinder.model.core import *\n",
    "from snkrfinder.model.transfer import *\n",
    "from snkrfinder.model.cvae import *\n",
    "\n",
    "#from ipywidgets import widgets\n",
    "#from ipywidgets import HBox,VBox,widgets,Button,Checkbox,Dropdown,Layout,Box,Output,Label,FileUpload\n",
    "# from fastai.vision.widgets import *  # in imports\n",
    "#from ipywidgets import Tab #fastai didn't include Tab\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first snkrfinder.widgets.core\n",
    "\n",
    "\n",
    "\n",
    "## OVERVIEW: model module- MobileNet_v2 feature extractor\n",
    "\n",
    "This is a project initiated while an Insight Data Science fellow.  It grew out of my interest in making data driven tools in the fashion/retail space I had most recently been working.   The original over-scoped idea was to make a shoe desighn tool which could quickly develop some initial sneakers based on choosing some examples, and some text descriptors.  Designs are constrained by the \"latent space\" defined (discovered?) by a database of shoe images.  However, given the 3 week sprint allowed for development, I pared the tool down to a simple \"aesthetic\" recommender for sneakers, using the same idea of utilizing an embedding space defined by the database fo shoe images.\n",
    "\n",
    "Widgets:\n",
    "\n",
    "These are litterally the 2.0 version of SneakerFinder.  Suitable to import and run a simple viola notebook page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: create tools out of widgets... i.e. make SneakerFinder 2.0 in the fastai framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.7\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "print(fastai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ergonyc/Projects/Project2.0/snkrfinder/nbs\n",
      "/home/ergonyc/Projects/Project2.0/snkr-finder\n"
     ]
    }
   ],
   "source": [
    "print(Path().cwd())\n",
    "os.chdir(L_ROOT)\n",
    "print(Path().cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should go into a utils or cfg module\n",
    "HOME = get_home()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the decapitated featurenet e.g. mobilnet_v2 or resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "filename = ZAPPOS_FEATS_ALL_SORT # \"zappos-50k-mobilenetv2-features_sort_3\"\n",
    "df = pd.read_pickle(f\"data/{filename}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "images_path = D_ROOT/DBS['zappos']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SANITY CHECK: \n",
    "\n",
    "Just want to chack that we can we extract single features that match those we just calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>classes_md</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27079</th>\n",
       "      <td>Shoes/Sneakers and Athletic Shoes/Nike/7716996.288224.jpg</td>\n",
       "      <td>27079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            path  classes_md\n",
       "27079  Shoes/Sneakers and Athletic Shoes/Nike/7716996.288224.jpg       27079"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sicne we made sure our indices match up with our \"classes\" things should be easy\n",
    "query_image = \"Shoes/Sneakers and Athletic Shoes/Nike/7716996.288224.jpg\"\n",
    "\n",
    "df.loc[df.path==query_image,['path','classes_md']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataBlock performed a number of processing steps to prepare the images for embedding into the MobileNet_v2 space (a 2*1280 vector). (Because we pooled space as as _average_ and _max_ we have 2x dimensions.) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have made functional wrappers as well as a _fastai_ `pipeline` [02_models.ipynb], and confirmed that they are equivalent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example _by hand_ pipeline with the MobileNet V2 is:\n",
    "\n",
    "```python\n",
    "# get net, prep image\n",
    "mnet1 = get_mnetV2_feature_net(to_cuda=False)\n",
    "t_image1 = load_and_prep_sneaker(image_path,size=IMG_SIZE,to_cuda=False)\n",
    "    \n",
    "```\n",
    "\n",
    "versus the fastai based objects I defined:\n",
    "\n",
    "```python\n",
    "# FASTAI: get net, prep image, get feats \n",
    "\n",
    "mnet2 = create_cnn_featurenet(torchvision.models.mobilenet_v2,to_cuda=True)\n",
    "t_image2 = load_and_prep_tf_pipe() # Pipeline\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "mnet1 = get_mnetV2_feature_net()\n",
    "query_t1 = load_and_prep_sneaker(images_path/QUERY_IM)\n",
    "test_feats1 = get_convnet_feature(mnet1,query_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorImage(1.2784), TensorBase(1.2784), TensorImage(0.))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnet2 = create_cnn_featurenet('mobilenet_v2')\n",
    "query_t2 = load_and_prep_tf_pipe(images_path/QUERY_IM)\n",
    "test_feats2 = get_convnet_feature(mnet2,query_t2)\n",
    "\n",
    "\n",
    "test_feats1.mean(),test_feats2.mean(),(test_feats1-test_feats2).max(),\n",
    "#PILImage.create((query_t1-query_t2).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have the \"embeddings\" of the database in the mobileNet_v2 output space.  I can do a logistic regression on these vectors (should be identical to mapping these 1000 vectors to 4 categories (Part 3)) but I can also use an approximate KNN in this space to run the SneakerFinder tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors: a proxy for \"similar\"\n",
    "\n",
    "There realy isn't a ground truth to refer to for similarity of aesthetic preference so I'll start with a simple \"gut\" test: inspection of neighbors in our feature space.  Remember that the goal of all this is to find some shoes that someone will like, and we are using \"similar\" as the aproximation of human preference.\n",
    "\n",
    "Personally, I like Jordans so I chose this as my `query_image`: <img alt=\"Sample Jordan\" width=\"450\" src=\"/home/ergonyc/.fastai/data/ut-zap50k-images/Shoes/Sneakers and Athletic Shoes/Nike/7716996.288224.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the functions what will do it:\n",
    "\n",
    "```python\n",
    "        feats = get_mnet_feature(mnetv2,t_image,to_cuda=False)\n",
    "        reducer = get_umap_reducer(latents)\n",
    "        neighs = NearestNeighbors(n_neighbors=num_neighs) \n",
    "        neigh_images = query_neighs(q_feat, myneighs, data, root_path, show = True)\n",
    "        plot_sneak_neighs(neigh_images)\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widgets: preamble. load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "modelnm = 'mobilenet_v2'\n",
    "filename = f\"zappos-50k-{modelnm}-features_sort_3\"\n",
    "df = pd.read_pickle(f\"data/{filename}.pkl\")\n",
    "\n",
    "\n",
    "model = create_cnn_featurenet(modelnm,to_cuda=False)\n",
    "MODELS = {modelnm:model}\n",
    "        \n",
    "modelnm = 'resnet18'\n",
    "filename = f\"zappos-50k-{modelnm}-features_sort_3\"\n",
    "df = pd.read_pickle(f\"data/{filename}.pkl\")\n",
    "\n",
    "model = create_cnn_featurenet(modelnm,to_cuda=False)\n",
    "MODELS[modelnm] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_featurenets(model_list):\n",
    "    return {m:create_cnn_featurenet(m,to_cuda=False) for m in model_list }\n",
    "    \n",
    "\n",
    "MODELS = pack_featurenets(['mobilenet_v2','resnet18'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle we can easily load different feature models... but for now we only have mobilenet_v2 and resnet18 databases calculated for all the zappos data. \n",
    "\n",
    "TODO:  test that the outputs of xresnet and resnet are equivalent.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "        modelnm = 'resnet34'\n",
    "        model = create_cnn_featurenet(modelnm,to_cuda=False)\n",
    "        MODELS[modelnm] = model\n",
    "\n",
    "        modelnm = 'xresnet18'\n",
    "        model = create_cnn_featurenet(modelnm,to_cuda=False)\n",
    "        MODELS[modelnm] = model\n",
    "        \n",
    "        modelnm = 'xresnet34'\n",
    "        model = create_cnn_featurenet(modelnm,to_cuda=False)\n",
    "        MODELS[modelnm] = model\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Now lets load the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODELS['mobilenet_v2']\n",
    "\n",
    "num_neighs = 5\n",
    "\n",
    "# save the knns and umap reducers for later use\n",
    "filename = f\"data/{model.name}-knn{num_neighs}Xsize.pkl\"\n",
    "knns = load_pickle(filename)\n",
    "\n",
    "filename = f\"data/{model.name}-umapXsize.pkl\"\n",
    "reducers = load_pickle(filename)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"zappos-50k-{model.name}-features_sort_3\"\n",
    "df = pd.read_pickle(f\"data/{filename}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_feats = test_feats2\n",
    "neighs = knns['small']\n",
    "distance, nn_index = neighs.kneighbors(test_feats, return_distance=True)    \n",
    "\n",
    "dist = distance.tolist()[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #def get_umap_embedding(latents):\n",
    "# fn = df.path.values\n",
    "\n",
    "# features = f\"features_{SIZE_ABBR['small']}\"\n",
    "# print(features)\n",
    "# data = df[['Category',features]].copy()\n",
    "# db_feats = np.vstack(data[features].values)\n",
    "\n",
    "# type(db_feats)\n",
    "\n",
    "# snk2vec = dict(zip(fn,db_feats))\n",
    "\n",
    "# snk2vec[list(snk2vec.keys())[0]]\n",
    "\n",
    "# embedding = get_umap_embedding(db_feats)\n",
    "# snk2umap = dict(zip(fn,embedding))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # make the paths easily accessible\n",
    "# paths = df[['path','classes_sm','classes_md','classes_lg']]\n",
    "# neighbors = paths.iloc[nn_index.tolist()[0]].copy()\n",
    "\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widgets: make this into a \"tool\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts = [display(img.to_thumb(200,200))]+[display(i.to_thumb(100,100)) for i in images]\n",
    "\n",
    "# nnc = carousel(ts, width='1200px')\n",
    "\n",
    "# out_pl = HBox[display(img.to_thumb(200,200)), carousel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = {sz:i for (i,sz) in enumerate(IMG_SIZES)}\n",
    "    \n",
    "# [x for x in IMG_SIZES.values()].index(128)\n",
    "\n",
    "\n",
    "# #hide\n",
    "# filename = \"zappos-50k-mobilenetv2-features_sort_3\"\n",
    "# df = pd.read_pickle(f\"data/{filename}.pkl\")\n",
    "\n",
    "\n",
    "# mnetv2 = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In designing these widgets we'll use the naming convention of starting each object as \"type, underscore, description\" e.g. `btn_upload`\n",
    "\n",
    "Types are: `btn` - button, `out` - an output \"place\", `dd` - dropdown, `tab` - tab, and `lbl` - label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "# # Cell\n",
    "# @patch\n",
    "# def __getitem__(self:Box, i): return self.children[i]\n",
    "\n",
    "# # Cell\n",
    "# def widget(im, *args, **layout):\n",
    "#     \"Convert anything that can be `display`ed by IPython into a widget\"\n",
    "#     o = Output(layout=merge(*args, layout))\n",
    "#     with o: display(im)\n",
    "#     return o\n",
    "\n",
    "# # Cell\n",
    "# def _update_children(change):\n",
    "#     for o in change['owner'].children:\n",
    "#         if not o.layout.flex: o.layout.flex = '0 0 auto'\n",
    "\n",
    "# # Cell\n",
    "# def carousel(children=(), **layout):\n",
    "#     \"A horizontally scrolling carousel\"\n",
    "#     def_layout = dict(overflow='scroll hidden', flex_flow='row', display='flex')\n",
    "#     res = Box([], layout=merge(def_layout, layout))\n",
    "#     res.observe(_update_children, names='children')\n",
    "#     res.children = children\n",
    "#     return res\n",
    "# def _open_thumb(fn, h, w): return Image.open(fn).to_thumb(h, w).convert('RGBA')\n",
    "\n",
    "class SneakerFinder:\n",
    "    \"A widget that displays a SneakerFinder `fnms` along with a `Dropdown`\"\n",
    "    def __init__(self, opts=(), height=128, width=256, max_n=30):\n",
    "        opts = ('<Keep>', '<Delete>')+tuple(opts)\n",
    "        store_attr('opts,height,width,max_n')\n",
    "        self.widget = carousel(width='100%')\n",
    "0\n",
    "    def set_fnms(self, fnms):\n",
    "        self.fnms = L(fnms)[:self.max_n]\n",
    "        ims = parallel(_open_thumb, self.fnms, h=self.height, w=self.width, progress=False,\n",
    "                       n_workers=min(len(self.fnms)//10,defaults.cpus))\n",
    "        self.widget.children = [VBox([widget(im, height=f'{self.height}px'), Dropdown(\n",
    "            options=self.opts, layout={'width': 'max-content'})]) for im in ims]\n",
    "\n",
    "    def _ipython_display_(self): display(self.widget)\n",
    "        \n",
    "    def values(self): return L(self.widget.children).itemgot(1).attrgot('value')\n",
    "    def delete(self): return self.values().argwhere(eq('<Delete>'))\n",
    "    def change(self):\n",
    "        idxs = self.values().argwhere(not_(in_(['<Delete>','<Keep>'])))\n",
    "        return idxs.zipwith(self.values()[idxs])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NearestNeighbors' object has no attribute 'neighbors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2168f02b6ff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# hide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mknns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'small'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NearestNeighbors' object has no attribute 'neighbors'"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "knns['small'].neighbors\n",
    "\n",
    "\n",
    "caption = widgets.Label(value='The values of slider1 and slider2 are synchronized')\n",
    "sliders1, slider2 = widgets.IntSlider(description='Slider 1'),\\\n",
    "                    widgets.IntSlider(description='Slider 2')\n",
    "l = widgets.link((sliders1, 'value'), (slider2, 'value'))\n",
    "display(caption, sliders1, slider2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DEFAULT GLOBALS to start\n",
    "im_sz = 'small'\n",
    "model = MODELS['mobilenet_v2']\n",
    "filename = f\"zappos-50k-{model.name}-features_sort_3\"\n",
    "df = pd.read_pickle(f\"data/{filename}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "load_pipe    = Pipeline([PILImage.create,\n",
    "                         FeatsResize(size=IMG_SIZE, method='pad', pad_mode='border')] )\n",
    "\n",
    "prep_tf_pipe = Pipeline([ToTensor(),\n",
    "                         IntToFloatTensor(),\n",
    "                         Normalize.from_stats(*imagenet_stats,cuda=False)])\n",
    "\n",
    "\n",
    "def plot_umap(data,im_sz,mname):\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.scatterplot(\n",
    "        x=\"umap-one\",\n",
    "        y=\"umap-two\",\n",
    "        hue=\"Category\",\n",
    "        hue_order = ['Sneakers', 'Shoes', 'Boots','Slippers'],\n",
    "        palette=sns.color_palette(\"hls\", 4),\n",
    "        data=data.sample(frac=sld_sampfrac.value),\n",
    "        legend=\"full\",\n",
    "        alpha=0.3,ax=ax\n",
    "    )\n",
    "    ax.set_aspect('equal', 'datalim')\n",
    "    ax.set_title(f'UMAP projection of {mname} embedded UT-Zappos data (sz={IMG_SIZES[im_sz]})', fontsize=12)\n",
    "    return ax\n",
    "\n",
    "def on_click_find_similar(change):\n",
    "    \"\"\" \n",
    "    this is the 'go' signal\n",
    "    \"\"\"\n",
    "    global im_sz\n",
    "    update_knn_reducer(im_sz)\n",
    "    find_similar()\n",
    "\n",
    "def find_similar():\n",
    "    \"\"\" \n",
    "    find the knn\n",
    "    \"\"\"\n",
    "    global knns,model,im_sz,df\n",
    "    neighs = knns[im_sz]\n",
    "    \n",
    "    # load the image\n",
    "    im = btn_upload.data[-1]\n",
    "    img = load_pipe(im)\n",
    "    tensor_im = prep_tf_pipe(img)\n",
    "    feats = get_convnet_feature(model, tensor_im)\n",
    "    \n",
    "    # find the neighbors\n",
    "    distance, nn_index = neighs.kneighbors(feats.numpy(), return_distance=True)    \n",
    "    dist = distance.tolist()[0] \n",
    "    # fix path to the database...\n",
    "    neighbors = df.iloc[nn_index.tolist()[0]].copy()\n",
    "    nbr = neighbors.index\n",
    "\n",
    "    \n",
    "    #widget(im, max_width=\"292px\")\n",
    "    \n",
    "    images = [ PILImage.create(D_ROOT/DBS['zappos']/f) for f in neighbors.path]\n",
    "    \n",
    "    ts = [VBox([widget(im, max_width=\"292px\"),Label(f\"d={d:.03f}\")]) for im,d in zip(images,dist)]\n",
    "    target_im = img.to_thumb(200,200)\n",
    "    \n",
    "    car_nn = carousel(ts, width='1200px')\n",
    "    \n",
    "    out_nn_imgs.clear_output()\n",
    "    with out_nn_imgs:\n",
    "        display(HBox([widget(target_im, max_width=\"500px\"), car_nn]))\n",
    "\n",
    "    #lbl_neighs.value = f'distances: {dist}\n",
    "\n",
    "\n",
    "def update_knn_reducer(size):\n",
    "    \"update knn & reducer for new size im, but nothing is recalculated until the btn_run is clicked\"\n",
    "    # set to the current \n",
    "    global model,knns,reducers,im_sz,df\n",
    "    im_sz = size\n",
    "        \n",
    "    umap = reducers[im_sz]\n",
    "    neighs = knns[im_sz]\n",
    "\n",
    "    features = f\"features_{SIZE_ABBR[im_sz]}\"\n",
    "    data = df[['Category',features]].copy()\n",
    "    \n",
    "    db_feats = np.vstack(data[features].values)   \n",
    "    # this is probably the bottleneck...\n",
    "    embedding = umap.transform(db_feats)    \n",
    "    data['umap-one'] = embedding[:,0]\n",
    "    data['umap-two'] = embedding[:,1] \n",
    "\n",
    "    out_umap.clear_output()\n",
    "    with out_umap:\n",
    "        ax = plot_umap(data,size,model.name)\n",
    "        plt.show(ax)\n",
    "\n",
    "    find_similar()  \n",
    "\n",
    "def update_model(model_name,size):\n",
    "    \" update the model but nothing is recalculated until the btn_run is clicked\"\n",
    "    #key = {sz:i for (i,sz) in enumerate(IMG_SIZES)}\n",
    "    global model,knns,reducers,df\n",
    "    model = MODELS[model_name]\n",
    "\n",
    "    num_neighs = 5\n",
    "    if model_name!=model.name :  print(f\"dammit, '{model_name}'!='{model.name}'\")\n",
    "    # save the knns and umap reducers for later use\n",
    "    knns = load_pickle(f\"data/{model.name}-knn{num_neighs}Xsize.pkl\")\n",
    "\n",
    "    reducers = load_pickle(f\"data/{model.name}-umapXsize.pkl\")   \n",
    "    \n",
    "    filename = f\"zappos-50k-{model.name}-features_sort_3\"\n",
    "    df = pd.read_pickle(f\"data/{filename}.pkl\")\n",
    "\n",
    "    update_knn_reducer(size)\n",
    "\n",
    "#Events\n",
    "def dd_im_size_eh(change):\n",
    "    update_knn_reducer(change.new)\n",
    "    \n",
    "def dd_model_eh(change):\n",
    "    update_model(change.new,dd_im_size.value)\n",
    "\n",
    "\n",
    "#define my widgets\n",
    "btn_run = Button(description='Find similar sneaks!',layout = Layout(width='25%', height='80px'))\n",
    "btn_upload = FileUpload(layout = Layout(width='25%', height='80px'))\n",
    "\n",
    "out_umap = Output() # not doing anything here yet...\n",
    "# lbl_neighs = Label() # labels for neighbors\n",
    "out_nn_imgs = Output() # VBox([out_im,out_car])\n",
    "\n",
    "dd_im_size = Dropdown(options=IMG_SIZES.keys(),value='small',description='Image Size:' )                       \n",
    "dd_model = Dropdown(options=['mobilenet_v2','resnet18'], \n",
    "                    value='mobilenet_v2',\n",
    "                    disabled=False,\n",
    "                    description='Model:')\n",
    "                    #,layout = Layout(width='40%') ) #style=style,\n",
    "    \n",
    "sld_sampfrac = widgets.FloatSlider(value=.5,\n",
    "                min=0,\n",
    "                max=1.0,\n",
    "                step=0.05,\n",
    "                description='sample %:',\n",
    "                disabled=False,\n",
    "                continuous_update=False,\n",
    "                orientation='vertical',\n",
    "                readout=True,\n",
    "                readout_format='.2f',\n",
    ")\n",
    "              \n",
    "#item_layout = widgets.Layout(margin='0 0 50px 0')\n",
    "#input_widgets = widgets.HBox([dd_model, dd_im_size])\n",
    "\n",
    "knn_select = HBox([dd_model, dd_im_size])\n",
    "\n",
    "\n",
    "tab = widgets.Tab(children=[out_nn_imgs,HBox([out_umap, sld_sampfrac]) ] )#,layout=item_layout)\n",
    "tab.set_title(0, 'Dataset Exploration')\n",
    "tab.set_title(1, 'UMAP Plot')\n",
    "\n",
    "cta = HBox([widgets.Label('Find your sneaker!    '),\n",
    "            btn_upload,\n",
    "            btn_run])\n",
    "\n",
    "console = Label()\n",
    "dashboard =  VBox([ cta,\n",
    "                    knn_select,\n",
    "                    tab,\n",
    "                  console])\n",
    "\n",
    "console = Label()\n",
    "# actions\n",
    "btn_run.on_click(on_click_find_similar)\n",
    "dd_im_size.observe(dd_im_size_eh, names='value')\n",
    "dd_model.observe(dd_model_eh, names='value')\n",
    "# dd_lat_dim.observe(dd_lat_dim_eh, names='value')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4315006ad5394f23a3cf601bd0ec207c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Find your sneaker!    '), FileUpload(value={}, description='Upload'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning data cleaning tool\n",
    "\n",
    "\n",
    "\n",
    "WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNReLU(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNReLU(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_categories = 4\n",
    "\n",
    "transfer_mobilenet_v2(num_categories,freeze=True)\n",
    "\n",
    "# could also make a resnet transfer in a few lines of fastai api\n",
    "\n",
    "#resnet = create_cnn_model(models.resnet18, 4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ef8415459779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this should infer the number of categories and automattical re-head the resnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m learn = Learner(dls,resnet18, \n\u001b[0m\u001b[1;32m      3\u001b[0m                     \u001b[0;31m#splitter=mobilenet_split,cut=-1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     pretrained=True,metrics=error_rate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dls' is not defined"
     ]
    }
   ],
   "source": [
    "# this should infer the number of categories and automattical re-head the resnet\n",
    "learn = Learner(dls,resnet18, \n",
    "                    #splitter=mobilenet_split,cut=-1, \n",
    "                    pretrained=True,metrics=error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01a_zappos_data.ipynb.\n",
      "Converted 01b_scraped_data.ipynb.\n",
      "Converted 02a_model.ipynb.\n",
      "Converted 02b_transferlearning_model.ipynb.\n",
      "Converted 03b_latenttarget_cvae.ipynb.\n",
      "Converted 04_widgets.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import FloatSlider, interact\n",
    "from fastai2.vision.all import *\n",
    "from fastai2.vision.widgets import *\n",
    "from IPython.display import display,clear_output, Javascript\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "RED = '\\033[31m'\n",
    "BLUE = '\\033[94m'\n",
    "GREEN = '\\033[92m'\n",
    "BOLD   = '\\033[1m'\n",
    "ITALIC = '\\033[3m'\n",
    "RESET  = '\\033[0m'\n",
    "\n",
    "def dashboard_one():\n",
    "    \"\"\"GUI for first accordion window\"\"\"\n",
    "    import torchvision\n",
    "    try:\n",
    "        import fastai2; fastver = fastai2.__version__\n",
    "    except ImportError:\n",
    "        fastver = 'fastai not found'\n",
    "    try:\n",
    "        import fastprogress; fastprog = fastprogress.__version__\n",
    "    except ImportError:\n",
    "        fastprog = 'fastprogress not found'\n",
    "    try:\n",
    "        import fastpages; fastp = fastpages.__version__\n",
    "    except ImportError:\n",
    "        fastp = 'fastpages not found'\n",
    "    try:\n",
    "        import nbdev; nbd = nbdev.__version__\n",
    "    except ImportError:\n",
    "        nbd = 'nbdev not found'\n",
    "\n",
    "    print (BOLD +  RED + '>> fastGUI\\n')\n",
    "    button = widgets.Button(description='System', button_style='success')\n",
    "    ex_button = widgets.Button(description='Explore', button_style='success')\n",
    "    display(button)\n",
    "\n",
    "    out = widgets.Output()\n",
    "    display(out)\n",
    "\n",
    "    def on_button_clicked_info(b):\n",
    "        with out:\n",
    "            clear_output()\n",
    "            print(BOLD + BLUE + \"fastai2 version: \" + RESET + ITALIC + str(fastver))\n",
    "            print(BOLD + BLUE + \"nbdev version: \" + RESET + ITALIC + str(nbd))\n",
    "            print(BOLD + BLUE + \"fastprogress version: \" + RESET + ITALIC + str(fastprog))\n",
    "            print(BOLD + BLUE + \"fastpages version: \" + RESET + ITALIC + str(fastp) + '\\n')\n",
    "            print(BOLD + BLUE + \"python version: \" + RESET + ITALIC + str(sys.version))\n",
    "            print(BOLD + BLUE + \"torchvision: \" + RESET + ITALIC + str(torchvision.__version__))\n",
    "            print(BOLD + BLUE + \"torch version: \" + RESET + ITALIC + str(torch.__version__))\n",
    "            print(BOLD + BLUE + \"\\nCuda: \" + RESET + ITALIC + str(torch.cuda.is_available()))\n",
    "            print(BOLD + BLUE + \"cuda version: \" + RESET + ITALIC + str(torch.version.cuda))\n",
    "\n",
    "    button.on_click(on_button_clicked_info)\n",
    "def dashboard_two():\n",
    "    \"\"\"GUI for second accordion window\"\"\"\n",
    "    dashboard_two.datas = widgets.ToggleButtons(\n",
    "        options=['PETS', 'CIFAR', 'IMAGENETTE_160', 'IMAGEWOOF_160', 'MNIST_TINY'],\n",
    "        description='Choose',\n",
    "        value=None,\n",
    "        disabled=False,\n",
    "        button_style='info',\n",
    "        tooltips=[''],\n",
    "        style=style\n",
    "    )\n",
    "    display(dashboard_two.datas)\n",
    "\n",
    "    button = widgets.Button(description='Explore', button_style='success')\n",
    "    display(button)\n",
    "    out = widgets.Output()\n",
    "    display(out)\n",
    "    def on_button_explore(b):\n",
    "        with out:\n",
    "            clear_output()\n",
    "            ds_choice()\n",
    "            show()\n",
    "    button.on_click(on_button_explore)\n",
    "\n",
    "#Helpers for dashboard two\n",
    "def ds_choice():\n",
    "    \"\"\"Helper for dataset choices\"\"\"\n",
    "    if dashboard_two.datas.value == 'PETS':\n",
    "        ds_choice.source = untar_data(URLs.DOGS)\n",
    "    elif dashboard_two.datas.value == 'CIFAR':\n",
    "        ds_choice.source = untar_data(URLs.CIFAR)\n",
    "    elif dashboard_two.datas.value == 'IMAGENETTE_160':\n",
    "        ds_choice.source = untar_data(URLs.IMAGENETTE_160)\n",
    "    elif dashboard_two.datas.value == 'IMAGEWOOF_160':\n",
    "        ds_choice.source = untar_data(URLs.IMAGEWOOF_160)\n",
    "    elif dashboard_two.datas.value == 'MNIST_TINY':\n",
    "        ds_choice.source = untar_data(URLs.MNIST_TINY)\n",
    "\n",
    "def plt_classes():\n",
    "    ds_choice()\n",
    "    print(BOLD + BLUE + \"Dataset: \" + RESET + BOLD + RED + str(dashboard_two.datas.value))\n",
    "    \"\"\"Helper for plotting classes in folder\"\"\"\n",
    "    Path.BASE_PATH = ds_choice.source\n",
    "    train_source = (ds_choice.source/'train/').ls().items\n",
    "    print(BOLD + BLUE + \"\\n\" + \"No of classes: \" + RESET + BOLD + RED + str(len(train_source)))\n",
    "\n",
    "    num_l = []\n",
    "    class_l = []\n",
    "    for j, name in enumerate(train_source):\n",
    "        fol = (ds_choice.source/name).ls().sorted()\n",
    "        names = str(name)\n",
    "        class_split = names.split('train')\n",
    "        class_l.append(class_split[1])\n",
    "        num_l.append(len(fol))\n",
    "\n",
    "    y_pos = np.arange(len(train_source))\n",
    "    performance = num_l\n",
    "\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    plt.style.use('seaborn')\n",
    "    plt.bar(y_pos, performance, align='center', alpha=0.5, color=['black', 'red', 'green', 'blue', 'cyan'])\n",
    "    plt.xticks(y_pos, class_l, rotation=90)\n",
    "    plt.ylabel('Images')\n",
    "    plt.title('Images per Class')\n",
    "    plt.show()\n",
    "\n",
    "def display_images():\n",
    "    \"\"\"Helper for displaying images from folder\"\"\"\n",
    "    train_source = (ds_choice.source/'train/').ls().items\n",
    "    for i, name in enumerate(train_source):\n",
    "        fol = (ds_choice.source/name).ls().sorted()\n",
    "        fol_disp = fol[0:5]\n",
    "        filename = fol_disp.items\n",
    "        fol_tensor = [tensor(Image.open(o)) for o in fol_disp]\n",
    "        img = fol_tensor[0]\n",
    "        print(BOLD + BLUE + \"Loc: \" + RESET + str(name) + \" \" + BOLD + BLUE + \"Number of Images: \" + RESET +\n",
    "              BOLD + RED + str(len(fol)))\n",
    "\n",
    "        fig = plt.figure(figsize=(15,15))\n",
    "        columns = 5\n",
    "        rows = 1\n",
    "        ax = []\n",
    "\n",
    "        for i in range(columns*rows):\n",
    "            for i, j in enumerate(fol_tensor):\n",
    "                img = fol_tensor[i]    # create subplot and append to ax\n",
    "                ax.append( fig.add_subplot(rows, columns, i+1))\n",
    "                ax[-1].set_title(\"ax:\"+str(filename[i]))  # set title\n",
    "                plt.tick_params(bottom=\"on\", left=\"on\")\n",
    "                plt.xticks([])\n",
    "                plt.imshow(img)\n",
    "        plt.show()\n",
    "def browse_images():\n",
    "    print(BOLD + BLUE + \"Use slider to choose image\" + RESET)\n",
    "    ds_choice()\n",
    "    items = get_image_files(ds_choice.source/'train/')\n",
    "    n = len(items)\n",
    "    def view_image(i):\n",
    "        plt.imshow(Image.open(items[i]), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        plt.title('Training: %s' % items[i])\n",
    "        browse_images.img = items[i]\n",
    "        plt.show()\n",
    "    interact(view_image, i=(0,n-1))\n",
    "\n",
    "def show():\n",
    "    a = widgets.Output()\n",
    "    b = widgets.Output()\n",
    "    c = widgets.Output()\n",
    "    with a:\n",
    "        plt_classes()\n",
    "    with b:\n",
    "        display_images()\n",
    "    with c:\n",
    "        browse_images()\n",
    "    view_one = VBox([a, c])\n",
    "    view_two = HBox([view_one, b])\n",
    "    display(view_two)\n",
    "\n",
    "def aug_show():\n",
    "    aug_button = widgets.Button(description='Augmentations', button_style='success')\n",
    "    display(aug_button)\n",
    "    aug_out = widgets.Output()\n",
    "    display(aug_out)\n",
    "    def on_aug_button(b):\n",
    "        with aug_out:\n",
    "            clear_output()\n",
    "            j = widgets.Output()\n",
    "            u = widgets.Output()\n",
    "            with j:\n",
    "                print(browse_images.img)\n",
    "                display(Image.open(browse_images.img))\n",
    "            with u:\n",
    "                aug_dash()\n",
    "            display(HBox([j, u]))\n",
    "    aug_button.on_click(on_aug_button)\n",
    "\n",
    "def aug_paras():\n",
    "    \"\"\"If augmentations is choosen show available parameters\"\"\"\n",
    "    print(BOLD + BLUE + \"Choose Augmentation Parameters: \")\n",
    "    button_paras = widgets.Button(description='Confirm', button_style='success')\n",
    "\n",
    "    aug_paras.hh = widgets.ToggleButton(value=False, description='Erase', button_style='info',\n",
    "                                      style=style)\n",
    "    aug_paras.cc = widgets.ToggleButton(value=False, description='Contrast', button_style='info',\n",
    "                                      style=style)\n",
    "    aug_paras.dd = widgets.ToggleButton(value=False, description='Rotate', button_style='info',\n",
    "                                      style=style)\n",
    "    aug_paras.ee = widgets.ToggleButton(value=False, description='Warp', button_style='info',\n",
    "                                      style=style)\n",
    "    aug_paras.ff = widgets.ToggleButton(value=False, description='Bright', button_style='info',\n",
    "                                      style=style)\n",
    "    aug_paras.gg = widgets.ToggleButton(value=False, description='DihedralFlip', button_style='info',\n",
    "                                      style=style)\n",
    "    aug_paras.ii = widgets.ToggleButton(value=False, description='Zoom', button_style='info',\n",
    "                                      style=style)\n",
    "\n",
    "    qq = widgets.HBox([aug_paras.hh, aug_paras.cc, aug_paras.dd, aug_paras.ee, aug_paras.ff, aug_paras.gg, aug_paras.ii])\n",
    "    display(qq)\n",
    "    display(button_paras)\n",
    "    aug_par = widgets.Output()\n",
    "    display(aug_par)\n",
    "    def on_button_two_click(b):\n",
    "        with aug_par:\n",
    "            clear_output()\n",
    "            aug_dash_choice()\n",
    "    button_paras.on_click(on_button_two_click)\n",
    "\n",
    "def aug():\n",
    "    \"\"\"Aug choice helper\"\"\"\n",
    "    #Erase\n",
    "    if aug_paras.hh.value == True:\n",
    "            aug.b_max = FloatSlider(min=0,max=50,step=1,value=0, description='max count',\n",
    "                                     orientation='horizontal', disabled=False)\n",
    "            aug.b_pval = FloatSlider(min=0,max=1,step=0.1,value=0, description=r\"$p$\",\n",
    "                                     orientation='horizontal', disabled=False)\n",
    "            aug.b_asp = FloatSlider(min=0.1,max=5, step=0.1, value=0.3, description=r'$aspect$',\n",
    "                                     orientation='horizontal', disabled=False)\n",
    "            aug.b_len = FloatSlider(min=0.1,max=5, step=0.1, value=0.3, description=r'$sl$',\n",
    "                                     orientation='horizontal', disabled=False)\n",
    "            aug.b_ht = FloatSlider(min=0.1,max=5, step=0.1, value=0.3, description=r'$sh$',\n",
    "                                     orientation='horizontal', disabled=False)\n",
    "            aug.erase_code = 'this is ERASE on'\n",
    "    if aug_paras.hh.value == False:\n",
    "            aug.b_max = FloatSlider(min=0,max=10,step=1,value=0, description='max count',\n",
    "                                     orientation='horizontal', disabled=True)\n",
    "            aug.b_pval = FloatSlider(min=0,max=1,step=0.1,value=0, description='p',\n",
    "                                     orientation='horizontal', disabled=True)\n",
    "            aug.b_asp = FloatSlider(min=0.1,max=1.7,value=0.3, description='aspect',\n",
    "                                     orientation='horizontal', disabled=True)\n",
    "            aug.b_len = FloatSlider(min=0.1,max=1.7,value=0.3, description='length',\n",
    "                                     orientation='horizontal', disabled=True)\n",
    "            aug.b_ht = FloatSlider(min=0.1,max=1.7,value=0.3, description='height',\n",
    "                                     orientation='horizontal', disabled=True)\n",
    "            aug.erase_code = 'this is ERASE OFF'\n",
    "    #Contrast\n",
    "    if aug_paras.cc.value == True:\n",
    "            aug.b1_max = FloatSlider(min=0,max=0.9,step=0.1,value=0.2, description='max light',\n",
    "                                  orientation='horizontal', disabled=False)\n",
    "            aug.b1_pval = FloatSlider(min=0,max=1.0,step=0.05,value=0.75, description='p',\n",
    "                                  orientation='horizontal', disabled=False)\n",
    "            aug.b1_draw = FloatSlider(min=0,max=100,step=1,value=1, description='draw',\n",
    "                                  orientation='horizontal', disabled=False)\n",
    "    else:\n",
    "            aug.b1_max = FloatSlider(min=0,max=0.9,step=0.1,value=0, description='max light',\n",
    "                                  orientation='horizontal', disabled=True)\n",
    "            aug.b1_pval = FloatSlider(min=0,max=1.0,step=0.05,value=0.75, description='p',\n",
    "                                  orientation='horizontal', disabled=True)\n",
    "            aug.b1_draw = FloatSlider(min=0,max=100,step=1,value=1, description='draw',\n",
    "                                  orientation='horizontal', disabled=True)\n",
    "    #Rotate\n",
    "    if aug_paras.dd.value == True:\n",
    "            aug.b2_max = FloatSlider(min=0,max=10,step=1,value=0, description='max degree',\n",
    "                                  orientation='horizontal', disabled=False)\n",
    "            aug.b2_pval = FloatSlider(min=0,max=1,step=0.1,value=0.5, description='p',\n",
    "                                  orientation='horizontal', disabled=False)\n",
    "    else:\n",
    "            aug.b2_max = FloatSlider(min=0,max=10,step=1,value=0, description='max degree',\n",
    "                                  orientation='horizontal', disabled=True)\n",
    "            aug.b2_pval = FloatSlider(min=0,max=1,step=0.1,value=0, description='p',\n",
    "                                  orientation='horizontal', disabled=True)\n",
    "    #Warp\n",
    "    if aug_paras.ee.value == True:\n",
    "            aug.b3_mag = FloatSlider(min=0,max=10,step=1,value=0, description='magnitude',\n",
    "                                  orientation='horizontal', disabled=False)\n",
    "            aug.b3_pval = FloatSlider(min=0,max=1,step=0.1,value=0, description='p',\n",
    "                                  orientation='horizontal', disabled=False)\n",
    "    else:\n",
    "            aug.b3_mag = FloatSlider(min=0,max=10,step=1,value=0, description='magnitude',\n",
    "                                  orientation='horizontal', disabled=True)\n",
    "            aug.b3_pval = FloatSlider(min=0,max=10,step=1,value=0, description='p',\n",
    "                                  orientation='horizontal', disabled=True)\n",
    "    #Bright\n",
    "    if aug_paras.ff.value == True:\n",
    "            aug.b4_max = FloatSlider(min=0,max=10,step=1,value=0, description='max light',\n",
    "                                  orientation='horizontal', disabled=False)\n",
    "            aug.b4_pval = FloatSlider(min=0,max=1,step=0.1,value=0, description='p',\n",
    "                                  orientation='horizontal', disabled=False)\n",
    "    else:\n",
    "            aug.b4_max = FloatSlider(min=0,max=10,step=1,value=0, description='max_light',\n",
    "                                  orientation='horizontal', disabled=True)\n",
    "            aug.b4_pval = FloatSlider(min=0,max=1,step=0.1,value=0, description='p',\n",
    "                                  orientation='horizontal', disabled=True)\n",
    "    #DihedralFlip\n",
    "    if aug_paras.gg.value == True:\n",
    "            aug.b5_pval = FloatSlider(min=0,max=1,step=0.1, description='p',\n",
    "                                     orientation='horizontal', disabled=False)\n",
    "            aug.b5_draw = FloatSlider(min=0,max=7,step=1, description='p',\n",
    "                                     orientation='horizontal', disabled=False)\n",
    "    else:\n",
    "            aug.b5_pval = FloatSlider(min=0,max=1,step=0.1, description='p',\n",
    "                                     orientation='horizontal', disabled=True)\n",
    "            aug.b5_draw = FloatSlider(min=0,max=7,step=1, description='p',\n",
    "                                     orientation='horizontal', disabled=True)\n",
    "    #Zoom\n",
    "    if aug_paras.ii.value == True:\n",
    "            aug.b6_zoom = FloatSlider(min=1,max=5,step=0.1, description='max_zoom',\n",
    "                                     orientation='horizontal', disabled=False)\n",
    "            aug.b6_pval = FloatSlider(min=0,max=1,step=0.1, description='p',\n",
    "                                     orientation='horizontal', disabled=False)\n",
    "    else:\n",
    "            aug.b6_zoom = FloatSlider(min=1,max=5,step=0.1, description='max_zoom',\n",
    "                                     orientation='horizontal', disabled=True)\n",
    "            aug.b6_pval = FloatSlider(min=0,max=1,step=1, description='p',\n",
    "                                     orientation='horizontal', disabled=True)\n",
    "\n",
    "def aug_dash_choice():\n",
    "    \"\"\"Augmention parameter display helper\"\"\"\n",
    "    button_aug_dash = widgets.Button(description='View', button_style='success')\n",
    "    item_erase_val= widgets.HBox([aug.b_max, aug.b_pval, aug.b_asp, aug.b_len, aug.b_ht])\n",
    "    item_erase = widgets.VBox([aug_paras.hh, item_erase_val])\n",
    "\n",
    "    item_contrast_val = widgets.HBox([aug.b1_max, aug.b1_pval, aug.b1_draw])\n",
    "    item_contrast = widgets.VBox([aug_paras.cc, item_contrast_val])\n",
    "\n",
    "    item_rotate_val = widgets.HBox([aug.b2_max, aug.b2_pval])\n",
    "    item_rotate = widgets.VBox([aug_paras.dd, item_rotate_val])\n",
    "\n",
    "    item_warp_val = widgets.HBox([aug.b3_mag, aug.b3_pval])\n",
    "    item_warp = widgets.VBox([aug_paras.ee, item_warp_val])\n",
    "\n",
    "    item_bright_val = widgets.HBox([aug.b4_max, aug.b4_pval])\n",
    "    item_bright = widgets.VBox([aug_paras.ff, item_bright_val])\n",
    "\n",
    "    item_dihedral_val = widgets.HBox([aug.b5_pval, aug.b5_draw])\n",
    "    item_dihedral = widgets.VBox([aug_paras.gg, item_dihedral_val])\n",
    "\n",
    "    item_zoom_val = widgets.HBox([aug.b6_zoom, aug.b6_pval])\n",
    "    item_zoom = widgets.VBox([aug_paras.ii, item_zoom_val])\n",
    "\n",
    "    items = [item_erase, item_contrast, item_rotate, item_warp, item_bright, item_dihedral, item_zoom]\n",
    "    dia = Box(items, layout=Layout(\n",
    "                    display='flex',\n",
    "                    flex_flow='column',\n",
    "                    flex_grow=0,\n",
    "                    flex_wrap='wrap',\n",
    "                    border='solid 1px',\n",
    "                    align_items='flex-start',\n",
    "                    align_content='flex-start',\n",
    "                    justify_content='space-between',\n",
    "                    width='flex'\n",
    "                    ))\n",
    "    display(dia)\n",
    "    display(button_aug_dash)\n",
    "    aug_dash_out = widgets.Output()\n",
    "    display(aug_dash_out)\n",
    "    def on_button_two(b):\n",
    "        with aug_dash_out:\n",
    "            clear_output()\n",
    "            print(browse_images.img)\n",
    "    button_aug_dash.on_click(on_button_two)\n",
    "\n",
    "def aug_dash():\n",
    "    \"\"\"GUI for augmentation dashboard\"\"\"\n",
    "    tg = widgets.Button(description='Pad', disabled=True, button_style='info')\n",
    "    aug_dash.pad = widgets.ToggleButtons(value='Reflection', options=['Zeros', 'Reflection', 'Border'], description='',\n",
    "                                         button_style='info',style=style, layout=Layout(width='auto'))\n",
    "    th = widgets.Button(description='ResizeMethod', disabled=True, button_style='warning')\n",
    "    aug_dash.rzm = widgets.ToggleButtons(value='Squish', options=['Squish', 'Pad', 'Crop'], description='',\n",
    "                                         button_style='warning', style=style, layout=Layout(width='auto'))\n",
    "    ti = widgets.Button(description='Resize', disabled=True, button_style='primary')\n",
    "    aug_dash.res = widgets.ToggleButtons(value='128', options=['28', '64', '128', '194', '254'], description='',\n",
    "                                         button_style='primary', style=style, layout=Layout(width='auto'))\n",
    "    aug_paras.hh = widgets.ToggleButton(value=False, description='Erase', button_style='info',\n",
    "                                      style=style)\n",
    "    aug_paras.cc = widgets.ToggleButton(value=False, description='Contrast', button_style='info',\n",
    "                                      style=style)\n",
    "    aug_paras.dd = widgets.ToggleButton(value=False, description='Rotate', button_style='info',\n",
    "                                      style=style)\n",
    "    aug_paras.ee = widgets.ToggleButton(value=False, description='Warp', button_style='info',\n",
    "                                      style=style)\n",
    "    aug_paras.ff = widgets.ToggleButton(value=False, description='Bright', button_style='info',\n",
    "                                      style=style)\n",
    "    aug_paras.gg = widgets.ToggleButton(value=False, description='DihedralFlip', button_style='info',\n",
    "                                      style=style)\n",
    "    aug_paras.ii = widgets.ToggleButton(value=False, description='Zoom', button_style='info',\n",
    "                                      style=style)\n",
    "\n",
    "    qq = widgets.HBox([aug_paras.hh, aug_paras.cc, aug_paras.dd, aug_paras.ee, aug_paras.ff, aug_paras.gg, aug_paras.ii])\n",
    "\n",
    "    it2 = [tg, aug_dash.pad]\n",
    "    it3 = [th, aug_dash.rzm]\n",
    "    it4 = [ti, aug_dash.res]\n",
    "    il = widgets.HBox(it2)\n",
    "    ik = widgets.HBox(it3)\n",
    "    ij = widgets.HBox(it4)\n",
    "    ir = widgets.VBox([il, ik, ij])\n",
    "    display(ir)\n",
    "    print(BOLD + BLUE + \"Choose Augmentation Parameters: \")\n",
    "    display(qq)\n",
    "    aug_img()\n",
    "\n",
    "def show_imagee(im, **kwargs):\n",
    "    \"Show_image helper for viewing images in Voila\"\n",
    "    # Handle pytorch axis order\n",
    "    if hasattrs(im, ('data','cpu','permute')):\n",
    "        im = im.data.cpu()\n",
    "        if im.shape[0]<5: im=im.permute(1,2,0)\n",
    "    elif not isinstance(im,np.ndarray): im=array(im)\n",
    "    # Handle 1-channel images\n",
    "    if im.shape[-1]==1: im=im[...,0]\n",
    "    it = Tensor(im)\n",
    "    img = Image.fromarray(im, 'RGB')\n",
    "    display(img)\n",
    "\n",
    "def aug_img():\n",
    "    aug_img_b = widgets.Button(description='Confirm', button_style='success')\n",
    "    display(aug_img_b)\n",
    "    aug_img_out = widgets.Output()\n",
    "    display(aug_img_out)\n",
    "    def aug_img_(b):\n",
    "        with aug_img_out:\n",
    "            clear_output()\n",
    "            aug_img = browse_images.img\n",
    "            imgt = Image.open(aug_img)\n",
    "            h1, w1 = imgt.shape\n",
    "            pil_img = PILImage(PILImage.create(aug_img).resize((w1,h1))) #flip\n",
    "            print(BOLD + BLUE + 'Size:' + RED + aug_dash.res.value + BLUE + ' ResizeMode:' + RED +\n",
    "                  aug_dash.rzm.value + BLUE + ' Padding:' + RED + aug_dash.pad.value + RESET)\n",
    "            if aug_dash.rzm.value == 'Pad': method = ResizeMethod.Pad\n",
    "            if aug_dash.rzm.value == 'Squish': method = ResizeMethod.Squish\n",
    "            if aug_dash.rzm.value == 'Crop': method = ResizeMethod.Crop\n",
    "            if aug_dash.pad.value == 'Zeros': pad = PadMode.Zeros\n",
    "            if aug_dash.pad.value == 'Border': pad = PadMode.Border\n",
    "            if aug_dash.pad.value == 'Reflection': pad = PadMode.Reflection\n",
    "            rsz = Resize(int(aug_dash.res.value), method=method, pad_mode=pad)\n",
    "            display(show_imagee(rsz(pil_img)))\n",
    "    aug_img_b.on_click(aug_img_)\n",
    "\n",
    "def display_ui():\n",
    "    \"\"\" Display tabs for visual display\"\"\"\n",
    "    out1a = widgets.Output()\n",
    "    out1 = widgets.Output()\n",
    "    out2 = widgets.Output()\n",
    "    data1a = pd.DataFrame(np.random.normal(size = 50))\n",
    "    data1 = pd.DataFrame(np.random.normal(size = 100))\n",
    "    data2 = pd.DataFrame(np.random.normal(size = 150))\n",
    "\n",
    "    with out1a: #info\n",
    "        clear_output()\n",
    "        dashboard_one()\n",
    "\n",
    "    with out1: #data\n",
    "        clear_output()\n",
    "        dashboard_two()\n",
    "\n",
    "    with out2: #augmentation\n",
    "        clear_output()\n",
    "        aug_show()\n",
    "\n",
    "    display_ui.tab = widgets.Tab(children = [out1a, out1, out2])\n",
    "    display_ui.tab.set_title(0, 'Info')\n",
    "    display_ui.tab.set_title(1, 'Data')\n",
    "    display_ui.tab.set_title(2, 'Augmentation')\n",
    "    display(display_ui.tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "fnm = \"junk/london-raw.csv\"\n",
    "\n",
    "df_london = pd.read_csv(fnm)\n",
    "\n",
    "df_london.head()\n",
    "\n",
    "df_london.loc[:,'visits']=df_london[\"Visits (000s)\"]\n",
    "df_london.loc[:,'spend']=df_london[\"Spend (£m)\"]\n",
    "df_london.loc[:,'nights']=df_london[\"Nights (000s)\"]\n",
    "df_london = df_london.sample(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "ALL = 'ALL'\n",
    "def unique_sorted_values_plus_ALL(array):\n",
    "    unique = array.unique().tolist()\n",
    "    unique.sort()\n",
    "    unique.insert(0, ALL)\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "def colour_ge_value(value, comparison):\n",
    "    if value >= comparison:\n",
    "        return 'color: red'\n",
    "    else:\n",
    "        return 'color: black'\n",
    "    \n",
    "\n",
    "output = widgets.Output()\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "dropdown_year = widgets.Dropdown(options = unique_sorted_values_plus_ALL(df_london.year),\n",
    "                                 description='Year:' )                       \n",
    "dropdown_purpose = widgets.Dropdown(options = unique_sorted_values_plus_ALL(df_london.purpose), \n",
    "                                    description='Purpose:')\n",
    "bounded_num = widgets.BoundedFloatText(min=0, max=100000, value=5, step=1, \n",
    "                                       description='Number:')\n",
    "\n",
    "def common_filtering(year, purpose,num):\n",
    "    output.clear_output()\n",
    "    if (year == ALL) & (purpose == ALL):\n",
    "        common_filter = df_london\n",
    "    elif (year == ALL):\n",
    "        common_filter = df_london[df_london.purpose == purpose]\n",
    "    elif (purpose == ALL):\n",
    "        common_filter = df_london[df_london.year == year]\n",
    "    else:\n",
    "        common_filter = df_london[(df_london.year == year)&(df_london.purpose == purpose)]\n",
    "                                 \n",
    "    with output:\n",
    "        display(common_filter.style.applymap(lambda x: colour_ge_value(x, num),\n",
    "                                                subset=['visits','spend', 'nights'] ) )\n",
    "    with plot_output:\n",
    "        sns.kdeplot(common_filter['visits'], shade=True)\n",
    "        plt.show()\n",
    "    \n",
    "def dropdown_year_eventhandler(change):\n",
    "    common_filtering(change.new, dropdown_purpose.value, bounded_num.value)\n",
    "                                 \n",
    "def dropdown_purpose_eventhandler(change):\n",
    "    common_filtering(dropdown_year.value, change.new, bounded_num.value)\n",
    "    \n",
    "def bounded_num_eventhandler(change):\n",
    "    common_filtering(dropdown_year.value, dropdown_purpose.value, \n",
    "                     change.new)\n",
    "\n",
    "dropdown_purpose.observe(dropdown_purpose_eventhandler, names='value')\n",
    "dropdown_year.observe(dropdown_year_eventhandler, names='value')\n",
    "bounded_num.observe(bounded_num_eventhandler, names='value')                                 \n",
    "\n",
    "\n",
    "item_layout = widgets.Layout(margin='0 0 50px 0')\n",
    "input_widgets = widgets.HBox([dropdown_year, dropdown_purpose, bounded_num],\n",
    "                            layout=item_layout)\n",
    "\n",
    "tab = widgets.Tab([output, plot_output],layout=item_layout)\n",
    "\n",
    "tab.set_title(0, 'Dataset Exploration')\n",
    "tab.set_title(1, 'KDE Plot')\n",
    "\n",
    "dashboard = widgets.VBox([input_widgets, tab])\n",
    "\n",
    "display(dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "\n",
    "########## this is WIP\n",
    "# import re\n",
    "# import time\n",
    "# # import matplotlib.pyplot as pltmodel\n",
    "# import matplotlib.image as mpimg\n",
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# import plotly\n",
    "# import plotly.express as px\n",
    "# import plotly.figure_factory as FF\n",
    "\n",
    "\n",
    "import bokeh.plotting as bplt #import figure, show, output_notebook\n",
    "#from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "import bokeh\n",
    "# from bokeh.palettes import Spectral10\n",
    "\n",
    "import umap\n",
    "\n",
    "#from scipy import spatial  #for now just brute force to find neighbors\n",
    "import scipy \n",
    "#from scipy.spatial import distance\n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "\n",
    "\n",
    "########################################3\n",
    "#  BOKEH\n",
    "#\n",
    "##########################################3\n",
    "def init_bokeh_plot(umap_df):\n",
    "\n",
    "    bplt.output_notebook()\n",
    "\n",
    "    datasource = bokeh.models.ColumnDataSource(umap_df)\n",
    "    color_mapping = bokeh.models.CategoricalColorMapper(factors=[\"sns\",\"goat\"],\n",
    "                                        palette=bokeh.palettes.Spectral10)\n",
    "\n",
    "    plot_figure = bplt.figure(\n",
    "        title='UMAP projection VAE latent',\n",
    "        plot_width=1000,\n",
    "        plot_height=1000,\n",
    "        tools=('pan, wheel_zoom, reset')\n",
    "    )\n",
    "\n",
    "    plot_figure.add_tools(bokeh.models.HoverTool(tooltips=\"\"\"\n",
    "    <div>\n",
    "        <div>\n",
    "            <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style='font-size: 14px'>@fname</span>\n",
    "            <span style='font-size: 14px'>@loss</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "    plot_figure.circle(\n",
    "        'x',\n",
    "        'y',\n",
    "        source=datasource,\n",
    "        color=dict(field='db', transform=color_mapping),\n",
    "        line_alpha=0.6,\n",
    "        fill_alpha=0.6,\n",
    "        size=4\n",
    "    )\n",
    "\n",
    "    return plot_figure\n",
    "\n",
    "\n",
    "def embeddable_image(label):\n",
    "    return image_formatter(label)\n",
    "\n",
    "def get_thumbnail(path):\n",
    "    i = Image.open(path)\n",
    "    i.thumbnail((64, 64), Image.LANCZOS)\n",
    "    return i\n",
    "\n",
    "def image_base64(im):\n",
    "    if isinstance(im, str):\n",
    "        im = get_thumbnail(im)\n",
    "    with BytesIO() as buffer:\n",
    "        im.save(buffer, 'png')\n",
    "        return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "def image_formatter(im):\n",
    "    return f\"data:image/png;base64,{image_base64(im)}\"\n",
    "\n",
    "\n",
    "\n",
    "# do we need it loaded... it might be fast enough??\n",
    "#@st.cache\n",
    "def load_UMAP_data():\n",
    "    data_dir = f\"data/{model_name}-X{params['x_dim'][0]}-Z{params['z_dim']}\"\n",
    "    load_dir = os.path.join(data_dir,f\"kl_weight{int(params['kl_weight']):03d}\")\n",
    "    snk2umap = ut.load_pickle(os.path.join(load_dir,\"snk2umap.pkl\"))\n",
    "    \n",
    "    return snk2umap\n",
    "\n",
    "\n",
    "def load_latent_data():\n",
    "    data_dir = f\"data/{model_name}-X{params['x_dim'][0]}-Z{params['z_dim']}\"\n",
    "    snk2umap = load_UMAP_data()\n",
    "\n",
    "    # load df (filenames and latents...)\n",
    "\n",
    "    mids = list(snk2vec.keys())\n",
    "    vecs = np.array([snk2vec[m] for m in mids])\n",
    "    vec_tree = scipy.spatial.KDTree(vecs)\n",
    "\n",
    "\n",
    "    latents = np.array(list(snk2vec.values()))\n",
    "    losses = np.array(list(snk2loss.values()))\n",
    "    labels = np.array(mids)\n",
    "\n",
    "    labels2 = np.array(list(snk2umap.keys()))\n",
    "    embedding = np.array(list(snk2umap.values()))\n",
    "\n",
    "    assert(np.all(labels == labels2))    \n",
    "    umap_df = pd.DataFrame(embedding, columns=('x', 'y'))\n",
    "\n",
    "    umap_df['digit'] = [str(x.decode()) for x in labels]\n",
    "    umap_df['image'] = umap_df.digit.map(lambda f: embeddable_image(f))\n",
    "    umap_df['fname'] = umap_df.digit.map(lambda x: f\"{x.split('/')[-3]} {x.split('/')[-1]}\")\n",
    "    umap_df['db'] = umap_df.digit.map(lambda x: f\"{x.split('/')[-3]}\")\n",
    "    umap_df['loss'] = [f\"{x:.1f}\" for x in losses]\n",
    "\n",
    "    return umap_df,snk2vec,latents, labels, vecs,vec_tree,mids\n",
    "\n",
    "\n",
    "#%%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
