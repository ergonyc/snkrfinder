# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02c_model.cvae.ipynb (unless otherwise specified).

__all__ = ['prep_df_for_datablocks', 'get_ae_btfms', 'get_ae_no_aug', 'TensorPoint', 'Tensor2Vect', 'LatentsTensor',
           'df_get_x', 'df_get_y', 'LatentsTensorBlock', 'df_ae_x', 'df_ae_y', 'LatentTupleBlock', 'get_ae_DataBlock',
           'UpsampleBlock', 'LatentLayer', 'AEEncoder', 'AEDecoder', 'build_AE_encoder', 'build_AE_decoder', 'AE',
           'AELoss', 'MyMetric', 'L1LatentReg', 'KLD', 'KLDiv', 'L2MeanMetric', 'L1MeanMetric', 'L2Metric', 'L1Metric',
           'L2BMeanMetric', 'L1BMeanMetric', 'KLWeightMetric', 'RawKLDMetric', 'WeightedKLDMetric', 'MuMetric',
           'MuSDMetric', 'StdMetric', 'StdSDMetric', 'LogvarMetric', 'LogvarSDMetric', 'default_AE_metrics',
           'short_AE_metrics', 'AnnealedLossCallback', 'default_KL_anneal_in', 'bn_splitter', 'resnetVAE_split',
           'AE_split', 'get_conv_parts', 'get_pretrained_parts', 'get_encoder_parts', 'VAELinear', 'VAELayer', 'BVAE',
           'BVAELoss', 'default_VAE_metrics', 'short_VAE_metrics', 'gaussian_kernel', 'MMD', 'rawMMD', 'MMDVAE',
           'MaxMeanDiscrepancy', 'MMDLoss', 'MMDMetric', 'short_MMEVAE_metrics', 'default_MMEVAE_metrics',
           'UpsampleResBlock', 'get_resblockencoder_parts', 'ResBlockAEDecoder', 'build_ResBlockAE_decoder',
           'ResBlockAE']

# Cell
from ..imports import *
from ..core import *
from ..data.munge import *
from .core import *
#from snkrfinder.model.transfer import *

from fastai.test_utils import show_install, synth_learner, nvidia_smi, nvidia_mem

# Cell

def prep_df_for_datablocks(df):
    df = df[["path","train","test","validate","t_t_v","Category"]].copy()
    # I could remove all the "test" rows... for now i'll choose an alternate strategy:
    # Drop all the "test" rows for now, and create an "is_valid" column...
    # should probably drop a ton of columns to jus tkeep the file paths...
    # just keep what we'll need below
    df.loc[:,'is_valid'] = df.test | df.validate
    df.loc[:,'og_idx'] = df.index

    return df



# Cell
def get_ae_btfms(stats = 'sneaker'):
    # could use globals IM_STATS['sneaker'] and IM_STATS['imagenet']
    im_stats = ([.5,.5,.5],[.5,.5,.5]) if stats == 'sneaker' else imagenet_stats
    batch_tfms = Normalize.from_stats(*im_stats)
    #batch_tfms = Normalize.from_stats([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    rand_tfms = aug_transforms(mult=1.0,
               do_flip=True,
               flip_vert=False,
               max_rotate=5.0,
               min_zoom=.95,
               max_zoom=1.0,
               max_lighting=0.1,
               max_warp=0.1,
               p_affine=0.66,
               p_lighting=0.2,
               xtra_tfms=None,
               size=None,
               mode='bilinear',
               pad_mode='border',
               align_corners=True,
               batch=False,
               min_scale=1.0)
    return rand_tfms+[batch_tfms]

def get_ae_no_aug(stats = 'sneaker'):
    im_stats = ([.5,.5,.5],[.5,.5,.5]) if stats == 'sneaker' else imagenet_stats
    batch_tfms = Normalize.from_stats(*im_stats)
    return [batch_tfms]

# Cell

# NO CLUE WHY WE NEED TO HAVE THIS.... copied
class TensorPoint(TensorBase):
    "Basic type for points in an image"
    _show_args = dict(s=10, marker='.', c='r')

    @classmethod
    def create(cls, t, img_size=None)->None:
        "Convert an array or a list of points `t` to a `Tensor`"
        return cls(tensor(t).view(-1, 2).float(), img_size=img_size)

    def show(self, ctx=None, **kwargs):
        if 'figsize' in kwargs: del kwargs['figsize']
        x = self.view(-1,2)
        ctx.scatter(x[:, 0], x[:, 1], **{**self._show_args, **kwargs})
        return ctx


class Tensor2Vect(TensorPoint): pass
# TODO:  instantiate a show method


class LatentsTensor(Tensor2Vect):
    "Basic type for latents as Tensor inheriting from TensorPoint (vectors)"
    @classmethod
    def create(cls, ts, img_size=IMG_SIZE):
        "create IMG_SIZE attr to register plotting..."

        if isinstance(ts,tuple):
            mu,logvar = ts
        elif ts is None:
            mu,logvar = None,None
        else:
            mu = None
            logvar = None
        if mu is None: mu = torch.empty(0)
        elif not isinstance(mu, Tensor): Tensor(mu)

        if logvar is None: logvar = torch.empty(0)
        elif not isinstance(logvar,Tensor): Tensor(logvar)

        t = torch.cat([mu,logvar],dim=-1) # in case its a batch?

        return cls(tensor(t).view(-1, 2).float(), img_size=img_size)

    #     def show(self, ctx=None, **kwargs):
    #         if 'figsize' in kwargs: del kwargs['figsize']
    #         x = self.view(-1,2)
    #         ctx.scatter(x[:, 0], x[:, 1], **{**self._show_args, **kwargs})
    #         return ctx
    #         mu,logvar = self
    #         if not isinstance(mu, Tensor) or not isinstance(logvar,Tensor): return ctx

    #         title_str = f"mu-> {mu.mean():e}, {mu.std():e}  logvar->{logvar.mean():e}, {logvar.std():e}"

    #         if 'figsize' in kwargs: del kwargs['figsize']
    #         if 'title' in kwargs: kwargs['title']=title_str
    #         if ctx is None:
    #             _,axs = plt.subplots(1,2, figsize=(12,6))
    #             x=torch.linspace(0,1,mu[0].shape[0])
    #             axs[0].scatter(x, mu[:], **{**self._show_args, **kwargs})
    #             axs[1].scatter(x, logvar[:], **{**self._show_args, **kwargs})
    #             ctx = axs[1]

    #         ctx.scatter(mu[:], logvar[:], **{**self._show_args, **kwargs})
    #         return ctx


# Cell

def df_get_x(r):
    "datablock df helper for VAE Block using `LatentTuple`"
    return image_path/r['path']

def df_get_y(r):
    "datablock df helper for VAE Block using `LatentTuple`"
    return (df_get_x(r),None,None)

# Cell


def LatentsTensorBlock():
    "Class wrapper for the AE `LatentTensor` Block"
    return TransformBlock(type_tfms=LatentsTensor.create, batch_tfms=noop)


def df_ae_x(r,im_path=L_ROOT/"data"):
    "Autoencoder LatentsTensorBlock datablock df helper"
    return im_path/r['path']


# need to make sure that we get the image whihc is "Identical" to the input.. how to test?

def df_ae_y(r):
    "The target is the same as the input for AE"# lambda o: o
    return df_ae_x(r)



#export
# could we do a typedispatch to manage the transforms...?
# def VAETargetTupleBlock():
#     return TransformBlock(type_tfms=VAETargetTuple.create, batch_tfms=IntToFloatTensor)

def LatentTupleBlock():
    "Class wrapper for the AE `LatentTuple` Block (depricated)"
    return TransformBlock(type_tfms=LatentTuple.create, batch_tfms=noop)



# Cell
#

def get_ae_DataBlock(aug=True,im_path=L_ROOT/"data",stats = 'sneaker',im_size=IMG_SIZE):
    "wrapper to get the standard AE datablock with `ImageBlock`,`LatentTensor` target"
    # use partials or a class wrapper to get around this yucky hack
    global image_path
    image_path = im_path

    mytfms = get_ae_btfms(stats=stats) if aug else get_ae_no_aug(stats=stats)
    block = DataBlock(blocks=(ImageBlock(cls=PILImage), ImageBlock(cls=PILImage), LatentsTensorBlock ),
              get_x=df_ae_x,
              get_y=[df_ae_y, noop], #don't need to get the LatentsTensorBlock, just create
              splitter=ColSplitter('is_valid'),
              item_tfms= FeatsResize(im_size,method='pad', pad_mode='border'),
              batch_tfms = mytfms,
              n_inp = 1)

    return block



# Cell
class UpsampleBlock(Module):
    def __init__(self, up_in_c:int, final_div:bool=True, blur:bool=False, **kwargs):
        """
        Upsampling using PixelShuffle_INCR and ConvLayer
            - up_in_c :  "Upsample input channel"
        """
        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)
        ni = up_in_c//2
        nf = ni if final_div else ni//2
        self.conv1 = ConvLayer(ni, nf, **kwargs) # since we'll apply it by hand...
        self.conv2 = ConvLayer(nf, nf, **kwargs)

    def forward(self, up_in:Tensor) -> Tensor:
        up_out = self.shuf(up_in)
        return self.conv2(self.conv1(up_out))




# Cell

class LatentLayer(Module):
    """
    This layer encodes the latent "bottleneck" and is constructed to work with the specified VAE DataBlock be a replacement for
    the variational (reparameter trick) layer for otherwise identical architecture
    """

    def __init__(self,in_features,latent_features):
        """
        Compose a linear latent layer such that the mechanics are equivalent to the VAE
              the "dummy" can be used for a shaddow logvar track a KLD estimate divergence
              from latent gaussian prior
              compute the variance across batchs for each latent feature as the dummy_var
        """
        self.latent = nn.Linear(in_features,latent_features)

    def forward(self,h):
        z = self.latent(h)
        #dummy_var = (z.var(dim=1).unsqueeze(-1).expand(z.size()) ) #variance across latent dim for each image
        dummy_var = (z.var(dim=0).unsqueeze(0).expand(z.size()) ) #latent variance across batch
        dummy_mu = z
        return z, dummy_mu, dummy_var.log()

        #return z, torch.zeros_like(z)


# Cell

class AEEncoder(Module):
    def __init__(self,arch_body,enc_dim, hidden_dim=None, im_size=IMG_SIZE):
        """
        arch_body   list of layers (e.g. arch.children()[:cut])
        enc_dim,
        hidden_dim. number of linear features to sandwich between the feature encoder and the latent layers
        """
        arch = arch_body + [Flatten()]

        if hidden_dim:  # i.e. is not None
            arch += [nn.Linear(enc_dim,hidden_dim)]
                    # [LinBnDrop(enc_dim,hidden_dim,bn=True,p=0.0,act=nn.ReLU(),lin_first=True)]

        self.encoder = nn.Sequential(*arch)
        store_attr('enc_dim,hidden_dim')

    def forward(self, x):
        return self.encoder(x)




#### TODO: refactor this to take a "BLOCK" input so we can have either UpsampleBlocks or ResBlockUpsampleBlocks
class AEDecoder(Module):
    def __init__(self, hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):
        """
        Decoder Module made of `UpsampleBlock`s returning the latent representation back into an "image"
            latent_dim - dimension of latent representation
            hidden_dim - optional additional linear layer between the latent and decoder
            im_size - passed to make sure we are scaling back to the right size
            out_range - ensures the output is on teh same scale as the _normalized_ input image
        """
        #decoder
        n_blocks = 5
        BASE = im_size//2**5

        hidden = im_size*BASE*BASE if hidden_dim is None else hidden_dim
        z_fc = [nn.Linear(latent_dim,hidden)] # [LinBnDrop(latent_dim,hidden,bn=True,p=0.0,act=nn.ReLU(),lin_first=True)]
        if hidden_dim:  # i.e. is not None
            z_fc += [nn.Linear(hidden,im_size*BASE*BASE)]  # should the hidden layer have activationa and/or batchnorm?
            #z_fc += [LinBnDrop(hidden,im_size*n_blocks*n_blocks,bn=True,p=0.0,act=nn.ReLU(),lin_first=True)]


        nfs = [3] + [2**i*BASE for i in range(n_blocks+1)]
        nfs.reverse()
        n = len(nfs)

        modules =  [UpsampleBlock(nfs[i]) for i in range(n - 2)]
        self.decoder = nn.Sequential(*z_fc,
                                      ResizeBatch(im_size,BASE,BASE),
                                      *modules,
                                      ConvLayer(nfs[-2],nfs[-1],
                                                ks=1,padding=0, norm_type=None, #act_cls=nn.Sigmoid) )
                                                act_cls=partial(SigmoidRange, *out_range)))

        store_attr('latent_dim, hidden_dim,im_size,out_range')

    def forward(self, z):
        return self.decoder(z)



# Cell

def build_AE_encoder(arch_body,enc_dim, hidden_dim=None, im_size=IMG_SIZE):
    "wrapper to sequential-ize AEEncoder class"
    encoder = AEEncoder(arch_body,enc_dim=enc_dim, hidden_dim=hidden_dim, im_size=im_size)
    return nn.Sequential(*list(encoder.children()))



def build_AE_decoder(hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):
    "wrapper to sequential-ize AEDecoder class"
    decoder = AEDecoder(hidden_dim=hidden_dim, latent_dim=latent_dim, im_size=im_size,out_range=out_range)
    return nn.Sequential(*list(decoder.children()))




# Cell

class AE(Module):
    def __init__(self,enc_parts,hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):

        """
        inputs:
            arch, cut,pretrained
            enc_dim
            latent_dim
            hidden_dim

        """
        enc_arch,enc_feats,name = enc_parts

        BASE = im_size//2**5
        enc_dim = enc_feats * BASE**2  # 2**(3*3) * (im_size//32)**2 #(output of resneet) #12800

        #encoder
        self.encoder = build_AE_encoder(enc_arch,enc_dim=enc_dim, hidden_dim=hidden_dim, im_size=im_size)

        in_dim = enc_dim if hidden_dim is None else hidden_dim

        # AE Bottleneck
        self.bn = LatentLayer(in_dim,latent_dim)

        #decoder
        self.decoder = build_AE_decoder(hidden_dim=hidden_dim, latent_dim=latent_dim, im_size=im_size,out_range=out_range)

        store_attr('name,enc_dim, in_dim,hidden_dim,latent_dim,im_size,out_range') # do i need all these?


    def decode(self, z):
        return self.decoder(z)

    def encode(self, x):
        h = self.encoder(x)
        return self.bn(h)


    def forward(self, x):
        #z, mu, logvar = self.encode(x)
        #         h = self.encoder(x)
        #         z, mu, logvar = self.bn(h) # reparam happens in the VAE layer
        #         x_hat = self.decoder(z)

        z,mu,logvar = self.encode(x)  # z and mu are the same for
        x_hat = self.decode(z)
        latents = torch.stack([mu,logvar],dim=-1)

        return x_hat, latents # assume dims are [batch,latent_dim,concat_dim]



# Cell


# class L1LatentReg(Module):
#     """
#      add alpha?
#     """
#     def __init__(self, batchmean=False):
#         """
#         reduction 'sum', else 'batchmean'

#         """
#         l_one = self._L1mean if batchmean else self._L1
#         store_attr('batchmean,l_one')

#     def _L1(self, a):
#         return a.abs().sum()

#     def _L1mean(self, a):
#         return a.abs().sum(dim=1).mean()

#     def forward(self,z):
#         return self.l_one(z)


class AELoss(Module):
    """
    wrapper for loss_func which deals with potential annealed kl_weight
    does MSE with 'mean' reduction
    'batchmean'  averages as 'sum' MSE over batches
    simple L1 regularizer on latent dimension
    """
    def __init__(self, batchmean=False, alpha=1.0,useL1=False):
        """
        reduction 'sum'

        """
        pix_loss = MSELossFlat(reduction='sum') if not useL1 else L1LossFlat(reduction='sum')
        store_attr('pix_loss,alpha,batchmean')

    def l_one_reg(self,pix_dim,z):
        l_one = z.abs().sum()
        l_one *= (3*pix_dim*pix_dim)/z.size()[1]
        return l_one

    def forward(self, preds, *target):
        """
        pred =(x_hat,KLD,kl_weight) #mu,log_var, kl_weight)
        target is x (original)
        """
        # this handles the annealed kl_weight and passing the mu,logvar around we added...
        if(len(preds) == 3):
            x_hat, latents, _ = preds
        else: #if len(preds) == 2:  # we should never get here... unless we delete teh callback
            x_hat, latents = preds


        z, _ = latents.split(1,dim=2)
        bs = latents.size()[0]

        #note: both mse and l1_reg are summing errors over batches, and pixels or latents
        pix_err = self.pix_loss(x_hat, target[0])
        pix_dim = x_hat.size()[-1]
        l1_reg = self.l_one_reg(pix_dim,z)

        total =  pix_err + self.alpha*l1_reg

        total *= (1./bs) if self.batchmean else 1.0

        return total

# Cell

class MyMetric(Metric):
    "meta-class for simple average over epoch metric quantities"
    def reset(self):
        "Clear all targs and preds"
        self.vals = []
    @property
    def value(self):
        return np.array(self.vals).mean()

class L1LatentReg(MyMetric):
    "Latent Regularizer with sum reduction and optinal batchmean scaling"
    def __init__(self,batchmean=False,alpha=1.0):
        vals = []
        store_attr('vals,batchmean,alpha')

    def accumulate(self, learn):
        #         pix_dim = to_detach(learn.y[0].size()[-1])
        latents = to_detach(learn.pred[1])
        bs = latents.size()[0]
        z, _ = latents.split(1,dim=2)
        #nll = torch.abs(recon_x - x).mean()
        l_one = z.abs().sum()
        #         l_one *= (3*pix_dim*pix_dim)/z.size()[1]
        l_one *= (self.alpha/bs) if self.batchmean else self.alpha

        self.vals.append(l_one)



# Cell

def KLD(mu,logvar):
    "KLD helper which sum across latents, but not batches"
    return -0.5 * torch.sum(1 + logvar - mu*mu - logvar.exp(),1)


class KLDiv(Module):
    """
    Module for computing the KL Divergence from a unit normal distribution.
    'batchmean' option sums first and averages over batches
    """
    def __init__(self, batchmean=False):
        """
        reduction 'sum', else 'batchmean'

        """
        store_attr('batchmean')

    def __KLD(self,mu,logvar):
        "KLD helper which sum across latents, but not batches"
        return -0.5 * torch.sum(1 + logvar - mu*mu - logvar.exp(),1)


    def forward(self, mu, logvar):
        """
        pred =(x_hat,KLD,kl_weight) #mu,log_var, kl_weight)
        target is x (original)
        """
        kld = self.__KLD(mu,logvar)

        kld = kld.mean() if self.batchmean else kld.sum()
        return kld




# Cell

class L2MeanMetric(MyMetric):
    "Mean square error"
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        x = to_detach(learn.y[0])
        recon_x = to_detach(learn.pred[0])
        nll = (recon_x - x).pow(2).mean()
        #nll = torch.mean((recon_x - x)**2)
        self.vals.append(nll)

class L1MeanMetric(MyMetric):
    "Mean absolute error"
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        x = to_detach(learn.y[0])
        recon_x = to_detach(learn.pred[0])
        #nll = torch.abs(recon_x - x).mean()
        nll = (recon_x - x).abs().mean()
        self.vals.append(nll)

class L2Metric(MyMetric):
    "Sum square error"
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        x = to_detach(learn.y[0])
        recon_x = to_detach(learn.pred[0])
        nll = (recon_x - x).pow(2).sum()
        #nll = torch.mean((recon_x - x)**2)
        self.vals.append(nll)

class L1Metric(MyMetric):
    "Sum absolute error"
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        x = to_detach(learn.y[0])
        recon_x = to_detach(learn.pred[0])
        #nll = torch.abs(recon_x - x).mean()
        nll = (recon_x - x).abs().sum()
        self.vals.append(nll)


class L2BMeanMetric(MyMetric):
    "Summed square error average across batch "
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        x = to_detach(learn.y[0])
        recon_x = to_detach(learn.pred[0])
        nll = (recon_x - x).pow(2).sum(dim=[1,2,3]).mean()
        #nll = torch.mean((recon_x - x)**2)
        self.vals.append(nll)

class L1BMeanMetric(MyMetric):
    "Summed abs error average across batch "
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        x = to_detach(learn.y[0])
        recon_x = to_detach(learn.pred[0])
        #nll = torch.abs(recon_x - x).mean()
        nll = (recon_x - x).abs().sum(dim=[1,2,3]).mean()
        self.vals.append(nll)


class KLWeightMetric(MyMetric):
    "Injected KLD weighting"
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        #kl = learn.model.kl_weight
        kl = learn.opt.hypers[0]['kl_weight']
        self.vals.append(to_detach(kl))


class RawKLDMetric(MyMetric):
    "KLD Metric, `batchmean` averages across batches"
    def __init__(self,batchmean=False):
        vals = []
        _KLD = KLDiv(batchmean=batchmean)
        store_attr('vals,batchmean,_KLD')

    def accumulate(self, learn):
        latents = learn.pred[1]
        mu, logvar = latents.split(1,dim=2)
        kld = self._KLD(mu,logvar)
        self.vals.append(to_detach(kld))

class WeightedKLDMetric(MyMetric):
    """weighted KLD Metric, `batchmean` averages across batches
            the "effective" KLD regularization in e.g. a ðœ·-BAE
    """
    def __init__(self,batchmean=False,alpha=1.0):
        vals = []
        _KLD = KLDiv(batchmean=batchmean)
        store_attr('vals,batchmean,alpha,_KLD')

    def accumulate(self, learn):
        latents = learn.pred[1]
        mu, logvar = latents.split(1,dim=2)
        kld = self.alpha*self._KLD(mu,logvar)
        self.vals.append(to_detach(kld))
        #         latents = to_detach(learn.pred[1])
        #         mu, logvar = latents.split(1,dim=2)
        #         kld = _KLD(mu,logvar).mean() if self.batchmean else _KLD(mu,logvar).sum()
        #         self.vals.append(self.alpha*kld)


class MuMetric(MyMetric):
    "average latent value (e.g. avg(`mu`)"
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        latents = to_detach(learn.pred[1])
        mu, logvar = latents.split(1,dim=2)
        self.vals.append(mu.mean())


class MuSDMetric(MyMetric):
    "standard deviation of latent ð value (e.g. std(`mu`) )"
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        latents = to_detach(learn.pred[1])
        mu, logvar = latents.split(1,dim=2)
        self.vals.append(mu.std())


class StdMetric(MyMetric):
    "average of latent ðˆ value (e.g. std(exp(.5*`logvar`) )"
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        latents = learn.pred[1]
        mu, logvar = latents.split(1,dim=2)
        std = torch.exp(0.5 * logvar).mean()
        self.vals.append(to_detach(std))

class StdSDMetric(MyMetric):
    "standard deviation of latent ðˆ value (e.g. std(exp(.5*`logvar`) )"
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        latents = learn.pred[1]
        mu, logvar = latents.split(1,dim=2)
        std = torch.exp(0.5 * logvar).std()
        self.vals.append(to_detach(std))

class LogvarMetric(MyMetric):
    "average of latent log(ðˆ*ðˆ) value (e.g. mean(`logvar`))"
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        latents = to_detach(learn.pred[1])
        mu, logvar = latents.split(1,dim=2)
        self.vals.append(logvar.mean())

class LogvarSDMetric(MyMetric):
    "standard deviation of latent log(ðˆ*ðˆ)  value (e.g. std(`logvar`)"
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        latents = to_detach(learn.pred[1])
        mu, logvar = latents.split(1,dim=2)
        self.vals.append(logvar.std())





# Cell

def default_AE_metrics(alpha,batchmean,useL1):
    "long-ish default list of metrics for the AE"

    first = L2BMeanMetric() if batchmean else L2MeanMetric()
    second = L1BMeanMetric() if batchmean else L2MeanMetric()

    if useL1: first,second = second,first

    metrics = [first,
                L1LatentReg(batchmean=batchmean,alpha=alpha),
                MuMetric(),
                StdMetric(),
                LogvarMetric(),
                second,
                WeightedKLDMetric(batchmean=batchmean,alpha=alpha),
                MuSDMetric(),
                LogvarSDMetric(),
               ]
    return metrics

def short_AE_metrics(alpha,batchmean,useL1):
    "short default list of metrics for the AE"

    first = L2BMeanMetric() if batchmean else L2MeanMetric()
    second = L1BMeanMetric() if batchmean else L2MeanMetric()

    if useL1: first,second = second,first

    metrics = [first,
                L1LatentReg(batchmean=batchmean,alpha=alpha),
                MuMetric(),
               ]
    return metrics


# Cell

class AnnealedLossCallback(Callback):
    "injects `kl_weight` for access during loss function calculation"
    def after_pred(self):
        kl_weight = self.learn.pred[0].new(1)
        kl_weight[0] = self.opt.hypers[0]['kl_weight'] if 'kl_weight' in self.opt.hypers[0].keys() else 1.0
        self.learn.pred = self.learn.pred + (kl_weight,)
    def after_batch(self):
        pred, latents, _ = self.learn.pred
        self.learn.pred = (pred,latents)


def default_KL_anneal_in():
    "reasonable default for 'warming up' the KL Div"
    return combine_scheds([ .7, .3], [SchedCos(0,1), SchedNo(1,1)])

# Cell

def bn_splitter(m):
    "splits all the batchnorm layers out"
    def _bn_splitter(l, g1, g2):
        if isinstance(l, nn.BatchNorm2d): g2 += l.parameters()
        elif hasattr(l, 'weight'): g1 += l.parameters()
        for ll in l.children(): _bn_splitter(ll, g1, g2)

    g1,g2 = [],[]
    _bn_splitter(m[0], g1, g2)

    g2 += m[1:].parameters()
    return g1,g2

def resnetVAE_split(m):
    "simple splitter to freeze the non batch norm pre-trained encoder"
    to_freeze, dont_freeze = bn_splitter(m.encoder)
    #return L(to_freeze, dont_freeze + params(m.bn)+params(m.dec[:2]), params(m.dec[2:]))
    return L(to_freeze, dont_freeze + params(m.bn)+params(m.decoder))
    #return L(fz, nofz + params(m.bn)+params(m.dec[:6]), params(m.dec[6:]))



def AE_split(m):
    "generic splitter for my AE classes- BVAE & AE & MMDVAE."
    to_freeze, dont_freeze = bn_splitter(m.encoder)
    return L(to_freeze, dont_freeze + params(m.bn)+params(m.decoder))





# Cell

#### TODO: refactor this to take a "BLOCK" input so we can have either ConvLayer or ResBlock pieces
def get_conv_parts(im_size=IMG_SIZE):
    """
    make a simple convolutional ladder encoder
    """
    n_blocks = 5
    BASE = im_size//2**5
    nfs = [3]+[(2**i)*BASE for i in range(n_blocks)]
    n = len(nfs)

    modules =  [ConvLayer(nfs[i],nfs[i+1],
                            ks=5,stride=2,padding=2) for i in range(n - 1)]

    return modules,nfs[-1],'vanilla'



def get_pretrained_parts(arch=resnet18):
    "this works for mobilnet_v2, resnet, and xresnet"
    cut = model_meta[arch]['cut']
    name = arch.__name__
    arch = arch(pretrained=True)
    enc_arch = list(arch.children())[:cut]
    enc_feats = 512
    return enc_arch, enc_feats, name



def get_encoder_parts(enc_type='vanilla',im_size=IMG_SIZE):
    encoder_parts = get_conv_parts(im_size=im_size) if isinstance(enc_type,str) else get_pretrained_parts(arch=enc_type)
    return encoder_parts # returns enc_arch,enc_dim,arch.__name__



# Cell

class VAELinear(Module):
    "maps hidden (input) features to two latents (mu and logvar)"
    def __init__(self,in_features,latent_features):
        self.mu_linear = nn.Linear(in_features,latent_features)
        self.logvar_linear = nn.Linear(in_features,latent_features)

    def forward(self,h):
        #h = self.fc_in(h)
        return self.mu_linear(h), self.logvar_linear(h)



class VAELayer(Module):
    """
    The VAE : in_features to latent_features through
        the "Variational" magic: "reparamaterization trick"
    """
    def __init__(self,in_features,latent_features):
        self.mu_logvar = VAELinear(in_features,latent_features)

    #
    def reparam(self,mu,logvar):
        # should we pass through a deterministic code when not training?
        if False: return mu # self.training

        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        return z


    def forward(self,h):
        mu,logvar = self.mu_logvar(h)
        #logvar = F.softplus(logvar)   # force logvar>0
        z = self.reparam(mu,logvar) # adds the noise by the reparam trick

        return z, mu, logvar



# Cell
### TODO:  refactor the BVAE and AE to a single architecture... with a "sample" function ot

class BVAE(AE):
    """
    simple VAE made with an encoder passed in, and some builder function for the Latent (VAE reparam trick) and decoder
    """
    def __init__(self,enc_parts,hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):

        """
        inputs:
            enc_arch (pre-cut / pretrained)
            enc_dim
            latent_dim
            hidden_dim
            im_size,out_range
        """
        enc_arch,enc_feats,name = enc_parts

        # encoder
        #  arch,cut = xresnet18(pretrained=True),-4
        #  enc_arch = list(arch.children())[:cut]

        BASE = im_size//2**5
        enc_dim = enc_feats * BASE**2  # 2**(3*3) * (im_size//32)**2 #(output of resneet) #12800

        self.encoder = build_AE_encoder(enc_arch,enc_dim=enc_dim, hidden_dim=hidden_dim, im_size=im_size)

        in_dim = enc_dim if hidden_dim is None else hidden_dim

        # VAE Bottleneck
        self.bn = VAELayer(in_dim,latent_dim)

        #decoder
        self.decoder = build_AE_decoder(hidden_dim=hidden_dim, latent_dim=latent_dim, im_size=im_size,out_range=out_range)

        store_attr('name,enc_dim, in_dim,hidden_dim,latent_dim,im_size,out_range') # do i need all these?


#     def decode(self, z):
#         return self.decoder(z)

#     def encode(self, x):
#         h = self.encoder(x)
#         z, mu, logvar = self.bn(h) # reparam happens in the VAE layer
#         return z, mu, logvar

#     def forward(self, x):
#         #z, mu, logvar = self.encode(x)
#         #         h = self.encoder(x)
#         #         z, mu, logvar = self.bn(h) # reparam happens in the VAE layer
#         #         x_hat = self.decoder(z)

#         z,mu,logvar = self.encode(x)
#         x_hat = self.decode(z)
#         latents = torch.stack([mu,logvar],dim=-1)

#         return x_hat, latents # assume dims are [batch,latent_dim,concat_dim]


# # AE
#    def decode(self, z):
#         return self.decoder(z)

#     def encode(self, x):
#         h = self.encoder(x)
#         return self.bn(h)

#     def forward(self, x):
#         """
#         pass the "latents" out to keep the learn mechanics consistent...
#         """
#         h = self.encoder(x)
#         z,logvar = self.bn(h)
#         x_hat = self.decoder(z)
#         latents = torch.stack([z,logvar] ,dim=-1)

#         return x_hat , latents



# Cell

# called `after_batch`

class BVAELoss(Module):
    """
    Measures how well we have created the original image,
    plus the KL Divergence with the unit normal distribution
    batchmean option sums first and averages over batches (for smaller total error magnitudes.. cosmentic)
    """
    def __init__(self, batchmean=False, alpha=1.0,useL1=False):
        """
        reduction 'sum', else 'batchmean'

        """
        pix_loss = MSELossFlat(reduction='sum') if not useL1 else L1LossFlat(reduction='sum')
        _KLD = KLDiv(batchmean=False) # force to full sum
        store_attr('pix_loss,alpha,batchmean,_KLD')


    def forward(self, preds, *target):
        """
        pred =(x_hat,KLD,kl_weight) #mu,log_var, kl_weight)
        target is x (original)
        """

        # this handles the annealed kl_weight and passing the mu,logvar around we added...
        if(len(preds) == 3):
            x_hat, latents, kl_weight = preds
        else: #if len(preds) == 2:  # we should never get here... unless we delete the callback
            x_hat, latents = preds

            kl_weight = x_hat[0].new(1)
            kl_weight[0] = 1.0

        mu, logvar = latents.split(1,dim=2)

        #note: both mse and KLD are summing errors over batches, and pixels or latents
        pix_err = self.pix_loss(x_hat, target[0])
        kld_err = self.alpha * self._KLD(mu,logvar).sum() #_KLD doesn't sum over batches by default
        total =  (pix_err + kld_err*kl_weight)
        if self.batchmean: total *= (1./mu.size()[0])
        return total




# Cell

def default_VAE_metrics(alpha,batchmean,useL1):
    "long default list of metrics for the VAE"

    first = L2BMeanMetric() if batchmean else L2Metric()
    second = L1BMeanMetric() if batchmean else L1Metric()

    if useL1: first,second = second,first

    metrics = [first,
                MuMetric(),
                StdMetric(),
                LogvarMetric(),
                WeightedKLDMetric(batchmean=batchmean,alpha=alpha),
                KLWeightMetric(),
                second,
                MuSDMetric(),
                StdSDMetric(),
                LogvarSDMetric(),
               ]
    return metrics



def short_VAE_metrics(alpha,batchmean,useL1):
    "short default list of metrics for the AE"

    first = L2BMeanMetric() if batchmean else L2MeanMetric()
    second = L1BMeanMetric() if batchmean else L2MeanMetric()

    if useL1: first,second = second,first

    metrics = [first,
                MuMetric(),
                StdMetric(),
                LogvarMetric(),
                WeightedKLDMetric(batchmean=batchmean,alpha=alpha)
               ]
    return metrics




# Cell

def gaussian_kernel(a, b):
    "helper for computing MMD"
    dim1_1, dim1_2 = a.shape[0], b.shape[0]
    depth = a.shape[1]
    a = a.view(dim1_1, 1, depth)
    b = b.view(1, dim1_2, depth)
    a_core = a.expand(dim1_1, dim1_2, depth)
    b_core = b.expand(dim1_1, dim1_2, depth)
    numerator = (a_core - b_core).pow(2).mean(2)/depth
    return torch.exp(-numerator)

def MMD(a, b):
    "Max Mean Discrepancy"
    return gaussian_kernel(a, a).mean() + gaussian_kernel(b, b).mean() - 2*gaussian_kernel(a, b).mean()

def rawMMD(a, b):
    "_raw_ values from gauss kernals, assuming that and b have the same shape"
    return gaussian_kernel(a, a) + gaussian_kernel(b, b) - 2*gaussian_kernel(a, b)



# the MMDVAE is built on the basic AE archiecure
class MMDVAE(AE): pass


class MaxMeanDiscrepancy(Module):
    """
     MMD
     add alpha?
    """
    def __init__(self, batchmean=False):
        """
        reduction 'mean', else 'batchmean' means only over batch

        """
        MMD = self._MMDsum if batchmean else self._MMDmean
        store_attr('batchmean,MMD')

    def _gaus_ker(self,a, b):
        "gaussian kernal"
        dim1_1, dim1_2 = a.shape[0], b.shape[0]
        depth = a.shape[1]
        numerator = 1.0/depth
        a = a.view(dim1_1, 1, depth)
        b = b.view(1, dim1_2, depth)
        a_core = a.expand(dim1_1, dim1_2, depth)
        b_core = b.expand(dim1_1, dim1_2, depth)
        a_m_b = a_core - b_core
        numerator *= (a_m_b*a_m_b).mean(2)
        #numerator = (a_core - b_core).pow(2).mean(2)   /depth
        return torch.exp(-numerator)

    def _rawMMD(self, a, b):
        return self._gaus_ker(a, a) +  self._gaus_ker(b, b) - 2*self._gaus_ker(a, b)

    def _MMDmean(self, a, b):
        return self._rawMMD( a, b).mean()

    def _MMDsum(self, a, b):
        return self._rawMMD( a, b).sum()


    def forward(self,true_samples, latent):
#         bs = latents.size()[0]
#         latent_dim = z.size()[1]
#         true_samples = torch.randn((bs,latent_dim), requires_grad=False).cuda()
        mmd = self.MMD(true_samples, latent)
        return mmd


class MMDLoss(Module):
    """
    Measures mean square error of prediction and original image,
    regularized by MMD.

    Note: using reuction = 'mean' because it keeps the regularization relatively potent (i.e. pixels>>latents)
    """
    def __init__(self, batchmean=False, alpha=1.0,useL1=False):
        """
        reduction 'sum', else 'batchmean'

        """
        if batchmean:
            pix_loss = MSELossFlat(reduction='sum') if not useL1 else L1LossFlat(reduction='sum')
            #mmd = _MMDsum
        else:
            pix_loss = MSELossFlat(reduction='mean') if not useL1 else L1LossFlat(reduction='mean')
            #mmd = _MMD

        mmd = MaxMeanDiscrepancy(batchmean=batchmean)
        store_attr('pix_loss,alpha,batchmean,mmd')


    def forward(self, preds, *target):
        """
        pred =(x_hat,KLD,kl_weight) #mu,log_var, kl_weight)
        target is x (original)
        """

        # this handles the annealed kl_weight and passing the mu,logvar around we added...
        if(len(preds) == 3):
            x_hat, latents, kl_weight = preds
        else: #if len(preds) == 2:  # we should never get here... unless we delete teh callback
            x_hat, latents = preds

            kl_weight = x_hat[0].new(1)
            kl_weight[0] = 1.0

        z, _ = latents.split(1,dim=2)

        #note: both mse and KLD are summing errors over batches, and pixels or latents
        pix_err = self.pix_loss(x_hat, target[0])


        bs = latents.size()[0]
        latent_dim = z.size()[1]
        true_samples = torch.randn((bs,latent_dim), requires_grad=False).cuda()
        mmd_loss = self.mmd(true_samples, z) * self.alpha

        total =  (pix_err + mmd_loss*kl_weight)
        total *= (1./bs) if self.batchmean else 1.0
        return total




class MMDMetric(MyMetric):
    def __init__(self,batchmean=False,alpha=1.0):
        vals = []
        #mmd = _MMDsum if batchmean else _MMD
        mmd = MaxMeanDiscrepancy(batchmean=batchmean)

        store_attr('vals,batchmean,alpha,mmd')

    def accumulate(self, learn):
        latents = learn.pred[1]
        z, _ = latents.split(1,dim=2)

        bs = latents.size()[0]
        latent_dim = z.size()[1]
        true_samples = torch.randn((bs,latent_dim), requires_grad=False).cuda()
        mmd_loss = self.mmd(true_samples, z)
        mmd_loss *= (self.alpha/bs) if self.batchmean else self.alpha

        self.vals.append(to_detach(mmd_loss))

# export

def short_MMEVAE_metrics(alpha,batchmean,useL1):
    "short list of metrics for the VAE"

    first = L2BMeanMetric() if batchmean else L2MeanMetric()
    second = L1BMeanMetric() if batchmean else L1MeanMetric()

    if useL1: first,second = second,first

    metrics = [first,
                MMDMetric(batchmean=batchmean,alpha=alpha),
                MuMetric(),
                MuSDMetric(),
                ]
    return metrics

def default_MMEVAE_metrics(alpha,batchmean,useL1):
    "long default list of metrics for the VAE"

    first = L2BMeanMetric() if batchmean else L2MeanMetric()
    second = L1BMeanMetric() if batchmean else L1MeanMetric()

    if useL1: first,second = second,first

    metrics = [first,
                MMDMetric(batchmean=batchmean,alpha=alpha),
                MuMetric(),
                StdMetric(),
                second,
                MuSDMetric(),
                LogvarMetric(),
                L1LatentReg(batchmean=batchmean,alpha=alpha),
                WeightedKLDMetric(batchmean=batchmean,alpha=alpha),
                LogvarSDMetric()]
    return metrics




# Cell
class UpsampleResBlock(Module):
    def __init__(self, up_in_c:int, final_div:bool=True, blur:bool=False, **kwargs):
        """
        Upsampling using PixelShuffle_INCR and ResBlocks
        - up_in_c :  "Upsample input channel"
        """
        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)
        ni = up_in_c//2
        nf = ni if final_div else ni//2
        self.conv1 = ResBlock(1,ni, nf, **kwargs) # since we'll apply it by hand...
        self.conv2 = ResBlock(1,nf, nf, **kwargs)

    def forward(self, up_in:Tensor) -> Tensor:
        up_out = self.shuf(up_in)
        return self.conv2(self.conv1(up_out))



def get_resblockencoder_parts(enc_type='vanilla',im_size=IMG_SIZE):
    """
    make a simple (hence 'vanilla') convolutional ladder encoder with ResBlock parts
    """
    n_blocks = 5
    BASE = im_size//2**5
    nfs = [3]+[(2**i)*BASE for i in range(n_blocks)]
    n = len(nfs)


    modules =  [ResBlock(1, nfs[i],nfs[i+1],
                          stride=2, act_cls=Mish)  for i in range(n - 1)]

    return modules,nfs[-1],'resblock'



# def build_ResBlockAE_decoder(hidden_dim=2048, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):
#         BASE = im_size//2**5
#         #store_attr('enc_dim,latent_dim, hidden_dim,im_size')



#         #decoder
#         n_blocks = 5
#         nfs = [3] + [2**i*n_blocks for i in range(n_blocks+1)]
#         nfs.reverse()
#         n = len(nfs)

#         modules =  [UpsampleResBlock(nfs[i]) for i in range(n - 2)]
#         decoder = nn.Sequential( LinBnDrop(latent_dim,hidden_dim,
#                                                 bn=True,# batch normalizaiton shouldn't be a problem here
#                                                 p=0.0,act=nn.ReLU(),lin_first=True),
#                                      LinBnDrop(hidden_dim,im_size*n_blocks*n_blocks,
#                                                 bn=True,# batch normalizaiton shouldn't be a problem here
#                                                 p=0.0,act=nn.ReLU(),lin_first=True),
#                                       ResizeBatch(im_size,n_blocks,n_blocks),
#                                       *modules,
#                                       ResBlock(1,nfs[-2],nfs[-1],
#                                                 ks=1,padding=0, norm_type=None, #act_cls=nn.Sigmoid) )
#                                                 act_cls=partial(SigmoidRange, *out_range)))

#         return decoder


class ResBlockAEDecoder(Module):
    def __init__(self, hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):
        """
        Decoder Module made of ResBlocks returning the latent representation back into an "image"
            latent_dim - dimension of latent representation
            hidden_dim - optional additional linear layer between the latent and decoder
            im_size - passed to make sure we are scaling back to the right size
            out_range - ensures the output is on teh same scale as the _normalized_ input image
        """
        #decoder
        n_blocks = 5
        BASE = im_size//2**5

        hidden = im_size*BASE*BASE if hidden_dim is None else hidden_dim
        z_fc = [nn.Linear(latent_dim,hidden)]
        if hidden_dim:  # i.e. is not None
            z_fc += [nn.Linear(hidden,im_size*BASE*BASE)]

        nfs = [3] + [2**i*BASE for i in range(n_blocks+1)]
        nfs.reverse()
        n = len(nfs)

        modules =  [UpsampleResBlock(nfs[i]) for i in range(n - 2)]
        self.decoder = nn.Sequential(*z_fc,
                                      ResizeBatch(im_size,BASE,BASE),
                                      *modules,
                                      ResBlock(1,nfs[-2],nfs[-1],
                                                ks=1,padding=0, norm_type=None, #act_cls=nn.Sigmoid) )
                                                act_cls=partial(SigmoidRange, *out_range)))

        store_attr('latent_dim, hidden_dim,im_size,out_range')

    def forward(self, z):
        z = self.decoder(z)
        return z


def build_ResBlockAE_decoder(hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):
    "wrapper to sequential-ize ResBlockAEDecoder class"
    decoder = ResBlockAEDecoder(hidden_dim=hidden_dim, latent_dim=latent_dim, im_size=im_size,out_range=out_range)
    return nn.Sequential(*list(decoder.children()))



class ResBlockAE(AE):
    def __init__(self,enc_parts,hidden_dim=None, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE,isVAE=False):

        """
        inputs:
            enc_parts - encoder architecture
            latent_dim - dimension of latent representation
            hidden_dim - optional additional linear layer between the latent and decoder
            im_size - passed to make sure we are scaling back to the right size
            out_range - ensures the output is on teh same scale as the _normalized_ input image
            isVae - switch for the type of latent representation

        """
        enc_arch,enc_feats,name = enc_parts

        BASE = im_size//2**5
        enc_dim = enc_feats * BASE**2  # 2**(3*3) * (im_size//32)**2 #(output of resneet) #12800

        #encoder
        self.encoder = build_AE_encoder(enc_arch,enc_dim=enc_dim, hidden_dim=hidden_dim, im_size=im_size)

        in_dim = enc_dim if hidden_dim is None else hidden_dim

        # AE Bottleneck
        latent = VAELayer if isVAE else LatentLayer

        self.bn = latent(in_dim,latent_dim)

        #decoder
        self.decoder = build_ResBlockAE_decoder(hidden_dim=hidden_dim, latent_dim=latent_dim, im_size=im_size,out_range=out_range)

        store_attr('name,enc_dim, in_dim,hidden_dim,latent_dim,im_size,out_range') # do i need all these?




#     def decode(self, z):
#         return self.decoder(z)

#     def encode(self, x):
#         h = self.encoder(x)
#         return self.bn(h)

#     def forward(self, x):
#         """
#         pass the "latents" out to keep the learn mechanics consistent...
#         """
#         h = self.encoder(x)
#         z,logvar = self.bn(h)
#         x_reconst = self.decoder(z)
#         latents = torch.stack([z,logvar] ,dim=-1)

#         return x_reconst , latents
