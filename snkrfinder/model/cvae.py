# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02c_model.cvae.ipynb (unless otherwise specified).

__all__ = ['prep_df_for_datablocks', 'df_get_x', 'df_get_y', 'get_ae_btfms', 'get_ae_no_aug', 'LatentTuple',
           'TensorPoint', 'Tensor2Vect', 'LatentsTensor', 'LatentTupleBlock', 'LatentsTensorBlock', 'df_ae_x',
           'df_ae_y', 'get_ae_DataBlock', 'VAELinear', 'VAELayer', 'VAEBottleneck', 'UpsampleBlock', 'AEEncoder',
           'AEDecoder', 'build_AE_encoder', 'build_AE_decoder', 'VAEtrans', 'BVAELoss', 'MyMetric', 'MSEMetric',
           'RawKLDMetric', 'BWeightedKLDMetric', 'MuMetric', 'MuSDMetric', 'StdMetric', 'StdSDMetric', 'KLWeightMetric',
           'AnnealedLossCallback', 'default_KL_anneal_in', 'bn_splitter', 'resnetVAE_split', 'VAEtrans_split',
           'build_conv_encoder', 'AEtrans', 'AELoss', 'gaussian_kernel', 'MMD', 'MMELoss', 'RawKLDMetric', 'L2Metric',
           'MMDMetric']

# Cell
from ..imports import *
from ..core import *
from ..data.munge import *
from .core import *
#from snkrfinder.model.transfer import *

from fastai.test_utils import show_install, synth_learner, nvidia_smi, nvidia_mem

# Cell

def prep_df_for_datablocks(df):
    df = df[["path","train","test","validate","t_t_v"]].copy()
    # I could remove all the "test" rows... for now i'll choose an alternate strategy:
    # Drop all the "test" rows for now, and create an "is_valid" column...
    # should probably drop a ton of columns to jus tkeep the file paths...
    # just keep what we'll need below
    df.loc[:,'is_valid'] = df.test | df.validate
    return df



# Cell

# some helper functions borrowed from validating the feature embedding
def df_get_x(r):
    return image_path/r['path']

def df_get_y(r):
    # we want to return a tuple so that we predict latent variables...
    return (df_get_x(r),None,None)

# Cell
def get_ae_btfms():
    batch_tfms = Normalize.from_stats(*imagenet_stats)
    rand_tfms = aug_transforms(mult=1.0,
               do_flip=True,
               flip_vert=False,
               max_rotate=5.0,
               min_zoom=.95,
               max_zoom=1.0,
               max_lighting=0.1,
               max_warp=0.1,
               p_affine=0.66,
               p_lighting=0.2,
               xtra_tfms=None,
               size=None,
               mode='bilinear',
               pad_mode='border',
               align_corners=True,
               batch=False,
               min_scale=1.0)
    return rand_tfms+[batch_tfms]

def get_ae_no_aug():
    batch_tfms = Normalize.from_stats(*imagenet_stats)
    return [batch_tfms]

# Cell

class LatentTuple(fastuple):
    "Basic type for tuple of tensor (vectors)"
    _show_args = dict(s=10, marker='.', c='r')
    @classmethod
    def create(cls, ts):
        if isinstance(ts,tuple):
            mu,logvar = ts
        elif ts is None:
            mu,logvar = None,None
        else:
            mu = None
            logvar = None

        if mu is None: mu = torch.empty(0)
        elif not isinstance(mu, Tensor): Tensor(mu)

        if logvar is None: logvar = torch.empty(0)
        elif not isinstance(logvar,Tensor): Tensor(logvar)

        return cls( (mu,logvar) )

    def show(self, ctx=None, **kwargs):
        mu,logvar = self
        if not isinstance(mu, Tensor) or not isinstance(logvar,Tensor): return ctx

        title_str = f"mu-> {mu.mean():e}, {mu.std():e}  logvar->{logvar.mean():e}, {logvar.std():e}"

        if 'figsize' in kwargs: del kwargs['figsize']
        if 'title' in kwargs: kwargs['title']=title_str
        if ctx is None:
            _,axs = plt.subplots(1,2, figsize=(12,6))
            x=torch.linspace(0,1,mu[0].shape[0])
            axs[0].scatter(x, mu[:], **{**self._show_args, **kwargs})
            axs[1].scatter(x, logvar[:], **{**self._show_args, **kwargs})
            ctx = axs[1]

        ctx.scatter(mu[:], logvar[:], **{**self._show_args, **kwargs})
        return ctx



class TensorPoint(TensorBase):
    "Basic type for points in an image"
    _show_args = dict(s=10, marker='.', c='r')

    @classmethod
    def create(cls, t, img_size=None)->None:
        "Convert an array or a list of points `t` to a `Tensor`"
        return cls(tensor(t).view(-1, 2).float(), img_size=img_size)

    def show(self, ctx=None, **kwargs):
        if 'figsize' in kwargs: del kwargs['figsize']
        x = self.view(-1,2)
        ctx.scatter(x[:, 0], x[:, 1], **{**self._show_args, **kwargs})
        return ctx


class Tensor2Vect(TensorPoint): pass

class LatentsTensor(Tensor2Vect):
    "Basic type for latents as Tensor inheriting from TensorPoint (vectors)"
    @classmethod
    def create(cls, ts, img_size=IMG_SIZE):
        "create IMG_SIZE attr to register plotting..."

        if isinstance(ts,tuple):
            mu,logvar = ts
        elif ts is None:
            mu,logvar = None,None
        else:
            mu = None
            logvar = None
        if mu is None: mu = torch.empty(0)
        elif not isinstance(mu, Tensor): Tensor(mu)

        if logvar is None: logvar = torch.empty(0)
        elif not isinstance(logvar,Tensor): Tensor(logvar)

        t = torch.cat([mu,logvar],dim=-1) # in case its a batch?

        return cls(tensor(t).view(-1, 2).float(), img_size=img_size)


### TODO:  fixe the .show() method to make barcode style images appended to sneaker pic
#     def show(self, ctx=None, **kwargs):
#         if 'figsize' in kwargs: del kwargs['figsize']
#         x = self.view(-1,2)
#         ctx.scatter(x[:, 0], x[:, 1], **{**self._show_args, **kwargs})
#         return ctx
#         mu,logvar = self
#         if not isinstance(mu, Tensor) or not isinstance(logvar,Tensor): return ctx

#         title_str = f"mu-> {mu.mean():e}, {mu.std():e}  logvar->{logvar.mean():e}, {logvar.std():e}"

#         if 'figsize' in kwargs: del kwargs['figsize']
#         if 'title' in kwargs: kwargs['title']=title_str
#         if ctx is None:
#             _,axs = plt.subplots(1,2, figsize=(12,6))
#             x=torch.linspace(0,1,mu[0].shape[0])
#             axs[0].scatter(x, mu[:], **{**self._show_args, **kwargs})
#             axs[1].scatter(x, logvar[:], **{**self._show_args, **kwargs})
#             ctx = axs[1]

#         ctx.scatter(mu[:], logvar[:], **{**self._show_args, **kwargs})
#         return ctx


# TODO:  create Rank 1 tensor class with a show that either makes a bar-code to append to the image, or plots some latents kdes histograms

# Cell
# could we do a typedispatch to manage the transforms...?
# def VAETargetTupleBlock():
#     return TransformBlock(type_tfms=VAETargetTuple.create, batch_tfms=IntToFloatTensor)

def LatentTupleBlock():
    return TransformBlock(type_tfms=LatentTuple.create, batch_tfms=noop)


def LatentsTensorBlock():
    return TransformBlock(type_tfms=LatentsTensor.create, batch_tfms=noop)


def df_ae_x(r):
    return image_path/r['path']


# need to make sure that we get the image whihc is "Identical" to the input.. how to test?
# lambda o: o
def df_ae_y(r):
    # we want to return a tuple so that we predict latent variables...
    return df_get_x(r)


# Cell
#

def get_ae_DataBlock(aug=True):
    "wrapper to get the standard ae datablock"
    mytfms = get_ae_btfms() if aug else get_ae_no_aug()
    block = DataBlock(blocks=(ImageBlock(cls=PILImage), ImageBlock(cls=PILImage), LatentsTensorBlock ),
              get_x=df_ae_x,
              get_y=[df_ae_y, noop], #don't need to get the LatentsTensorBlock, just create
              splitter=ColSplitter('is_valid'),
              item_tfms= Resize(IMG_SIZE,method='pad', pad_mode='border'),
              batch_tfms = mytfms,
              n_inp = 1)

    return block



# Cell

class VAELinear(Module):
    def __init__(self,in_features,latent_features):
        self.mu_linear = nn.Linear(in_features,latent_features)
        self.logvar_linear = nn.Linear(in_features,latent_features)

    def forward(self,h):
        #h = self.fc_in(h)
        return self.mu_linear(h), self.logvar_linear(h)



class VAELayer(Module):
    """
    The VAE : in_features to latent_features through
        the "Variational" magic: "reparamaterization trick"
    """
    def __init__(self,in_features,latent_features):
        self.mu_logvar = VAELinear(in_features,latent_features)

    #
    def reparam(self,mu,logvar):
        # should we pass through a deterministic code when not training?
        if False: return mu # self.training

        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        return z


    def forward(self,h):
        mu,logvar = self.mu_logvar(h)
        #logvar = F.softplus(logvar)   # force logvar>0
        z = self.reparam(mu,logvar) # adds the noise by the reparam trick

        return z, mu, logvar


class VAEBottleneck(Module):
    """
    BOTTLENECK:  a fc/linear step down to hidden_dim before the latent_dim VAE
    """
    def __init__(self,input_dim,hiden_dim,latent_dim,bn=False,drop_p=0.0,act=nn.ReLU()):
        self.bn = nn.Sequential(LinBnDrop(input_dim,hidden_dim,bn=bn),
                        VAELayer(in_features=hidden_dim, out_features=latent_dim))

    def forward(self,h):
        # maybe assert that the shape is bs,encoder_features?
        z,mu,logvar = self.bn(h)
        return z, mu, logvar

# Cell
class UpsampleBlock(Module):
    def __init__(self, up_in_c:int, final_div:bool=True, blur:bool=False, leaky:float=None, **kwargs):
        """
        up_in_c :  "Upsample input channel"
        """
        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)
        ni = up_in_c//2
        nf = ni if final_div else ni//2
        self.conv1 = ConvLayer(ni, nf, **kwargs) # since we'llapply it by hand...
        self.conv2 = ConvLayer(nf, nf, **kwargs)

    def forward(self, up_in:Tensor) -> Tensor:
        up_out = self.shuf(up_in)
        return self.conv2(self.conv1(up_out))

    #     def shuff(self,up_in:Tensor) -> Tensor:
    #         up_out = self.shuf(up_in)
    #         cat_x = self.relu(up_out)
    #         return cat_x


# Cell

class AEEncoder(Module):
    def __init__(self,arch_body,enc_dim=512, hidden_dim=2048, im_size=IMG_SIZE):
        """
        arch_body   list of layers (e.g. arch.children()[:cut])
        enc_dim,
        hidden_dim
        """
        #store_attr('enc_dim,latent_dim, hidden_dim,im_size')

        BASE = im_size//2**5

        # encoder
        self.in_dim = enc_dim * BASE**2  # 2**(3*3) * (im_size//32)**2 #(output of resneet) #12800

        self.encoder = nn.Sequential(*arch_body,
                                 Flatten(),
                                 LinBnDrop(self.in_dim,hidden_dim,bn=True,p=0.0,act=nn.ReLU(),lin_first=True)
                                )

    def forward(self, x):
        return self.encoder(x)



class AEDecoder(Module):
    def __init__(self, hidden_dim=2048, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):
        """

        latent_dim,
        hidden_dim,
        im_size,
        out_range
        """
        BASE = im_size//2**5
        #store_attr('enc_dim,latent_dim, hidden_dim,im_size')



        #decoder
        n_blocks = 5
        nfs = [3] + [2**i*n_blocks for i in range(n_blocks+1)]
        nfs.reverse()
        n = len(nfs)

        modules =  [UpsampleBlock(nfs[i]) for i in range(n - 2)]
        self.decoder = nn.Sequential( LinBnDrop(latent_dim,hidden_dim,
                                                bn=True,# batch normalizaiton shouldn't be a problem here
                                                p=0.0,act=nn.ReLU(),lin_first=True),
                                     LinBnDrop(hidden_dim,im_size*n_blocks*n_blocks,
                                                bn=True,# batch normalizaiton shouldn't be a problem here
                                                p=0.0,act=nn.ReLU(),lin_first=True),
                                      ResizeBatch(im_size,n_blocks,n_blocks),
                                      *modules,
                                      ConvLayer(nfs[-2],nfs[-1],
                                                ks=1,padding=0, norm_type=None, #act_cls=nn.Sigmoid) )
                                                act_cls=partial(SigmoidRange, *out_range)))



    def forward(self, z):
        z = self.decoder(z)
        return z


# Cell


def build_AE_encoder(arch_body,enc_dim=512, hidden_dim=2048, im_size=IMG_SIZE):
        """
        arch_body   list of layers (e.g. arch.children()[:cut])
        enc_dim,
        hidden_dim
        """
        BASE = im_size//2**5
        # encoder
        in_dim = enc_dim * BASE**2  # 2**(3*3) * (im_size//32)**2 #(output of resneet) #12800

        encoder = nn.Sequential(*arch_body,
                                 Flatten(),
                                 LinBnDrop(in_dim,hidden_dim,bn=True,p=0.0,act=nn.ReLU(),lin_first=True)
                                )

        return encoder

def build_AE_decoder(hidden_dim=2048, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):
        BASE = im_size//2**5
        #store_attr('enc_dim,latent_dim, hidden_dim,im_size')



        #decoder
        n_blocks = 5
        nfs = [3] + [2**i*n_blocks for i in range(n_blocks+1)]
        nfs.reverse()
        n = len(nfs)

        modules =  [UpsampleBlock(nfs[i]) for i in range(n - 2)]
        decoder = nn.Sequential( LinBnDrop(latent_dim,hidden_dim,
                                                bn=True,# batch normalizaiton shouldn't be a problem here
                                                p=0.0,act=nn.ReLU(),lin_first=True),
                                     LinBnDrop(hidden_dim,im_size*n_blocks*n_blocks,
                                                bn=True,# batch normalizaiton shouldn't be a problem here
                                                p=0.0,act=nn.ReLU(),lin_first=True),
                                      ResizeBatch(im_size,n_blocks,n_blocks),
                                      *modules,
                                      ConvLayer(nfs[-2],nfs[-1],
                                                ks=1,padding=0, norm_type=None, #act_cls=nn.Sigmoid) )
                                                act_cls=partial(SigmoidRange, *out_range)))

        return decoder



class VAEtrans(Module):
    def __init__(self,enc_arch,enc_dim=512,hidden_dim=2048, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):

        """
        inputs:
            arch, cut,pretrained
            enc_dim
            latent_dim
            hidden_dim

        """
        store_attr('enc_dim, hidden_dim,latent_dim,im_size')

        # encoder
        #  arch,cut = xresnet18(pretrained=True),-4
        #  enc_arch = list(arch.children())[:cut]

        self.encoder = build_AE_encoder(enc_arch,enc_dim=enc_dim, hidden_dim=hidden_dim, im_size=im_size)


        # VAE Bottleneck
        self.bn = VAELayer(self.hidden_dim,self.latent_dim)

        #decoder
        self.decoder = build_AE_decoder(hidden_dim=hidden_dim, latent_dim=latent_dim, im_size=im_size,out_range=out_range)



    def decode(self, z):
        z = self.decoder(z)
        return z

    def reparam(self, h):
        return self.bn(h)

    def encode(self, x):
        h = self.encoder(x)
        z, mu, logvar = self.reparam(h)
        return z, mu, logvar

    def forward(self, x):
        z, mu, logvar = self.encode(x)
        x_hat = self.decode(z)
        latents = torch.stack([mu,logvar],dim=-1)
        return x_hat, latents # assume dims are [batch,latent_dim,concat_dim]



# Cell

# called `after_batch`
class BVAELoss(Module):
    """
    Measures how well we have created the original image,
    plus the KL Divergence with the unit normal distribution
    """
    def __init__(self, b_weight=1.):
        mse = MSELossFlat(reduction='sum')
        store_attr('mse,b_weight')

    def forward(self, preds, *target):
        """
        pred =(x_hat,KLD,kl_weight) #mu,log_var, kl_weight)
        target is x (original)
        """

        # this handles the annealed kl_weight and passing the mu,logvar around we added...
        if(len(preds) == 3):
            x_hat, latents, kl_weight = preds
        else: #if len(preds) == 2:  # we should never get here... unless we delete teh callback
            x_hat, latents = preds

            kl_weight = x_hat[0].new(1)
            kl_weight[0] = 1.0

        #mu,logvar = latents[:,:,0],latents[:,:,1]
        mu, logvar = latents.split(1,dim=2)

        #note: both mse and KLD are summing errors over batches, and pixels or latents
        total = self.mse(x_hat, target[0])

        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        KLD = self.b_weight*KLD.sum()

        return (total + KLD*kl_weight)



# Cell

# test: if i call them preds instead of vals might the Metric base class reset automatically?
class MyMetric(Metric):
    "for simple average over batch quantities"
    def reset(self):
        "Clear all targs and preds"
        self.vals = []
    @property
    def value(self):
        return np.array(self.vals).mean()

class MSEMetric(MyMetric):
    def __init__(self):
        self.vals= []
    def accumulate(self, learn):
        x_hat = learn.pred[0]
        x_targs = learn.y[0]
        self.vals.append(  to_detach( F.mse_loss(x_hat, x_targs, reduction='sum') ))

class RawKLDMetric(MyMetric):
    def __init__(self):
        self.vals = []
    def accumulate(self, learn):
        latents = learn.pred[1]
        mu, logvar = latents.split(1,dim=2)
        KLD =  -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        self.vals.append(to_detach(KLD))

class BWeightedKLDMetric(MyMetric):
    def __init__(self,b_weight=1.):
        self.vals = []
        self.b_weight = b_weight
    def accumulate(self, learn):
        latents = to_detach(learn.pred[1])
        mu, logvar = latents.split(1,dim=2)
        # _,mu,logvar = to_detach(learn.pred)
        KLD = self.b_weight * -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        self.vals.append( KLD )

class MuMetric(MyMetric):
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        latents = to_detach(learn.pred[1])
        mu, logvar = latents.split(1,dim=2)
        self.vals.append(mu.mean())


class MuSDMetric(MyMetric):
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        latents = to_detach(learn.pred[1])
        mu, logvar = latents.split(1,dim=2)
        self.vals.append(mu.std())



class StdMetric(MyMetric):
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        latents = to_detach(learn.pred[1])
        mu, logvar = latents.split(1,dim=2)
        self.vals.append(torch.exp(0.5 * logvar.mean()))

        torch.exp(0.5 * logvar)

class StdSDMetric(MyMetric):
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        latents = to_detach(learn.pred[1])
        mu, logvar = latents.split(1,dim=2)
        self.vals.append(torch.exp(0.5 * logvar.std()))



class KLWeightMetric(MyMetric):
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        #kl = learn.model.kl_weight
        kl = learn.opt.hypers[0]['kl_weight']
        #KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        self.vals.append(kl)


# Cell

class AnnealedLossCallback(Callback):
    def after_pred(self):
        kl = self.learn.pred[0].new(1)
        kl[0] = self.opt.hypers[0]['kl_weight']
        self.learn.pred = self.learn.pred + (kl,)
    def after_batch(self):
        pred, latents, _ = self.learn.pred
        self.learn.pred = (pred,latents)



def default_KL_anneal_in():
    return combine_scheds([.1, .7, .2], [SchedNo(0,0),SchedCos(0,1), SchedNo(1,1)])

# Cell

# note that it is crucial that you don't freeze the batch norm layers...
def bn_splitter(m):
    def _bn_splitter(l, g1, g2):
        if isinstance(l, nn.BatchNorm2d): g2 += l.parameters()
        elif hasattr(l, 'weight'): g1 += l.parameters()
        for ll in l.children(): _bn_splitter(ll, g1, g2)

    g1,g2 = [],[]
    _bn_splitter(m[0], g1, g2)

    g2 += m[1:].parameters()
    return g1,g2

def resnetVAE_split(m):
    to_freeze, dont_freeze = bn_splitter(m.encoder)
    #return L(to_freeze, dont_freeze + params(m.bn)+params(m.dec[:2]), params(m.dec[2:]))
    return L(to_freeze, dont_freeze + params(m.bn)+params(m.decoder))
    #return L(fz, nofz + params(m.bn)+params(m.dec[:6]), params(m.dec[6:]))



def VAEtrans_split(m):
    to_freeze, dont_freeze = bn_splitter(m.encoder)
    return L(to_freeze, dont_freeze + params(m.bn)+params(m.decoder))



# Cell

def build_conv_encoder(im_size=IMG_SIZE):
    """
    make a simple convolutional ladder encoder
    """
    n_blocks = 5
    BASE = im_size//2**5
    nfs = [3]+[(2**i)*BASE for i in range(n_blocks)]
    n = len(nfs)

    modules =  [ConvLayer(nfs[i],nfs[i+1],
                            ks=5,stride=2,padding=2) for i in range(n - 1)]

    return modules,nfs[-1]




# Cell

class AEtrans(Module):
    def __init__(self,enc_arch,enc_dim=512,hidden_dim=2048, latent_dim=128, im_size=IMG_SIZE,out_range=OUT_RANGE):

        """
        inputs:
            arch, cut,pretrained
            enc_dim
            latent_dim
            hidden_dim

        """
        store_attr('enc_dim, hidden_dim,latent_dim,im_size')

        # encoder
        #  arch,cut = xresnet18(pretrained=True),-4
        #  enc_arch = list(arch.children())[:cut]

        self.encoder = build_AE_encoder(enc_arch,enc_dim=enc_dim, hidden_dim=hidden_dim, im_size=im_size)


        # VAE Bottleneck
        #self.bn = VAELayer(self.hidden_dim,self.latent_dim)
        self.bn = nn.Linear(self.hidden_dim,self.latent_dim)

        #decoder
        self.decoder = build_AE_decoder(hidden_dim=hidden_dim, latent_dim=latent_dim, im_size=im_size,out_range=out_range)


    def decode(self, z):
        return self.decoder(z)

    def encode(self, x):
        h = self.encoder(x)
        return self.bn(h)

    def forward(self, x):
        """
        pass the "latents" out to keep the learn mechanics consistent...
        """
        h = self.encoder(x)
        z = self.bn(h)
        x_reconst = self.decoder(z)
        latents = torch.stack([z,torch.clamp(z,-30,30)] ,dim=-1)


        return x_reconst , latents


# Cell

# called `after_batch`
# not efficient, but keeps things consistent with VAE
class AELoss(Module):
    """
    Measures how well we have created the original image,
    plus the KL Divergence with the unit normal distribution
    """
    def __init__(self, b_weight=1.):
        mse = MSELossFlat(reduction='sum')
        store_attr('mse,b_weight')

    def forward(self, preds, *target):
        """
        pred =(x_hat,KLD,kl_weight) #mu,log_var, kl_weight)
        target is x (original)
        """
        # For now there is no latent regularization.... so we can just ignore kl_weight

        # this handles the annealed kl_weight and passing the mu,logvar around we added...
        if(len(preds) == 3):
            x_hat, latents, kl_weight = preds
        else: #if len(preds) == 2:  # we should never get here... unless we delete teh callback
            x_hat, latents = preds

            kl_weight = x_hat[0].new(1)
            kl_weight[0] = 1.0

        #mu,logvar = latents[:,:,0],latents[:,:,1]
        mu, logvar = latents.split(1,dim=2)

        #note: both mse and KLD are summing errors over batches, and pixels or latents
        total = self.mse(x_hat, target[0])

        KLD = torch.zeros_like(mu) #-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        KLD = self.b_weight*KLD.sum()


        return (total + KLD*kl_weight)




# Cell

def gaussian_kernel(a, b):
    dim1_1, dim1_2 = a.shape[0], b.shape[0]
    depth = a.shape[1]
    a = a.view(dim1_1, 1, depth)
    b = b.view(1, dim1_2, depth)
    a_core = a.expand(dim1_1, dim1_2, depth)
    b_core = b.expand(dim1_1, dim1_2, depth)
    numerator = (a_core - b_core).pow(2).mean(2)/depth
    return torch.exp(-numerator)

def MMD(a, b):
    return gaussian_kernel(a, a).mean() + gaussian_kernel(b, b).mean() - 2*gaussian_kernel(a, b).mean()


class MMELoss(Module):
    """
    Measures how well we have created the original image,
    plus the KL Divergence with the unit normal distribution
    """
    def __init__(self, b_weight=1.):
        mse = MSELossFlat(reduction='mean')
        store_attr('mse,b_weight')

    def forward(self, preds, *target):
        """
        pred =(x_hat,KLD,kl_weight) #mu,log_var, kl_weight)
        target is x (original)
        """

        # this handles the annealed kl_weight and passing the mu,logvar around we added...
        if(len(preds) == 3):
            x_hat, latents, kl_weight = preds
        else: #if len(preds) == 2:  # we should never get here... unless we delete teh callback
            x_hat, latents = preds

            kl_weight = x_hat[0].new(1)
            kl_weight[0] = 1.0

        z, dummy = latents.split(1,dim=2)

        #note: both mse and KLD are summing errors over batches, and pixels or latents
        total = self.mse(x_hat, target[0])

        bs = latents.shape[0]
        latent_dim = z.shape[1]
        true_samples = torch.randn((bs,latent_dim), requires_grad=False).cuda()
        mmd_loss = MMD(true_samples, z) * self.b_weight

        return (total + mmd_loss*kl_weight)



# def compute_kernel(x, y):
#     x_size = x.shape[0]
#     y_size = y.shape[0]
#     dim = x.shape[1]

#     tiled_x = x.view(x_size,1,dim).repeat(1, y_size,1)
#     tiled_y = y.view(1,y_size,dim).repeat(x_size, 1,1)

#     return torch.exp(-torch.mean((tiled_x - tiled_y)**2,dim=2)/dim*1.0)


# def compute_mmd(x, y):
#     x_kernel = compute_kernel(x, x)
#     y_kernel = compute_kernel(y, y)
#     xy_kernel = compute_kernel(x, y)
#     return torch.mean(x_kernel) + torch.mean(y_kernel) - 2*torch.mean(xy_kernel)


# class VAERecreatedLoss(Module):
#     "Measures how well we have created the original tabular inputs, plus the KL Divergence with the unit normal distribution"
#     def __init__(self, cat_dict, dataset_size, bs, hidden_size, mmd_weight = 1000, reduction='mean'):
#         ce = CrossEntropyLossFlat(reduction='none')
#         mse = MSELossFlat(reduction='none')
#         store_attr('cat_dict,ce,mse,dataset_size,bs,hidden_size,mmd_weight,reduction')

#     def forward(self, preds, cat_targs, cont_targs):
#         if(len(preds) == 4):
#             cats,conts, z, kl_weight = preds
#         else:
#             cats,conts, z = preds
#             kl_weight = 1

#         true_samples = torch.randn((cats.shape[0],self.hidden_size))
#         true_samples = nn.Parameter(true_samples).cuda()

#         tot_ce, pos = [], 0
#         for i, (k,v) in enumerate(self.cat_dict.items()):
#             tot_ce += [self.ce(cats[:, pos:pos+v], cat_targs[:,i])]
#             pos += v

#         tot_ce = torch.stack(tot_ce, dim=1).mean(dim=1)
#         cont_loss = self.mse(conts, cont_targs).view(conts.shape).mean(dim=1)
#         recons_loss = (tot_ce + cont_loss)

#         mmd_loss = compute_mmd(true_samples, z).repeat(cats.shape[0])

#         total_loss = recons_loss + (mmd_loss * self.mmd_weight)

#         if self.reduction == 'mean':
#             return total_loss.mean()
#         elif self.reduction == 'sum':
#             return total_loss.sum()

#         return total_loss

# # test: if i call them preds instead of vals might the Metric base class reset automatically?
# class MyMetric(Metric):
#     "for simple average over batch quantities"
#     def reset(self):
#         "Clear all targs and preds"
#         self.vals = []
#     @property
#     def value(self):
#         return np.array(self.vals).mean()



class RawKLDMetric(MyMetric):
    def __init__(self):
        self.vals = []
    def accumulate(self, learn):
        latents = learn.pred[1]
        mu, logvar = latents.split(1,dim=2)
        KLD =  -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        self.vals.append(to_detach(KLD))


class L2Metric(MyMetric):
    def __init__(self): self.vals = []
    def accumulate(self, learn):
        x = to_detach(learn.y)
        recon_x = to_detach(learn.pred[0])
        nll = torch.mean((recon_x - x)**2)
        self.vals.append(nll)



class MMDMetric(MyMetric):
    def __init__(self, alpha=1):
        self.vals = []
        store_attr('alpha')

    def accumulate(self, learn):
        latents = learn.pred[1]
        z, dummy = latents.split(1,dim=2)

        bs = latents.shape[0]
        latent_dim = z.shape[1]
        true_samples = torch.randn((bs,latent_dim), requires_grad=False).cuda()
        mmd_loss = MMD(true_samples, z) * self.alpha

        self.vals.append(to_detach(mmd_loss))

